{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\layout\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Layout\\NTHU_CGV_Layout_exp\\Horizon_and_SAM\\Horizon\n",
      "d:\\Projects\\Layout\\NTHU_CGV_Layout_exp\\Horizon_and_SAM\\Horizon\n"
     ]
    }
   ],
   "source": [
    "# Data Loader\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import json\n",
    "#from CustomDataset import * \n",
    "from Horizon_DataLoader import * \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from config import *\n",
    "from file_helper import *\n",
    "#from Horizon_and_SAM.Horizon import PE_helper\n",
    "from  PE_helper import *\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint , Callback\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "#=================================\n",
    "#             Augmentation\n",
    "#=================================\n",
    "\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self ,\n",
    "                 train_dir ,\n",
    "                 test_dir , batch_size = 2,\n",
    "                 num_workers = 0 , img_size=[IMG_WIDTH, IMG_HEIGHT] , use_aug = True ,padding_count = 24 ,c =0.1\n",
    "                   ):\n",
    "        super().__init__()\n",
    "        self.train_dir = train_dir\n",
    "        self.test_dir = test_dir\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_size = img_size      \n",
    "        self.use_aug = use_aug\n",
    "        self.padding_count  = padding_count\n",
    "        self.c = c\n",
    "        \n",
    "\n",
    "        pass\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download dataset\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # Create dataset...          \n",
    "                \n",
    "        self.entire_dataset = CustomDataset(self.train_dir  , use_aug= self.use_aug ,  c=self.c , img_size=self.img_size)\n",
    "        self.train_ds , self.val_ds = random_split(self.entire_dataset , [0.9, 0.1])        \n",
    "        self.test_ds = CustomDataset(self.test_dir  , use_aug= False , img_size=self.img_size  )\n",
    "        \n",
    "        print(\"image size \",self.img_size)\n",
    "        pass\n",
    "\n",
    "    # ToDo: Reture Dataloader...\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=True)\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "dm = CustomDataModule ( train_dir= f\"../anno/test_visiable_10_no_cross.json\" ,\n",
    "                       test_dir= f\"../anno/test_visiable_10_no_cross.json\" , padding_count=1024, c =0.96,  img_size=[1024,512]\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "[   3   25   40   63  104  125  148  169  200  225  245  264  297  320\n",
      "  343  411  426  469  494  515  537  584  608  635  660  696  712  728\n",
      "  762  788  813  843  875  919  934  973 1003]\n",
      "[2.01227583 1.12423002 1.55002673 1.99133811 1.63046599 2.0892903\n",
      " 2.10653048 1.40111428 2.00786516 1.86988776 1.75866088 1.57124277\n",
      " 2.55658487 2.26872989 1.7262805  1.85227827 2.55975386 1.62991757\n",
      " 1.90061288 1.93088978 2.24738659 1.53587063 2.03083011 1.57529355\n",
      " 2.03659575 1.65624237 1.62391341 2.6632398  2.16912829 1.39338887\n",
      " 3.08385021 2.0042373  2.20866359 1.84091923 1.81488133 1.81734301\n",
      " 1.3303189 ]\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage.filters import maximum_filter\n",
    "def find_N_peaks2(signal, r=29, min_v=0.05, N=None):\n",
    "    max_v = maximum_filter(signal, size=r, mode='wrap')\n",
    "    pk_loc = np.where(max_v == signal)[0]\n",
    "    if(min_v>0):\n",
    "        pk_loc = pk_loc[signal[pk_loc] > min_v]\n",
    "    if N is not None:\n",
    "        order = np.argsort(-signal[pk_loc])\n",
    "        pk_loc = pk_loc[order[:N]]\n",
    "        pk_loc = pk_loc[np.argsort(pk_loc)]\n",
    "    return pk_loc, signal[pk_loc]\n",
    "\n",
    "\n",
    "a = np.random.randn(1024)\n",
    "pk_loc , signal = find_N_peaks(a )\n",
    "print(pk_loc.shape)\n",
    "print(pk_loc)\n",
    "print(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size  [1024, 512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | HorizonNet | 81.6 M\n",
      "-------------------------------------\n",
      "81.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "81.6 M    Total params\n",
      "163.165   Total estimated model params size (MB)\n",
      "d:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:229: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "d:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1613: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  category=PossibleUserWarning,\n",
      "d:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:229: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 1/2 [00:04<00:04,  4.31s/it, loss=0.58, v_num=][val] total_loss tensor(nan, device='cuda:0')\n",
      "Epoch 0: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it, loss=0.58, v_num=]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it, loss=0.58, v_num=]\n"
     ]
    }
   ],
   "source": [
    "from raw_model_pixelwize_bbox import *\n",
    "from Horizon_DataLoader import  * \n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from torch import Tensor\n",
    "def unpad_data( x :[Tensor] ) :\n",
    "\tnon_zero_indices = torch.nonzero(x)\n",
    "\t#print(non_zero_indices)\n",
    "\t# Get the non-zero values\n",
    "\tnon_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "\tunique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\t\n",
    "\tnon_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "\t\n",
    "\treturn non_zero_values\n",
    "@torch.no_grad()\n",
    "def pack_visualize( gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , dv_btm_b ):\n",
    "    \n",
    "    if isinstance(gt_u_b, torch.Tensor):\n",
    "        sizes = [t.numel() for t in gt_u_b]               \n",
    "        us = gt_u_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "        us[1::2]+=gt_du_b.flatten()\n",
    "        us = torch.split(us.view(-1,2) , sizes)\n",
    "\n",
    "        tops = gt_vtop_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "        tops[1::2]=gt_dvtop_b.flatten()\n",
    "        tops = torch.split(tops.view(-1,2) , sizes)\n",
    "\n",
    "        btms = gt_vbtm_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "        btms[1::2]=dv_btm_b.flatten()\n",
    "        btms = torch.split(btms.view(-1,2) , sizes)\n",
    "\n",
    "    elif isinstance(gt_u_b, tuple) and all(isinstance(t, torch.Tensor) for t in gt_u_b):        \n",
    "        sizes = [len(t) for t in gt_u_b]               \n",
    "        us = torch.cat(gt_u_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "        us[1::2]+=torch.cat(gt_du_b).view(-1)\n",
    "        us = torch.split(us.view(-1,2) , sizes)\n",
    "\n",
    "        tops = torch.cat(gt_vtop_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "        tops[1::2]=torch.cat(gt_dvtop_b).view(-1)\n",
    "        tops = torch.split(tops.view(-1,2) , sizes)\n",
    "\n",
    "        btms = torch.cat(gt_vbtm_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "        btms[1::2]=torch.cat(dv_btm_b).view(-1)\n",
    "        btms = torch.split(btms.view(-1,2) , sizes)\n",
    "    else:\n",
    "        assert(\"Wrong Type.\")\n",
    "    \n",
    "    return us , tops ,btms\n",
    "\n",
    "class Model(pl.LightningModule):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = HorizonNet(backbone='resnet50', use_rnn=True)\n",
    "        self.log_folder = os.path.join(os.getcwd() , \"all_0227\" )\n",
    "        self.post_thrshold = 0.5\n",
    "        \n",
    "    def forward(self , x):\n",
    "        prob , boxs  = self.model(x)  # [ b , _ , 1024]\n",
    "        prob = prob.permute((0,2,1))\n",
    "        boxs = boxs.permute((0,2,1))\n",
    "        \n",
    "\n",
    "        return prob , boxs\n",
    "        pass\n",
    "\n",
    "    def training_step(self , input_b ,batch_idx ):\n",
    "        img = input_b['image']        \n",
    "        gt_pro = input_b['u_grad']\n",
    "\n",
    "        gt_u_b = unpad_data( input_b['u'])          \n",
    "        gt_vtop_b =unpad_data(input_b['v_top'])\n",
    "        gt_vbtm_b = unpad_data (input_b['v_btm'])\n",
    "        gt_du_b = unpad_data(input_b['du'])\n",
    "        gt_dvtop_b = unpad_data(input_b['dv_top'])\n",
    "        gt_dv_btm_b = unpad_data(input_b['dv_btm'])\n",
    "\n",
    "        out_prob , out_boxs = self.forward(img)\n",
    "        batch_size = out_prob.shape[0]\n",
    "        \n",
    "        total_loss = 0\n",
    "        b_cnt = 0\n",
    "        for u,vtop,vbtm,du,dvtop, dvbtm , pred_cls , pred_box , gt_cls in zip(gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , gt_dv_btm_b , out_prob , out_boxs , gt_pro):\n",
    "            gt_box =  torch.vstack([ u, vtop,vbtm,  du ,dvtop , dvbtm]).permute(1,0)   # [n , 6]  \n",
    "            gt_cnt = gt_box.shape[0]\n",
    "\n",
    "            # Select n peak column\n",
    "            pkloc , signal = find_N_peaks2( pred_cls.detach().cpu().numpy().astype(np.float32).flatten() , min_v=-2, N=gt_cnt)            \n",
    "            pred_u = pkloc / 1024\n",
    "            #print(\"pkloc\" , pkloc)\n",
    "            #print(\"pred_u\" , pred_u)\n",
    "            #print(\"gt u\" , u)\n",
    "            \n",
    "            # Match gt for each selected peak\n",
    "            u_cdist =distance.cdist( np.expand_dims(pred_u, axis=-1),  u.unsqueeze(-1).detach().cpu().numpy() )\n",
    "            row_idx , col_idx = linear_sum_assignment ( u_cdist )\n",
    "\n",
    "            #print(\"row_idx \" , row_idx)\n",
    "            #print(\"col_idx\" , col_idx)\n",
    "            #print(\"pred_box[row_idx]\" , pred_box[row_idx])\n",
    "            #print(\"gt_box[col_idx]\" , gt_box[col_idx])\n",
    "            l1_loss = F.l1_loss(pred_box[pkloc][row_idx] , gt_box[col_idx,1:]   )         \n",
    "            total_loss += l1_loss / gt_cnt\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #if batch_idx % 5 ==0:\n",
    "                if self.current_epoch > 0 and self.current_epoch % 5 == 0  and batch_idx <5 :                \n",
    "                    save_path =  os.path.join(self.log_folder , f\"gt_ep_{self.current_epoch}-{self.global_step}-{batch_idx}\" )                    \n",
    "                    # View GT\n",
    "                    gt_us , gt_tops , gt_btms = pack_visualize(u.view(1 , -1 ) , vtop , vbtm , du , dvtop , dvbtm )                   \n",
    "                    vis_imgs = visualize_2d_single(gt_us , gt_tops , gt_btms , u_grad =  gt_cls.view(1 , -1 ), imgs= img[b_cnt] , title=\"GT\",save_path=save_path )                \n",
    "                    #plt.imshow(vis_imgs)                    \n",
    "                    #plt.show()\n",
    "                    \n",
    "                    # View Prediction\n",
    "                    #decode_pred = decode_pred_b[row_idx]\n",
    "                    decode_pred = pred_box[pkloc][row_idx].detach().cpu()\n",
    "                    save_path =  os.path.join(self.log_folder , f\"pred_ep_{self.current_epoch}-{self.global_step}-{batch_idx}\" )\n",
    "                    pred_us , pred_tops , pred_btms = pack_visualize( torch.as_tensor(pred_u[row_idx]) , decode_pred[:,0],decode_pred[:,1],\n",
    "                                                                          decode_pred[:,2] ,decode_pred[:,3],decode_pred[:,4] )                    \n",
    "                    vis_imgs = visualize_2d_single(pred_us , pred_tops , pred_btms , u_grad = F.sigmoid(pred_cls).view(1 , -1 ) , imgs=  img[b_cnt] ,\n",
    "                                                    title=f\"Pred_row{row_idx}-\\n u:{pred_us}\" , save_path= save_path  )\n",
    "                    #plt.imshow(vis_imgs)                    \n",
    "                    #plt.show()\n",
    "                pass\n",
    "            b_cnt+=1\n",
    "        total_loss/= batch_size\n",
    "        \n",
    "        cls_loss = F.binary_cross_entropy_with_logits(out_prob.view(batch_size , -1) , gt_pro.view(batch_size , -1))\n",
    "        self.log(f\"train_cls_loss\" , cls_loss)\n",
    "        self.log(f\"train_box_loss\" , total_loss)\n",
    "        total_loss += cls_loss \n",
    "        self.log(f\"train_total_loss\" , total_loss)\n",
    "        #print(\"cls_loss\" , cls_loss)\n",
    "        #print(\"total_loss\" , total_loss)\n",
    "        return total_loss\n",
    "    \n",
    "    def validation_step(self, input_b, batch_idx):\n",
    "        #ToDo...\n",
    "        img = input_b['image']        \n",
    "        out_prob , out_boxs = self.forward(img)\n",
    "\n",
    "        gt_pro = input_b['u_grad']\n",
    "\n",
    "        gt_u_b = unpad_data( input_b['u'])          \n",
    "        gt_vtop_b =unpad_data(input_b['v_top'])\n",
    "        gt_vbtm_b = unpad_data (input_b['v_btm'])\n",
    "        gt_du_b = unpad_data(input_b['du'])\n",
    "        gt_dvtop_b = unpad_data(input_b['dv_top'])\n",
    "        gt_dv_btm_b = unpad_data(input_b['dv_btm'])\n",
    "\n",
    "        out_prob , out_boxs = self.forward(img)\n",
    "        batch_size = out_prob.shape[0]\n",
    "        \n",
    "        total_loss = 0\n",
    "        b_cnt = 0\n",
    "        for u,vtop,vbtm,du,dvtop, dvbtm , pred_cls , pred_box , gt_cls in zip(gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , gt_dv_btm_b , out_prob , out_boxs , gt_pro):\n",
    "            gt_box =  torch.vstack([ u, vtop,vbtm,  du ,dvtop , dvbtm]).permute(1,0)   # [n , 6]  \n",
    "            gt_cnt = gt_box.shape[0]\n",
    "\n",
    "            # Select n peak column\n",
    "            pkloc , signal = find_N_peaks2( pred_cls.detach().cpu().numpy().astype(np.float32).flatten() , min_v=self.post_thrshold, N=None)            \n",
    "            pred_u = pkloc / 1024\n",
    "            \n",
    "            # Match gt for each selected peak\n",
    "            u_cdist =distance.cdist( np.expand_dims(pred_u, axis=-1),  u.unsqueeze(-1).detach().cpu().numpy() )\n",
    "            row_idx , col_idx = linear_sum_assignment ( u_cdist )\n",
    "\n",
    "            l1_loss = F.l1_loss(pred_box[pkloc][row_idx] , gt_box[col_idx,1:]   )         \n",
    "            total_loss += l1_loss / row_idx.shape[0] +  abs(gt_cnt - row_idx.shape[0])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #if batch_idx % 5 ==0:\n",
    "                if self.current_epoch > 0 and self.current_epoch % 5 == 0  and batch_idx <5 :                \n",
    "                    save_path =  os.path.join(self.log_folder , f\"val_gt_ep_{self.current_epoch}-{self.global_step}-{batch_idx}\" )                    \n",
    "                    # View GT\n",
    "                    gt_us , gt_tops , gt_btms = pack_visualize(u.view(1 , -1 ) , vtop , vbtm , du , dvtop , dvbtm )                   \n",
    "                    vis_imgs = visualize_2d_single(gt_us , gt_tops , gt_btms , u_grad =  gt_cls.view(1 , -1 ), imgs= img[b_cnt] , title=\"GT\",save_path=save_path )                \n",
    "                    #plt.imshow(vis_imgs)                    \n",
    "                    #plt.show()\n",
    "                    \n",
    "                    # View Prediction\n",
    "                    #decode_pred = decode_pred_b[row_idx]\n",
    "                    decode_pred = pred_box[pkloc][row_idx].detach().cpu()\n",
    "                    save_path =  os.path.join(self.log_folder , f\"val_pred_ep_{self.current_epoch}-{self.global_step}-{batch_idx}\" )\n",
    "                    pred_us , pred_tops , pred_btms = pack_visualize( torch.as_tensor(pred_u[row_idx]) , decode_pred[:,0],decode_pred[:,1],\n",
    "                                                                          decode_pred[:,2] ,decode_pred[:,3],decode_pred[:,4] )                    \n",
    "                    vis_imgs = visualize_2d_single(pred_us , pred_tops , pred_btms , u_grad = F.sigmoid(pred_cls).view(1 , -1 ) , imgs=  img[b_cnt] ,\n",
    "                                                    title=f\"Pred_row{row_idx}-\\n u:{pred_us}\" , save_path= save_path  )\n",
    "                    #plt.imshow(vis_imgs)                    \n",
    "                    #plt.show()\n",
    "                pass\n",
    "            b_cnt+=1\n",
    "        total_loss/= batch_size\n",
    "        \n",
    "        cls_loss = F.binary_cross_entropy_with_logits(out_prob.view(batch_size , -1) , gt_pro.view(batch_size , -1))\n",
    "        self.log(f\"val_cls_loss\" , cls_loss)\n",
    "        self.log(f\"val_box_loss\" , total_loss)\n",
    "        total_loss += cls_loss \n",
    "        self.log(f\"val_total_loss\" , total_loss)\n",
    "        \n",
    "        print(\"[val] total_loss\" , total_loss)\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #ToDo: Look at paper \n",
    "        opt = optim.Adam(self.parameters() , lr=0.00035 , betas= (0.9 , 0.999) , weight_decay= 0)\n",
    "        # Ref: https://github.com/sunset1995/HorizonNet/blob/master/train.py#L44\n",
    "\n",
    "        return [opt] , []\n",
    "\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger('tb_logs', name='Horizon_test')\n",
    "\n",
    "dm = CustomDataModule ( train_dir= f\"../../anno/train_visiable_all.json\" ,\n",
    "                        test_dir= f\"../../anno/test_visiable_all.json\" ,\n",
    "                        #test_dir= f\"../anno/train_visiable_20_no_cross.json\" ,\n",
    "                        padding_count=1024,\n",
    "                        use_aug=False , c= 0.96,batch_size=5,\n",
    "                        img_size=[1024,512]\n",
    "                       )\n",
    "\n",
    "\n",
    "save_path = create_folder( os.path.join(os.getcwd() , \"output\" , \"checkpoints\"))\n",
    "#save_file = os.path.join(save_path , \"test_d20_0226.ckpt\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_total_loss',  # The validation metric to monitor\n",
    "    dirpath= save_path ,  # Directory where checkpoints will be saved\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',  # Checkpoint file name\n",
    "    save_top_k=3,  # Save only the best model\n",
    "    mode='min'  # 'min' for metrics where lower is better (like loss), 'max' for metrics where higher is better (like accuracy)\n",
    ")\n",
    "trainer = pl.Trainer(accelerator='gpu' , devices=1 ,\n",
    "                    min_epochs=1, max_epochs=160 , precision=16 , fast_dev_run=False ,\n",
    "                    callbacks=[checkpoint_callback])\n",
    "m=Model()\n",
    "#m = m.load_from_checkpoint(save_file)\n",
    "trainer.fit(m , dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(20)\n",
    "b = np.expand_dims(a, axis=-1)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = create_folder( os.path.join(os.getcwd() , \"output\" , \"checkpoints\"))\n",
    "save_file = os.path.join(save_path , \"test_d20.ckpt\")\n",
    "trainer.save_checkpoint(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = create_folder( os.path.join(os.getcwd() , \"output\" , \"checkpoints\"))\n",
    "save_file = os.path.join(save_path , \"test.pth\")\n",
    "#torch.save(m , save_file)\n",
    "torch.save(m.state_dict(), save_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Layout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
