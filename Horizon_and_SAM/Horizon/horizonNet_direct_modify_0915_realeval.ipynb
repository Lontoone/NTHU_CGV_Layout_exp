{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "#device = 'cpu'  # TODO: CHANGE THIS!!!\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import cv2\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "\n",
    "#======= [ SETTING ] =======\n",
    "\n",
    "MAX_PREDICTION_COUNT = 90\n",
    "BATCH_SIZE=6\n",
    "C = 0.1\n",
    "R = 10\n",
    "CONFIDENCE_THRESHOLD = 0.05\n",
    "\n",
    "MODEL_FOLDER =r'./output/'\n",
    "#LONAD_MODEL_NAME =\"n90-c0.1-r10-aug-ep50-pickle.pth\"  \n",
    "LONAD_MODEL_NAME =\"n90-c0.1-r10-0913-all-Aug-1023-with_aug_ep166.pth\"  \n",
    "#LONAD_MODEL_NAME =\"n90-c0.1-r10-0912-all_noAug_ep80_0.664.pth\"  \n",
    "#TRAIN_NAME = f\"n{MAX_PREDICTION_COUNT}-c{C}-r{R}-0913-all-Aug\"\n",
    "#TRAIN_NAME = f\"n90-c0.1-r10-0912-all_noAug_best_EVAL\"\n",
    "DO_AUG= True\n",
    "#TRAIN_NAME = f\"n90-c0.1-r10-0913-all-Aug--AugModel_noAugTest-0920-2\"\n",
    "TRAIN_NAME = f\"n90-c0.1-r10-0913-all-Aug-1027_eval\"\n",
    "writer = SummaryWriter(TRAIN_NAME)\n",
    "\n",
    "LOADED_EPOCH = None\n",
    "LOADED_AP = None\n",
    "\n",
    "TRAIN_DATASET_NAME  = \"train_visiable_horizon_unique_w0.01_all_fixedbug.json\"\n",
    "TEST_DATASET_NAME   = \"test_visiable_horizon_unique_w0.01_all_fixedbug.json\"\n",
    "#==============\n",
    "\n",
    "#========= [ Log Setting ] ==========\n",
    "MAX_LOG_GAP = 5\n",
    "MAX_LOG_IT_COUNT = 5\n",
    "EVAL_GAP = 5\n",
    "save_auc = 0.2\n",
    "log_folder = os.path.join(os.getcwd() , \"output\" , TRAIN_NAME )\n",
    "if (not os.path.exists(log_folder)):\n",
    "    os.makedirs(log_folder)   \n",
    "#====================================\n",
    "\n",
    "def reset_ncr_config(n, c, r ):\n",
    "    global MAX_PREDICTION_COUNT\n",
    "    global C\n",
    "    global R\n",
    "    global TRAIN_NAME\n",
    "    global writer\n",
    "    global log_folder\n",
    "\n",
    "    MAX_PREDICTION_COUNT = n\n",
    "    C = c \n",
    "    R = r\n",
    "    #TRAIN_NAME = f\"n{MAX_PREDICTION_COUNT}-c{C}-r{R}-0908\"\n",
    "    TRAIN_NAME = f\"n{MAX_PREDICTION_COUNT}-c{C}-r{R}-0913-all-Aug-1023\"\n",
    "    writer = SummaryWriter(TRAIN_NAME)\n",
    "\n",
    "    log_folder = os.path.join(os.getcwd() , \"output\" , TRAIN_NAME )\n",
    "    if (not os.path.exists(log_folder)):\n",
    "        os.makedirs(log_folder)   \n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======= [ Load Model ] =======\n",
    "from horizon_model_direct import HorizonNet\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "net = HorizonNet('resnet50', True , MAX_PREDICTION_COUNT).to(device)   # For server (small memory)\n",
    "\n",
    "if (LONAD_MODEL_NAME is not \"\" or None):          \n",
    "    #state_dict = torch.load(f'./output/{LONAD_MODEL_NAME}', map_location='cpu')\n",
    "    model_path = os.path.join(MODEL_FOLDER , LONAD_MODEL_NAME)\n",
    "    state_dict = torch.load(model_path, map_location='cpu')\n",
    "    print(state_dict.keys())\n",
    "    net.load_state_dict(state_dict['state_dict'])\n",
    "    LOADED_EPOCH    = state_dict['epoch']\n",
    "    LOADED_AP       = state_dict['ap']\n",
    "    save_auc        = LOADED_AP  \n",
    "    print(LOADED_EPOCH,LOADED_AP)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, net.parameters()),\n",
    "        lr=1e-4, betas=(0.9, 0.999))\n",
    "'''\n",
    "'''\n",
    "\n",
    "def reset_model():\n",
    "    global net\n",
    "    global optimizer\n",
    "    net = HorizonNet('resnet50', True , MAX_PREDICTION_COUNT).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, net.parameters()),\n",
    "        lr=1e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LOADED_EPOCH,LOADED_AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "from shapely.geometry import Polygon\n",
    "def predict (data):    \n",
    "    imgs = data['image'].to(device)        \n",
    "    out = net(imgs)\n",
    "    \n",
    "    return out\n",
    "    \n",
    "\n",
    "# 輸出 [b , 5 , max_door_count]\n",
    "#   Note: [b,0]為x座標，非原本的u_grad\n",
    "def filter_peak_data(predict , min_threshold=-1):\n",
    "    u  , v_top ,v_btm , d_top , d_btm  = predict\n",
    "\n",
    "    b,n, _, w  = u.shape\n",
    "\n",
    "    result = torch.zeros((b,5,n)).to(device)\n",
    "    u = u .reshape(b,n,-1)\n",
    "    v_top = v_top.reshape(b,n,-1)\n",
    "    v_btm = v_btm.reshape(b,n,-1)\n",
    "    d_top = d_top.reshape(b,n,-1)\n",
    "    d_btm = d_btm.reshape(b,n,-1)\n",
    "\n",
    "    peak_col_idx = torch.argmax(u,2)     \n",
    "\n",
    "    #print(\"peak_idx\" , peak_col_idx)\n",
    "    for i in range(b):\n",
    "        _t = torch.arange(MAX_PREDICTION_COUNT).to(device)\n",
    "        idx = _t * w + peak_col_idx[i]             \n",
    "        \n",
    "        result[i,0,:]=peak_col_idx[i] / w  \n",
    "        result[i,1,:]=v_top.reshape(b , -1)[i,idx]\n",
    "        result[i,2,:]=v_btm.reshape(b , -1)[i,idx]\n",
    "        result[i,3,:]=d_top.reshape(b , -1)[i,idx]\n",
    "        result[i,4,:]=d_btm.reshape(b , -1)[i,idx]\n",
    "\n",
    "        if(min_threshold>0):\n",
    "            u_max_value_each_cha = torch.max(u[i] , 1)[0]            \n",
    "            low_u_idx = torch.where(u_max_value_each_cha < min_threshold)[0]            \n",
    "            result[i,0,low_u_idx] = -9999\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        pass    \n",
    "    return result\n",
    "\n",
    "def encode(packed_data):\n",
    "    _esp = 0.000001  # 避免有些門寬=0的爆炸 (真的會爆炸)\n",
    "    packed_data[:,1] = torch.log( 0.5 - packed_data[:,1])  #v_top\n",
    "    packed_data[:,2] = torch.log( packed_data[:,2] - 0.5)  #v_btm\n",
    "    packed_data[:,3] = torch.log( packed_data[:,3] + _esp )  #du\n",
    "    #packed_data[:,3] = torch.log( packed_data[:,3] )  #du\n",
    "    packed_data[:,4] = torch.log( 0.5 - packed_data[:,4] + _esp)  #v_top2\n",
    "    packed_data[:,5] = torch.log( packed_data[:,5] - 0.5 + _esp)  #v_btm2\n",
    "\n",
    "    zeros = torch.zeros_like(packed_data)\n",
    "    is_nan = torch.isnan(packed_data)\n",
    "    packed_data = torch.where(is_nan , zeros , packed_data)    \n",
    "    #packed_data = torch.nan_to_num(packed_data)\n",
    "    return packed_data\n",
    "    pass\n",
    "\n",
    "#from scipy.spatial import distance_matrix\n",
    "#def match_gt(gt_u , predict_u ):\n",
    "def match_gt(gt_data , predict_data):\n",
    "    \n",
    "    # Filter out zero\n",
    "    #zeros_mask = torch.zeros_like(gt_data[:,0,:])\n",
    "    #ones_mask = torch.ones_like(gt_data[:,0,:])\n",
    "    #print(\"gt_data[:,0,:]\" , gt_data[:,0,:])\n",
    "    nonZero_idx = torch.where(gt_data[:,0,:] != 0)[0]\n",
    "    nonZero_idx = torch.unique(nonZero_idx ,return_counts=True)[1]\n",
    "    #print(\"nonZero_idx\" , nonZero_idx)\n",
    "    \n",
    "    \n",
    "    gt_u = gt_data[:,0,:]    \n",
    "    #print(\"filtered gt u \" , gt_u )\n",
    "    predict_u = predict_data[:,0,:]\n",
    "\n",
    "\n",
    "    #print(\"mt u\",gt_u.shape)\n",
    "    #print(\"mt pu\",predict_u.shape)\n",
    "    #b , n , _ , __ = predict_u.shape\n",
    "    b , n   = predict_u.shape\n",
    "    #print(predict_u.shape)\n",
    "\n",
    "    matched_gt_u = []\n",
    "    matched_gt_vtop = []\n",
    "    matched_gt_vbtm = []\n",
    "    matched_gt_du = []\n",
    "    matched_gt_dvtop = []\n",
    "    matched_gt_dvbtm = []\n",
    "\n",
    "    matched_prd_u = []\n",
    "    matched_prd_vtop = []\n",
    "    matched_prd_vbtm = []\n",
    "    matched_prd_du = []\n",
    "    matched_prd_dvtop = []\n",
    "    matched_prd_dvbtm = []\n",
    "    gt_idxs=[]\n",
    "    #print(\"gt_before sort\" , gt_data)\n",
    "    #pos_idxs =[]\n",
    "    #neg_idxs =[]\n",
    "    #neg_pred_u = []\n",
    "    u_interval = 1 / MAX_PREDICTION_COUNT    \n",
    "    for i in range(b):\n",
    "        #dist_mat = distance_matrix(gt_u[i] , predict_u[i] )\n",
    "        # Sort by u distance\n",
    "        pos_count =nonZero_idx[i]        \n",
    "\n",
    "        sorted_gt_u , sorted_gt_idx  = torch.sort(gt_u[i,:pos_count] , dim=0)        \n",
    "        #gt_data[i,:,:pos_count] = gt_data[i,:,sorted_gt_idx]       \n",
    "        gt_data[i,:-1,:pos_count] = gt_data[i,:-1,sorted_gt_idx]        # -1: dont sort u_grad\n",
    "        \n",
    "        u_interval_idx = (sorted_gt_u / u_interval).type(torch.long)\n",
    "        gt_idxs.append(u_interval_idx)\n",
    "        #print(\"u_interval_idx\" , u_interval_idx)\n",
    "\n",
    "        target = torch.zeros((6,MAX_PREDICTION_COUNT)).to(device)                                \n",
    "        target[0] = gt_data[i,-1]   # u_grad\n",
    "        target[0,u_interval_idx] = 1   # 分類問題\n",
    "        target[1:,u_interval_idx] = gt_data[i,1:-1,:pos_count]\n",
    "        #target[1:,u_interval_idx] = gt_data[i,1:,:pos_count]\n",
    "\n",
    "\n",
    "        matched_gt_u.append(target[0,:])\n",
    "        matched_gt_vtop.append(target[1,:])\n",
    "        matched_gt_vbtm.append(target[2,:])\n",
    "        matched_gt_du.append(target[3,:])\n",
    "        matched_gt_dvtop.append(target[4,:])\n",
    "        matched_gt_dvbtm.append(target[5,:])\n",
    "\n",
    "\n",
    "        matched_prd_u.append(predict_data[i,0,:])\n",
    "        matched_prd_vtop.append(predict_data[i,1,:])\n",
    "        matched_prd_vbtm.append(predict_data[i,2,:])\n",
    "        matched_prd_du.append(predict_data[i,3,:])\n",
    "        matched_prd_dvtop.append(predict_data[i,4,:])\n",
    "        matched_prd_dvbtm.append(predict_data[i,5,:])\n",
    "        #neg_pred_u.append(predict_data[i,0,neg_u_idx])\n",
    "        \n",
    "    '''\n",
    "    return  matched_gt_u , matched_gt_vtop , matched_gt_vbtm ,\\\n",
    "            matched_gt_dtop ,matched_gt_dbtm , matched_prd_u , \\\n",
    "            matched_prd_vtop ,matched_prd_vbtm ,matched_prd_dtop , matched_prd_dbtm, \\\n",
    "            #pos_idxs,neg_idxs #neg_pred_u\n",
    "    '''\n",
    "    return  (matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,\\\n",
    "            (matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm,),\\\n",
    "            gt_idxs\n",
    "    \n",
    "    pass\n",
    "def get_grad_u(u ,_width = 1024):    \n",
    "    u_len = u.shape[0]\n",
    "    width = _width\n",
    "    dist = torch.arange(0, width)\n",
    "    #dist = dist.tile((u.shape[0],1) )            \n",
    "    dist = dist.repeat((u.shape[0],1) )            \n",
    "    dist = torch.abs( dist.float() - u.reshape((-1,1))*width )        \n",
    "    c_dist = C ** dist              \n",
    "    \n",
    "    #c_dist[:u_len//2] = torch.max(c_dist[ 0::2 ] , c_dist[ 1::2 ])\n",
    "    \n",
    "    return c_dist\n",
    "\n",
    "def get_grad_u_keep_batch(batch_u , pair =False , width = 1024):\n",
    "    result =[]\n",
    "    for u in batch_u:            \n",
    "        w = u.shape[0]\n",
    "        if (pair):\n",
    "            u1 = get_grad_u(u[:w//2].cpu().detach() , width)\n",
    "            u2 = get_grad_u(u[w//2:].cpu().detach() , width)\n",
    "            result.append( torch.max(u1,u2))\n",
    "        else:\n",
    "            result.append(get_grad_u(u.cpu().detach() , width) )\n",
    "    \n",
    "    return result\n",
    "def u_interval_to_real_u(u_interval , threshold = 0.25):\n",
    "    \n",
    "    mid_offset = 1/MAX_PREDICTION_COUNT *0.5\n",
    "    line = torch.arange(MAX_PREDICTION_COUNT).to(device).float() /  float( MAX_PREDICTION_COUNT) + mid_offset    \n",
    "    i = 0\n",
    "    masks = []    \n",
    "    for ui in u_interval:            \n",
    "        zero_mask = torch.zeros_like(ui)\n",
    "        one_mask = torch.ones_like(ui)\n",
    "        #mask = (torch.sigmoid(ui) > threshold)\n",
    "        mask = torch.where(torch.sigmoid(ui) > threshold , one_mask , zero_mask)\n",
    "        masks.append(mask)\n",
    "        u_interval[i] = mask * line\n",
    "        i+=1\n",
    "    \n",
    "    return u_interval , masks\n",
    "from scipy import ndimage\n",
    "def find_N_peaks(signal, r=29, min_v=0.05, N=None):\n",
    "    max_v = ndimage.maximum_filter(signal, size=r, mode='wrap')    \n",
    "    pk_loc = np.where(max_v == signal)[0]\n",
    "    pk_loc = pk_loc[signal[pk_loc] > min_v]\n",
    "    if N is not None:\n",
    "        order = np.argsort(-signal[pk_loc])\n",
    "        pk_loc = pk_loc[order[:N]]\n",
    "        pk_loc = pk_loc[np.argsort(pk_loc)]\n",
    "    return pk_loc, signal[pk_loc]\n",
    "\n",
    "def cal_loss(pred , gt , pk_idxs):\n",
    "    #b = len(gt)\n",
    "    l1_loss =  torch.nn.L1Loss()\n",
    "    bce = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    gt_u = torch.cat( gt[0])    \n",
    "    pred_u = torch.cat( pred[0])    \n",
    "    #print(\"gt vtop data \" , gt[1])\n",
    "    #print(\"pred vtop data \" , pred[1])\n",
    "    \n",
    "    #u_loss = F.binary_cross_entropy_with_logits( pred_u , gt_u)            \n",
    "    u_loss = bce( pred_u , gt_u)            \n",
    "    #print(\"u_loss pred u \",  pred_u)\n",
    "    #print(\"u_loss gt u \",  gt_u)\n",
    "    \n",
    "    #non_zero_idx = torch.where(gt_u > 0)[0]  \n",
    "    non_zero_idx=[]\n",
    "    i=0\n",
    "    for pk in pk_idxs:\n",
    "        non_zero_idx.append(pk + i * MAX_PREDICTION_COUNT)\n",
    "        i+=1\n",
    "    non_zero_idx = torch.cat(non_zero_idx)\n",
    "    #print(\"cal loss idx\" , non_zero_idx) \n",
    "\n",
    "    # other loss\n",
    "    gt_vtop = torch.cat(gt[1])[non_zero_idx]    \n",
    "    pred_vtop = torch.cat(pred[1])[non_zero_idx]\n",
    "    \n",
    "    v_top_loss = l1_loss(pred_vtop , gt_vtop )   \n",
    "\n",
    "    pred_vbtm   = torch.cat(pred[2])[non_zero_idx]\n",
    "    gt_vbtm     = torch.cat(gt[2])[non_zero_idx]\n",
    "    v_btm_loss = l1_loss(pred_vbtm , gt_vbtm )\n",
    "    #print(\"gt_vbtm\" ,gt_vbtm)\n",
    "    #print(\"pred_vbtm\" ,pred_vbtm)\n",
    "\n",
    "    pred_du = torch.cat(pred[3])[non_zero_idx]    \n",
    "    gt_du = torch.cat(gt[3])[non_zero_idx]\n",
    "    du_loss = l1_loss(pred_du , gt_du )\n",
    "\n",
    "    pred_dtop = torch.cat(pred[4])[non_zero_idx]    \n",
    "    gt_dtop = torch.cat(gt[4])[non_zero_idx]\n",
    "    d_top_loss = l1_loss(pred_dtop , gt_dtop )\n",
    "\n",
    "    pred_dbtm = torch.cat(pred[5])[non_zero_idx]    \n",
    "    gt_dbtm = torch.cat(gt[5])[non_zero_idx]\n",
    "    d_btm_loss = l1_loss(pred_dbtm , gt_dbtm )\n",
    "\n",
    "    #losses = {\"u_loss\":u_loss *10, \"v_top\":v_top_loss , \"v_btm\":v_btm_loss,\"du\":du_loss ,\"d_top\":d_top_loss ,\"d_btm\":d_btm_loss }    \n",
    "    #losses = {\"u_loss\":u_loss , \"v_top\":v_top_loss  , \"v_btm\":v_btm_loss  }    \n",
    "    losses = {\"u_loss\":u_loss *20, \"v_top\":v_top_loss , \"v_btm\":v_btm_loss ,\"du\":du_loss ,\"d_top\":d_top_loss ,\"d_btm\":d_btm_loss }    \n",
    "    #losses = {\"u_loss\":u_loss }    \n",
    "    \n",
    "    '''\n",
    "    '''\n",
    "    #print(losses)\n",
    "\n",
    "    return losses\n",
    "    '''\n",
    "    [Debug]\n",
    "    _img =  gt_grad_u[0].tile((10,1)).cpu().detach().numpy()\n",
    "    _img2 =  pred_grad_u[0].tile((10,1)).cpu().detach().numpy()\n",
    "    plt.imshow(_img , cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imshow(_img2,cmap='gray')\n",
    "    plt.show()\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def to_bbox( u_pack , vt_pack , vb_pack ):\n",
    "\n",
    "    u_flatten  = torch.cat(u_pack)\n",
    "    vt_flatten  = torch.cat(vt_pack)\n",
    "    vb_flatten  = torch.cat(vb_pack)\n",
    "\n",
    "    non_zero_idx = torch.where(u_flatten>0)[0]\n",
    "    u_flatten = u_flatten[non_zero_idx]\n",
    "    vt_flatten = vt_flatten[non_zero_idx].reshape(-1 , 2)\n",
    "    vb_flatten = vb_flatten[non_zero_idx].reshape(-1 , 2)\n",
    "\n",
    "    vt_flatten = torch.min(vt_flatten , 1)[0]\n",
    "    vb_flatten = torch.max(vb_flatten , 1)[0]\n",
    "\n",
    "    bboxes=[]\n",
    "    for i in range(vt_flatten.shape[0]):\n",
    "        x1 = u_flatten[2*i]\n",
    "        x2 = u_flatten[2*i+1]\n",
    "        y1 = vt_flatten[i]\n",
    "        y2 = vb_flatten[i]\n",
    "        bboxes.append((x1,y1,x2,y2))\n",
    "    bboxes = torch.as_tensor(bboxes).reshape(-1,4)\n",
    "    return bboxes \n",
    "    pass\n",
    "\n",
    "def decode(bdata , u_thresh = 0.25 , get_raw_u = False ):    \n",
    "    #b ,c ,w = datas.shape\n",
    "    #for bdata in datas:\n",
    "    \n",
    "    u = bdata[0] \n",
    "    vt = bdata[1]\n",
    "    vb = bdata[2]\n",
    "    du = bdata[3]\n",
    "    dvt = bdata[4]\n",
    "    dvb = bdata[5]        \n",
    "    pk_idxs =[]\n",
    "    #masks=[]\n",
    "    #real_gt_u=[]\n",
    "    for u_grad in u:\n",
    "        #pk_idx = find_N_peaks(u_grad.cpu().detach().numpy() , r=r , min_v = u_thresh )[0]\n",
    "        u_grad = torch.sigmoid (u_grad)    \n",
    "        pk_idx = find_N_peaks(u_grad.cpu().detach().numpy() , r= R , min_v = u_thresh )        \n",
    "        pk_idxs.append(pk_idx[0])\n",
    "     \n",
    "    \n",
    "    b = len(u)    \n",
    "    us =[]\n",
    "    vts = []\n",
    "    vbs = []\n",
    "    scores= []\n",
    "    for i in range(b):        \n",
    "        du[i]  = torch.exp(du[i])\n",
    "        pk_idx = pk_idxs[i]        \n",
    "        with torch.no_grad():\n",
    "            sig_u = torch.sigmoid(u[i][pk_idx])\n",
    "        #scores.append(u[i][pk_idx])\n",
    "        \n",
    "        scores.append(sig_u)\n",
    "\n",
    "        real_gt_u = pk_idx/MAX_PREDICTION_COUNT\n",
    "        real_gt_u = torch.from_numpy(real_gt_u).to(device)\n",
    "        if not get_raw_u:\n",
    "            #_u = torch.hstack(( real_gt_u[i] , real_gt_u[i]+ du[i] * masks[i] )) % 1            \n",
    "            #_u = torch.cat(( real_gt_u[i].float() , real_gt_u[i].float()+ du[i].float() * masks[i].float() ),0) % 1\n",
    "            _u = torch.cat(( real_gt_u.float() , real_gt_u.float()  + du[i][pk_idx].float()   ),0) % 1\n",
    "        else:\n",
    "            #_u = torch.hstack(( real_gt_u[i] , real_gt_u[i]+ du[i] * masks[i] ))\n",
    "            #_u = torch.cat(( real_gt_u[i].float() , real_gt_u[i].float()+ du[i].float() * masks[i].float() ),0)         \n",
    "            _u = torch.cat(( real_gt_u .float() , real_gt_u.float()  + du[i][pk_idx].float()   ),0)\n",
    "\n",
    "        vt1 = 0.5 - torch.exp(vt[i][pk_idx]) \n",
    "        vt2 = 0.5 - torch.exp(dvt[i][pk_idx]) \n",
    "\n",
    "        vb1 = torch.exp(vb[i][pk_idx]) +0.5\n",
    "        vb2 = torch.exp(dvb[i][pk_idx]) +0.5\n",
    "\n",
    "        #_vt = torch.hstack((vt1 , vt2 ))    \n",
    "        #_vb = torch.hstack(( vb1, vb2))\n",
    "        _vt = torch.cat((vt1 , vt2 ))    \n",
    "        _vb = torch.cat(( vb1, vb2))\n",
    "        \n",
    "        us.append(_u)\n",
    "        vts.append(_vt)\n",
    "        vbs.append(_vb)\n",
    "\n",
    "    #u_grads = get_grad_u_keep_batch(us , True)\n",
    "\n",
    "    #return u,vt,vb , u_grad\n",
    "    #return us ,vts,vbs , u_grads\n",
    "    return us ,vts,vbs ,scores\n",
    "\n",
    "def uv_to_xyz(u,v):\n",
    "    uu = ( u*360-180) * 0.01745\n",
    "    vv = ( v*180 -90) * 0.01745        \n",
    "    \n",
    "    # uv to 3D\n",
    "    x =  np.cos(uu) * np.cos(vv)    \n",
    "    y =  np.sin(uu) * np.cos(vv)\n",
    "    z =  np.sin(vv)\n",
    "    return x,y,z\n",
    "\n",
    "def xyz_to_uv(x,y,z):\n",
    "    theta   = np.arctan2(y,x)\n",
    "    phi     = np.arcsin(z/(np.sqrt(x**2 +y**2+z**2)))\n",
    "\n",
    "    theta   = (theta/ 0.01745 +180)/360\n",
    "    phi     = (phi/0.01745 + 90)/180\n",
    "    return theta,phi\n",
    "    \n",
    "def interplate_uv(u,v , count = 20):\n",
    "    xs,ys,zs =[],[],[]\n",
    "\n",
    "    for uu,vv in zip( u, v ):                    \n",
    "        x,y,z = uv_to_xyz(uu,vv)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "\n",
    "    # 插值\n",
    "    '''\n",
    "    intp_x  = np.linspace(np.min(xs) , np.max(xs) , num=count )\n",
    "    intp_y  = np.linspace(np.min(ys) , np.max(ys) , num=count )\n",
    "    intp_z  = np.linspace(np.min(zs) , np.max(zs), num=count )\n",
    "    '''\n",
    "    intp_x  = np.linspace(xs[0] , xs[1] , num=count )\n",
    "    intp_y  = np.linspace(ys[0] , ys[1] , num=count )\n",
    "    intp_z  = np.linspace(zs[0] , zs[1], num=count )\n",
    "\n",
    "    # 3D to uv\n",
    "    thetas  =[]\n",
    "    phis    =[]\n",
    "    for x,y,z in zip (intp_x, intp_y, intp_z):\n",
    "        theta , phi = xyz_to_uv(x,y,z)\n",
    "        thetas.append(theta)\n",
    "        phis.append(phi)\n",
    "    return thetas , phis\n",
    "\n",
    "def to_distorted_box(u,vt,vb , image = None  ,seg_cnt = None):\n",
    "\n",
    "    canvas = np.zeros((512,1024,3)) if image is None else image    \n",
    "    polys_per_img = []  #這張image的門，每個門可能有數個部位\n",
    "    \n",
    "    for _u , _vt , _bv  in zip(u, vt , vb):\n",
    "        cross_idx = -1        \n",
    "        previous_x = 0        \n",
    "        seg_count =max(int((_u[1] -_u[0])*1024)//10 , 5) if seg_cnt is None else seg_cnt\n",
    "        \n",
    "        # Upper line\n",
    "        all_points = [[None]*seg_count*2][0]\n",
    "        thetas , phis= interplate_uv(_u,_vt , seg_count)\n",
    "        i=0\n",
    "        for t, p in zip(thetas , phis):            \n",
    "            canvas = cv2.circle(canvas , (int(t*1024) , int(p *512)) , 3 , (255,0,0) ,-1 )            \n",
    "\n",
    "            if(t< previous_x):\n",
    "                cross_idx = i \n",
    "            \n",
    "            previous_x = t\n",
    "            all_points[i] = [t , p]\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        # Bottom line\n",
    "        thetas , phis= interplate_uv(_u,_bv , seg_count)        \n",
    "        i=1\n",
    "        for t, p in zip(thetas , phis):\n",
    "            canvas = cv2.circle(canvas , (int(t*1024) , int(p *512)) , 3 , (255,0,0) ,-1 )            \n",
    "            all_points[len(all_points)- i] = [t , p]\n",
    "            i+=1\n",
    "        \n",
    "        # check cross      \n",
    "        if(cross_idx >0):\n",
    "            left_start_top = [0 ,all_points[cross_idx][1] ]\n",
    "            left_start_btm = [0 ,all_points[seg_count + cross_idx][1] ]\n",
    "            \n",
    "            right_mid_top = [1 ,all_points[ cross_idx][1] ]\n",
    "            right_mid_btm = [1 ,all_points[seg_count + cross_idx][1] ]\n",
    "\n",
    "            part_right = all_points[:cross_idx] +[right_mid_top] +[right_mid_btm]+ all_points[seg_count + (seg_count - cross_idx):]\n",
    "            #part_left = all_points[cross_idx: seg_count + (seg_count - cross_idx)]             \n",
    "            part_left = [left_start_top] + \\\n",
    "                all_points[cross_idx: seg_count + (seg_count - cross_idx)] + [left_start_btm]            \n",
    "\n",
    "            # ============= Clipping ===============\n",
    "            \n",
    "            part_left = np.array(part_left)\n",
    "            part_right = np.array(part_right)\n",
    "            right_min = part_right[0][0]\n",
    "            right_clip_idx = np.where(part_right.flatten()[::2]<right_min)[0]\n",
    "\n",
    "            if (right_clip_idx.size > 0):\n",
    "                part_right[right_clip_idx,0]=1+0.00001\n",
    "\n",
    "            left_max = part_left[len(part_left)//2][0] \n",
    "            left_clip_idx = np.where(part_left.flatten()[::2] > left_max)[0]\n",
    "\n",
    "\n",
    "            '''\n",
    "            print(\"all_points\" , all_points)\n",
    "            print(\"cross_idx\" , cross_idx)\n",
    "            print(\"seg_count\" , seg_count)\n",
    "            print(\"part_left\" , part_left)\n",
    "            print(\"part_right\" , part_right)\n",
    "            '''\n",
    "\n",
    "            if (left_clip_idx.size > 0):\n",
    "                part_left[left_clip_idx,0]=left_max+0.0001\n",
    "            # ============= Clipping ===============\n",
    "\n",
    "            polys = [part_left , part_right]\n",
    "\n",
    "        else:\n",
    "            polys = [all_points]\n",
    "\n",
    "        polys_per_img.append(polys)\n",
    "    '''\n",
    "    # [DEBUG--- Show Result]\n",
    "        for poly in polys:            \n",
    "            poly = np.array(poly)\n",
    "            poly = poly.reshape((-1 , 2)) * np.tile(np.array([1024 , 512]) , (poly.size//2 , 1) )\n",
    "            poly = poly.astype('int32')               \n",
    "            canvas =  cv2.polylines(canvas, [poly], True, (0,255,0), 2)\n",
    "\n",
    "    plt.imshow(canvas)\n",
    "    plt.show()\n",
    "    '''\n",
    "    return polys_per_img\n",
    "\n",
    "def rearng(x):    \n",
    "    half_idx = len(x)//2\n",
    "    x1= x[:half_idx]\n",
    "    x2= x[half_idx:]\n",
    "    if(isinstance(x , np.ndarray)):\n",
    "        arr = np.zeros_like(x)\n",
    "    else:\n",
    "        arr = torch.zeros_like(x)\n",
    "    arr[::2] = x1    \n",
    "    arr[1::2] = x2      \n",
    "\n",
    "    return arr\n",
    "def rearrange_decoded(u,vt,vb):    \n",
    "    us ,vts ,vbs= [],[],[]\n",
    "    for batch_u , batch_vt , batch_vb in zip(u,vt,vb):   \n",
    "        if(isinstance(batch_u , np.ndarray)):\n",
    "            ru = rearng(batch_u)        \n",
    "            rvt = rearng(batch_vt)\n",
    "            rvb = rearng(batch_vb)\n",
    "        else:\n",
    "            ru = rearng(batch_u.detach().cpu().numpy())        \n",
    "            rvt = rearng(batch_vt.detach().cpu().numpy())\n",
    "            rvb = rearng(batch_vb.detach().cpu().numpy())\n",
    "\n",
    "        us.append(ru)\n",
    "        vts.append(rvt)\n",
    "        vbs.append(rvb)\n",
    "    \n",
    "    return us,vts,vbs\n",
    "\n",
    "'''\n",
    "boxu = [np.array([0.9556, 0.9921])]\n",
    "boxvt = [np.array([0.3810, 0.4109])]\n",
    "boxvb = [np.array([0.7335, 0.6852])]\n",
    "'''\n",
    "#Cross Image Set\n",
    "boxu = [np.array([0.7444, 1.0161])]\n",
    "boxvt = [np.array([0.2220, 0.2190])]\n",
    "boxvb = [np.array([0.8964, 0.8982])]\n",
    "'''\n",
    "\n",
    "# Multi door\n",
    "boxu = [np.array([0.8111, 0.3778, 1.2295, 0.6528])]\n",
    "boxvt = [np.array([0.4642, 0.2425, 0.4658, 0.2188])]\n",
    "boxvb = [np.array([0.5735, 0.8770, 0.5703, 0.8927])]\n",
    "'''\n",
    "\n",
    "gt_boxu = [ np.array([0.7111, 0.3578, 1.1095, 0.6528])]\n",
    "gt_boxvt =[ np.array([0.4642, 0.2425, 0.4658, 0.2188])]\n",
    "gt_boxvb =[ np.array([0.5735, 0.8770, 0.5703, 0.8927])]\n",
    "\n",
    "'''\n",
    "boxu = [np.array([0.1222, 0.6778, 0.7111, 0.1364, 0.6922, 0.7348])]\n",
    "boxvt = [np.array([0.4698, 0.4242, 0.4627, 0.4723, 0.4376, 0.4620])]\n",
    "boxvb = [np.array([0.5649, 0.6636, 0.5813, 0.5593, 0.6373, 0.5829])]\n",
    "'''\n",
    "boxu , boxvt , boxvb = rearrange_decoded(boxu , boxvt, boxvb)\n",
    "gt_boxu , gt_boxvt , gt_boxvb = rearrange_decoded(gt_boxu , gt_boxvt, gt_boxvb)\n",
    "#gt = [(0.8 , 0.23) , (1.1 , 0.23)  , (1.1 , 0.8) , (0.8 , 0.75) ]  # todo: gt不會cross borader\n",
    "from shapely.validation import make_valid,explain_validity\n",
    "def cal_poly_iou(poly_a , poly_b):\n",
    "    \n",
    "    if( len(poly_a) ==1 and len(poly_b) ==1): #比對的兩扇門都沒有跨畫面\n",
    "        a_pg = Polygon(poly_a[0])\n",
    "        b_pg = Polygon(poly_b[0])\n",
    "        \n",
    "        a_pg.buffer(0.0001)\n",
    "        a_pg = a_pg.simplify(0.0001 ,preserve_topology=False)\n",
    "        b_pg.buffer(0.0001)\n",
    "        b_pg = b_pg.simplify(0.0001 , preserve_topology=False)\n",
    "\n",
    "        poly_intersection   = a_pg.intersection(b_pg)\n",
    "        poly_union          = a_pg.union(b_pg)\n",
    "        if( poly_union.area== 0):\n",
    "            iou=0\n",
    "        else :\n",
    "            iou                 = poly_intersection.area / poly_union.area\n",
    "        #print(\"iou\" , iou)\n",
    "        return iou\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        iou_matrix = np.zeros((len(poly_a) , len(poly_b)))      \n",
    "        for i , a_points in enumerate( poly_a):\n",
    "            a_pg = Polygon(a_points)\n",
    "            a_pg.buffer(0.0001)\n",
    "            a_pg = a_pg.simplify(0.001 ,preserve_topology=False)\n",
    "\n",
    "            if(not a_pg.is_valid):                \n",
    "                a_pg = make_valid(a_pg)\n",
    "                print(\"a\",a_pg.is_valid , explain_validity(a_pg))\n",
    "\n",
    "            for j , b_points in enumerate( poly_b):\n",
    "                b_pg = Polygon(b_points)\n",
    "                b_pg.buffer(0.0001)\n",
    "                b_pg = b_pg.simplify(0.001 , preserve_topology=False)\n",
    "\n",
    "                if(not b_pg.is_valid):\n",
    "                    b_pg = make_valid(b_pg)\n",
    "                    print(\"b\" , b_pg.is_valid , explain_validity(b_pg))\n",
    "                \n",
    "                poly_intersection   = a_pg.intersection(b_pg)\n",
    "                poly_union          = a_pg.union(b_pg)\n",
    "                if( poly_union.area== 0):\n",
    "                    iou =0\n",
    "                else:                \n",
    "                    iou                 = poly_intersection.area / poly_union.area\n",
    "                iou_matrix[i][j] =  np.float32( iou)\n",
    "                #print(\"iou    \" ,iou    )\n",
    "        total_iou = np.sum(iou_matrix)/2\n",
    "        #print(\"iou_matrix\" , iou_matrix)\n",
    "        #print(\"total iou\" , total_iou)\n",
    "\n",
    "        return total_iou\n",
    "    pass\n",
    "\n",
    "def get_iou_matrix_distored(gt , pred):    \n",
    "    iou_matrix = np.zeros((len(gt) , len(pred)))        \n",
    "    for i , _gt in  enumerate(gt):        \n",
    "        for j , _pred in enumerate(pred):            \n",
    "            iou  = cal_poly_iou(_gt, _pred)\n",
    "            iou_matrix[i][j] = np.float32( iou)\n",
    "    \n",
    "    return iou_matrix\n",
    "\n",
    "\n",
    "for batched_uvv in zip(boxu , boxvt , boxvb ):  #each image in batch\n",
    "    #print(\"batched_uvv\" , batched_uvv)\n",
    "    u = batched_uvv[0].reshape(-1,2)\n",
    "    vt = batched_uvv[1].reshape(-1,2)\n",
    "    vb = batched_uvv[2].reshape(-1,2)    \n",
    "\n",
    "    polys_point = to_distorted_box(u , vt , vb)\n",
    "\n",
    "    u   = gt_boxu[0].reshape(-1,2)\n",
    "    vt  = gt_boxvt[0].reshape(-1,2)\n",
    "    vb  = gt_boxvb[0].reshape(-1,2)    \n",
    "\n",
    "    gt_polys_point = to_distorted_box(u , vt , vb,seg_cnt=5)\n",
    "    iou_matrix = get_iou_matrix_distored(gt_polys_point , polys_point)\n",
    "    print(\"final iou_matrix\",iou_matrix)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wdo_3D2pixel_withbc(wdo_bbox_3D, wdo_bbox_pixel ):\n",
    "    horizontal_start = np.array([0, -1, 0])\n",
    "    vertical_start = np.array([0, 0, 1])\n",
    "  \n",
    "    wdo_bbox_3D_check = list(wdo_bbox_3D.copy())\n",
    "\n",
    "    # project onto xy plane \n",
    "    #np_bbox_3d = np_bbox_3d[:,]/np.linalg.norm(np_bbox_3d[:,])   \n",
    "    # 減去高度，計算在xy平面到中心點的距離\n",
    "    \n",
    "    #for point in wdo_bbox_3D:\n",
    "    for point in wdo_bbox_3D_check:        \n",
    "        tmp_horizontal = np.array([point[0], point[1], 0])\n",
    "        horizontal_theta = getTheta(tmp_horizontal, horizontal_start)\n",
    "        print(\"horizontal_theta\" , horizontal_theta)\n",
    "        if point[0] > 0:\n",
    "            horizontal_theta *= -1        \n",
    "            pass\n",
    "            \n",
    "        vertical_theta = getTheta(point, vertical_start)\n",
    "        horizontal_pixel = getPixel(horizontal_theta, 360, 1024)\n",
    "        if point[0] > 0:\n",
    "            horizontal_pixel += 1024\n",
    "\n",
    "        vertical_pixel = getPixel(vertical_theta, 180, 512)\n",
    "        tmp_np = np.around(np.array([horizontal_pixel, vertical_pixel]).astype(np.float32), decimals=2)\n",
    "        wdo_bbox_pixel.append(tmp_np.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義dataloader\n",
    "# [Load Data]\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Lambda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import random_split\n",
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "IMG_SIZE =  np.array([1024,512]) # [w,h]\n",
    "#MAX_PREDICTION_COUNT = 256\n",
    "#MAX_PREDICTION_COUNT = 20\n",
    "\n",
    "#EPSILON = 0.00001\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "    \n",
    "debug_current_imgs_path =[\"\"]* BATCH_SIZE \n",
    "debug_count=0\n",
    "\n",
    "def gauss_noise_tensor(img):\n",
    "    if np.random.rand() < 0.5 and DO_AUG:\n",
    "        sigma = np.random.rand() *0.125\n",
    "        out = img + sigma * torch.randn_like(img)\n",
    "        return out\n",
    "    return img\n",
    "\n",
    "def blank(img):    \n",
    "    return img\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    global debug_current_imgs_path\n",
    "    def __init__(self, annotations_file_path , img_size=[1024,512] , debug_doTrans= True):\n",
    "        # Open json file        \n",
    "        json_path =  annotations_file_path\n",
    "        f= open(json_path)\n",
    "        anno = json.loads(f.read())\n",
    "        f.close()\n",
    "        self.anno = anno\n",
    "        self.img_size = img_size\n",
    "\n",
    "        do_jitter = np.random.rand() > 0.5 if DO_AUG else False        \n",
    "        self.debug_doTrans = debug_doTrans\n",
    "        \n",
    "        self.transform = transforms.Compose([    \n",
    "            transforms.ToPILImage(),                    \n",
    "            transforms.Resize((img_size[1], img_size[0])),            \n",
    "            transforms.ColorJitter((0.4 , 1) , (0.7,1) , (0.6,1) , (-0.5, 0.5)) if self.debug_doTrans else blank,\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),            \n",
    "            gauss_noise_tensor if self.debug_doTrans else blank,\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anno)\n",
    "    def get_bbox_count(self ):\n",
    "        count =0\n",
    "        for data in self.anno:            \n",
    "            count += len(data['bboxes'])\n",
    "        return count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global epoch\n",
    "        global train_itr\n",
    "\n",
    "        #img_path = os.path.join(\"./\", self.anno[idx]['image'])\n",
    "        img_path = os.path.join(\"/CGVLAB3/datasets/chingjia/data/data/\", self.anno[idx]['image'])\n",
    "        #img_path = os.path.join(\"./data/data/\", self.anno[idx]['image'])\n",
    "        #print(img_path)\n",
    "        #image = read_image(img_path)           \n",
    "        image = cv2.imread(img_path)        \n",
    "        #plt.imshow(image)\n",
    "        #plt.title(\"origin img\")\n",
    "        #plt.show()\n",
    "\n",
    "        if(self.transform!=None):\n",
    "            image= self.transform( image)\n",
    "\n",
    "        if self.debug_doTrans:\n",
    "            do_flip = np.random.rand() > 0 \n",
    "            do_roll = np.random.rand() > 0       \n",
    "        else:   \n",
    "            do_flip=False\n",
    "            do_roll=False\n",
    "\n",
    "        _, h,w = image.shape\n",
    "\n",
    "        target = self.anno[idx]\n",
    "        data = {}\n",
    "\n",
    "        u = torch.tensor(target['u'])\n",
    "        u_0idx = torch.where(u==0)[0]  #避免u = 0在match上出error\n",
    "        u[u_0idx] = 0.0000001\n",
    "        \n",
    "        v = torch.tensor(target['sticks_v'])        \n",
    "        du = u.flatten()[1::2] - u.flatten()[0::2]\n",
    "\n",
    "        #============ Transform ===========\n",
    "        if do_flip:\n",
    "            image = torch.flip(image, dims=[2])            \n",
    "            u = torch.flip( 1 - u , [1])\n",
    "            u_is_cross =  (u.flatten()[::2]<0).to(torch.float32)\n",
    "            u_is_cross = u_is_cross.repeat_interleave(2)            \n",
    "            u = (u.flatten() + 1* u_is_cross).reshape((-1,2))\n",
    "\n",
    "            # 左右交換 => 0跟2互換 , 1跟3互換\n",
    "            v_idx_all = torch.arange(v.numel())      \n",
    "            v_idx = v_idx_all.clone()\n",
    "            v_idx[0::4] = v_idx_all[2::4]\n",
    "            v_idx[1::4] = v_idx_all[3::4]\n",
    "            v_idx[2::4] = v_idx_all[0::4]\n",
    "            v_idx[3::4] = v_idx_all[1::4]\n",
    "            v= (v.flatten()[v_idx]).reshape(-1,4)\n",
    "\n",
    "        if do_roll:\n",
    "            shift_rand = torch.rand(1)\n",
    "            shift = int((w * shift_rand ).tolist()[0])\n",
    "            image = torch.roll(image , shift , 2 )            \n",
    "            u = (u + shift_rand) % 1\n",
    "        #============            ===========        \n",
    "        u_grad = get_grad_u(u.flatten()[::2].reshape((-1,1)) , _width=MAX_PREDICTION_COUNT)        \n",
    "        u_grad = torch.max(u_grad,0)[0]  \n",
    "        \n",
    "        #padding_count = (MAX_PREDICTION_COUNT - u.shape[0])\n",
    "        padding_count = (MAX_PREDICTION_COUNT - u.numel()//2)\n",
    "        padding_count = max(padding_count ,  0)        \n",
    "        \n",
    "        padding_count = abs( MAX_PREDICTION_COUNT*2 - u.numel())\n",
    "        u_pad = torch.cat(( u.reshape(-1) , torch.zeros((padding_count )) )  )                \n",
    "        du_pad = torch.cat(( du.reshape(-1) , torch.zeros((MAX_PREDICTION_COUNT - du.numel() )) )  )                \n",
    "\n",
    "        \n",
    "        v= v.flatten()        \n",
    "        \n",
    "        padding_count = abs( MAX_PREDICTION_COUNT *4 - v.numel() )\n",
    "        v_top_pad = torch.cat(( v[::2] , torch.zeros((padding_count//2 )) )  )        \n",
    "        v_btm_pad = torch.cat(( v[1::2] , torch.zeros((padding_count//2 )) )  )      \n",
    "        \n",
    "\n",
    "        #data['data_count'] = u.numel()\n",
    "        data['image'] = image\n",
    "        #data['u_grad'] = c_dist\n",
    "        data['u_grad'] = u_grad\n",
    "        data['u'] = u_pad [::2]\n",
    "        #data['door_count'] = u.numel()//2\n",
    "        #data['u'] = u_pad\n",
    "        data['v_top'] = v_top_pad[::2]\n",
    "        data['v_btm'] = v_btm_pad[::2]\n",
    "\n",
    "\n",
    "        #data['du'] = torch.abs( u_pad[1::2] - u_pad[0::2])\n",
    "        data['du'] = du_pad\n",
    "        \n",
    "        data['dv_top'] = v_top_pad[1::2]\n",
    "        data['dv_btm'] = v_btm_pad[1::2]\n",
    "\n",
    "        '''\n",
    "        data['d_top'] = depth_top_pad\n",
    "        data['d_btm'] = depth_btm_pad\n",
    "\n",
    "        '''\n",
    "\n",
    "        #=====================\n",
    "        #|    output shape   |\n",
    "        #=====================\n",
    "        #   u: [n,2]\n",
    "        #   u_grad: [n,1024]\n",
    "        #   v_top: [n]\n",
    "        #   v_btm: [n]\n",
    "\n",
    "        return data\n",
    "\n",
    "        #return data        \n",
    "\n",
    "# [ Test  : Can Delete]\n",
    "'''\n",
    "#dataset = CustomDataset( \"./output/train_visiable_horizon_2k.json\"   ) \n",
    "#dataset = CustomDataset( \"./output/train_visiable_horizon_200.json\"   ) \n",
    "train_dataset = CustomDataset( f\"./output/{TRAIN_DATASET_NAME}\"  ,debug_doTrans= False ) \n",
    "#eval_dataset = CustomDataset( f\"./output/{TEST_DATASET_NAME}\"   ) \n",
    "#eval_dataset = CustomDataset( \"./output/train_visiable_horizon_20.json\"   ) \n",
    "#train_dataloader = DataLoader(train_dataset, BATCH_SIZE , shuffle=True, drop_last =True)\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE , shuffle=False, drop_last =True)\n",
    "#eval_dataloader = DataLoader(eval_dataset, BATCH_SIZE , shuffle=True, drop_last =True)\n",
    "\n",
    "data = next(iter(train_dataloader)) \n",
    "#pack_gt = (data['u'] , data['v_top'] , data['v_btm'] , data['d_top'] , data['v_top'] )\n",
    "print(data['u'])\n",
    "#print(\"vtop\" ,data['v_top'])\n",
    "#print(\"vbtm\" ,data['v_btm'])\n",
    "print(\"du\" , data['du'])\n",
    "#print(data)\n",
    "print(data['u_grad'])\n",
    "visualize_2d(     \n",
    "    data['u'],\n",
    "    data['v_top'],\n",
    "    data['v_btm'],\n",
    "    data['image'],\n",
    "    data['u_grad'],\n",
    "    #out_u\n",
    "    #get_grad_u_keep_batch( real_gt_u)\n",
    ")\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "def to_polybox(u_pack , vt_pack , vb_pack):\n",
    "    \n",
    "    batches = []\n",
    "    for u,vt,vb in zip(u_pack , vt_pack , vb_pack):\n",
    "        '''\n",
    "    # input:\n",
    "    u_flatten  = torch.cat(u_pack)\n",
    "    vt_flatten  = torch.cat(vt_pack)\n",
    "    vb_flatten  = torch.cat(vb_pack)\n",
    "        '''\n",
    "\n",
    "        non_zero_idx = torch.where(u>0)[0]\n",
    "        u_flatten = u[non_zero_idx].cpu().detach()\n",
    "        vt_flatten = vt[non_zero_idx].cpu().detach()\n",
    "        vb_flatten = vb[non_zero_idx].cpu().detach()\n",
    "\n",
    "        poly_boxes= []\n",
    "        box_count = u_flatten.shape[0]//2\n",
    "        for i in range(box_count):\n",
    "            _box = ( \n",
    "                u_flatten[2*i] , vt_flatten[2*i] , # top left\n",
    "                u_flatten[2*i+1] , vt_flatten[2*i+1] , # top right\n",
    "                u_flatten[2*i+1] , vb_flatten[2*i+1] , # btm right\n",
    "                u_flatten[2*i] , vb_flatten[2*i] , # left btm\n",
    "            )\n",
    "            poly_boxes.append(torch.as_tensor(_box))\n",
    "        batches.append(poly_boxes)\n",
    "\n",
    "    return batches\n",
    "\n",
    "def uv_to_distorted_box(u,vt,vb):\n",
    "    polys =[]\n",
    "    _boxu , _boxvt , _boxvb = rearrange_decoded(u, vt , vb)\n",
    "    for bu,bvt,bvb  in zip(_boxu , _boxvt , _boxvb ):    \n",
    "        _u = bu.reshape(-1,2)\n",
    "        _vt = bvt.reshape(-1,2)\n",
    "        _vb = bvb.reshape(-1,2)    \n",
    "\n",
    "        p =to_distorted_box(_u , _vt , _vb )\n",
    "        polys.append(p)\n",
    "        \n",
    "    return polys\n",
    "\n",
    "class PR_Eval_Helper():    \n",
    "    def __init__(self, writer=None , ep=0):\n",
    "        self.iou_thresh = [0.05,0.5,0.75]\n",
    "        self.gt_count=0\n",
    "        self.all_iou=0\n",
    "        self.results_per_batch = [{\"tp\":[],\"fp\":[],\"scores\":[]} for _ in self.iou_thresh]\n",
    "        self.writer = writer\n",
    "        self.ep = ep\n",
    "        pass\n",
    "    \n",
    "    #==== [ Not Using ] =======\n",
    "    def get_iou_matrix(self):\n",
    "        # row: prediction\n",
    "        # col: gt\n",
    "        #iou_matrix = np.zeros((self.gt.shape[0] , self.predict_result.shape[0]))\n",
    "        iou_matrix = np.zeros((len(self.gt) , len(self.predict_result)))        \n",
    "        \n",
    "        for i , gt in enumerate(self.gt):            \n",
    "            polygon_gt = Polygon (gt.reshape((-1,2)))     \n",
    "            polygon_gt.buffer(0.01)     \n",
    "            for j , pred in enumerate(self.predict_result):                \n",
    "                polygon_pred = Polygon( pred.reshape((-1,2)))  \n",
    "                polygon_pred.buffer(0.01)     \n",
    "                try:\n",
    "                #if(polygon_pred.is_valid):\n",
    "                    poly_intersection   = polygon_pred.intersection(polygon_gt)\n",
    "                    poly_union          = polygon_pred.union(polygon_gt)\n",
    "                    iou                 = poly_intersection.area / poly_union.area\n",
    "                except:\n",
    "                #else:\n",
    "                    print(\"error\",polygon_pred )\n",
    "                    print(\"error origin\", pred)\n",
    "                    print(\"error gt\", polygon_gt)\n",
    "                    iou=0\n",
    "                iou_matrix[i][j] = np.float32( iou)\n",
    "                pass       \n",
    "        return iou_matrix\n",
    "\n",
    "    def eval_batch_pr( self, predict_result , gt , score ,  _debug_iteration =0 ):        \n",
    "        '''\n",
    "        #依照score排序                \n",
    "        '''\n",
    "        self.scores = score\n",
    "        self.predict_result = predict_result\n",
    "        self.gt = gt\n",
    "        \n",
    "        self.gt_count+= len(gt)              \n",
    "            \n",
    "        #iou_matrix = self.get_iou_matrix()\n",
    "        iou_matrix = get_iou_matrix_distored(gt,predict_result)\n",
    "        #print(\"iou_matrix\" , iou_matrix)\n",
    "        \n",
    "        pred_count = len(self.predict_result)\n",
    "        \n",
    "        for i,iou_thersh in enumerate( self.iou_thresh):\n",
    "            #print(\"iou_thersh\", iou_thersh)\n",
    "            #if self.predict_result.shape[0] > 1:\n",
    "            if pred_count >= 1:\n",
    "                iou_thresh_mask = np.where(iou_matrix >= iou_thersh , 1 , 0 )                \n",
    "                masked_iou_matrix = iou_matrix * iou_thresh_mask                \n",
    "                each_gt_best_iou_idx = np.argmax(masked_iou_matrix , axis=1)                \n",
    "                \n",
    "                # 過濾掉同個gt box有多個符合的box，只保留最好的\n",
    "                mono_gt = np.zeros_like(iou_matrix)\n",
    "                mono_gt[[i for i in range(len(mono_gt))] , [each_gt_best_iou_idx]] = iou_matrix[[i for i in range(len(mono_gt))] , [each_gt_best_iou_idx]]\n",
    "                # 紀錄miou\n",
    "                if(i==0):\n",
    "                    self.all_iou += np.sum(mono_gt)\n",
    "\n",
    "                mono_gt *= iou_thresh_mask  # 沒達到threshold的設為0\n",
    "\n",
    "                #找到單列最好的 (?)\n",
    "                pred_filtered_mask = np.zeros(pred_count)                \n",
    "                for row_gt in mono_gt:\n",
    "                    _max_idx = np.argmax(row_gt)\n",
    "                    if( row_gt[_max_idx]>0 ):\n",
    "                        pred_filtered_mask[_max_idx]=1                                                                \n",
    "                \n",
    "                tp_list = np.where(pred_filtered_mask > 0 , 1 , 0 )   \n",
    "                fp_list = np.where(pred_filtered_mask ==0 , 1 , 0 )    \n",
    "                \n",
    "                #print(\"batch append tp\" , tp_list.shape , tp_list)\n",
    "                #print(\"batch append self.scores\" , self.scores)\n",
    "                self.results_per_batch[i]['tp'].append(tp_list.flatten())\n",
    "                self.results_per_batch[i]['fp'].append(fp_list.flatten())\n",
    "                self.results_per_batch[i]['scores'].append(self.scores)    \n",
    "                \n",
    "            else:\n",
    "                batch_ap = 0\n",
    "                \n",
    "            '''\n",
    "                batch_pr , batch_rc , batch_ap = self.list_to_pr_auc(tp_list, fp_list , len(self.gt))\n",
    "                if self.writer is not None:\n",
    "                    self.writer.add_scalars(\"batch_eval\",{f\"ap_iou_{iou_thersh}\": batch_ap} ,  _debug_iteration)\n",
    "                    self.writer.close()  \n",
    "                print(f\"batch ap_{iou_thersh}\",batch_ap)\n",
    "            elif pred_count == 1:               \n",
    "                #filter_mask = iou_matrix.flatten() > \n",
    "                iou_thresh_mask = np.where(iou_matrix >= iou_thersh , 1 , 0 )    \n",
    "                tp_list = np.where(iou_thresh_mask > 0 , 1 , 0 )   \n",
    "                fp_list = np.where(iou_thresh_mask ==0 , 1 , 0 )   \n",
    "                \n",
    "                self.results_per_batch[i]['tp'].append(tp_list.flatten())\n",
    "                self.results_per_batch[i]['fp'].append(fp_list.flatten())             \n",
    "                self.results_per_batch[i]['scores'].append(self.scores)                       \n",
    "            '''\n",
    "\n",
    "    \n",
    "    def list_to_pr_auc(self, tp_list , fp_list , gt_count):\n",
    "        tp = np.cumsum(tp_list)     \n",
    "        fp = np.cumsum(fp_list) \n",
    "        all_prediction = tp+fp\n",
    "        precision = tp / all_prediction\n",
    "        recall = tp / gt_count            \n",
    "        self.all_prediction = precision\n",
    "        \n",
    "        recall = np.insert(recall , 0 , 0)\n",
    "        precision = np.insert(precision , 0 , 1)\n",
    "\n",
    "        print(\"gt_count\" , gt_count)\n",
    "        #print(\"tp\" , tp)\n",
    "        #print(\"fp\" , fp)\n",
    "\n",
    "        print(\"all_prediction\" , all_prediction)\n",
    "        print(\"recall\" , recall)\n",
    "        print(\"precision\" , precision)\n",
    "\n",
    "        auc = metrics.auc(recall,precision)\n",
    "        return precision , recall,auc\n",
    "    \n",
    "    def get_all_pr(self):\n",
    "        # combine each batch result and sort by scores\n",
    "        # print(\"self.results_per_batch\", self.results_per_batch)\n",
    "        self.final_result_dict =  [{} for _ in self.iou_thresh]        \n",
    "        for i , thresh in enumerate(self.iou_thresh):\n",
    "            #print(\"len\" , len(self.results_per_batch[i]['scores']))\n",
    "            if len(self.results_per_batch[i]['scores']) > 0 :            \n",
    "                all_tp = np.concatenate(self.results_per_batch[i]['tp'][:])\n",
    "                all_fp = np.concatenate(self.results_per_batch[i]['fp'][:])\n",
    "\n",
    "                sum_tp = np.sum(all_tp)\n",
    "                sum_fp = np.sum(all_fp)\n",
    "\n",
    "                recall_rate = sum_tp / self.gt_count\n",
    "                precision_rate = sum_tp / (sum_tp + sum_fp)\n",
    "                \n",
    "            if len(self.results_per_batch[i]['scores']) >1 :                        \n",
    "                precision , recall , auc = self.list_to_pr_auc(all_tp , all_fp , self.gt_count)\n",
    "            else:\n",
    "                precision = []\n",
    "                recall=[]\n",
    "                auc=0\n",
    "                recall_rate =0\n",
    "                precision_rate=0\n",
    "\n",
    "            self.final_result_dict[i]=(\n",
    "                {\"iou_thresh\": thresh ,\n",
    "                    \"recall\":recall,\n",
    "                    \"precision\":precision,\n",
    "                    \"recall_rate\": recall_rate,\n",
    "                    \"precision_rate\": precision_rate,\n",
    "                    \"ap\":auc} \n",
    "            )\n",
    "            print(f\"ap_{thresh}\",auc)\n",
    "        if self.writer is not None:\n",
    "            self.write_tensorboard()\n",
    "        \n",
    "        return precision,recall,auc\n",
    "    \n",
    "    def write_tensorboard(self, subName =\"sub\"):        \n",
    "        writer = self.writer\n",
    "        for i , thresh in enumerate(self.iou_thresh):            \n",
    "            #prcs = np.ascontiguousarray(self.final_result_dict[i][\"precision\"] )\n",
    "            #recs = np.ascontiguousarray(self.final_result_dict[i][\"recall\"] )\n",
    "            prcs = self.final_result_dict[i][\"precision\"] \n",
    "            recs = self.final_result_dict[i][\"recall\"] \n",
    "            \n",
    "            step = 0\n",
    "            for prc, rec in zip(prcs , recs ):\n",
    "                if(self.writer is not None):\n",
    "                    #writer.add_scalar(f\"{subName}/Precision_{thresh}-ep{self.ep}\" , prc , step)\n",
    "                    #writer.add_scalar(f\"{subName}/Recall_{thresh}-ep{self.ep}\" , rec , step)\n",
    "                    #writer.add_scalar(f\"{subName}/AUC_{thresh}-ep{self.ep}\" ,  prc , rec )  # tensor board bug                                                    \n",
    "                    writer.add_scalars(f\"Eval/Precision_{thresh}\",{f\"ep{self.ep}\":prc}  , step)\n",
    "                    writer.add_scalars(f\"Eval/Recall_{thresh}\" ,{f\"ep{self.ep}\":rec}, step)\n",
    "                    #writer.add_scalars(f\"Eval/AUC_{thresh}\" ,{f\"ep{self.ep}\": prc},  self.ep )  # tensor board bug                                                    \n",
    "                \n",
    "                step+=1\n",
    "\n",
    "            writer.add_scalar(f\"Eval/Precision_rate_{thresh}\", self.final_result_dict[i]['precision_rate'] , self.ep)\n",
    "            writer.add_scalar(f\"Eval/Recall_rate_{thresh}\" ,   self.final_result_dict[i]['recall_rate'] , self.ep)\n",
    "            writer.add_scalar(f\"Eval/AUC_{thresh}\" ,  self.final_result_dict[i]['ap'] , self.ep )  # tensor board bug                                                    \n",
    "            writer.add_scalar(f\"Eval/mIou\" ,  self.all_iou/self.gt_count , self.ep )  # tensor board bug                                                    \n",
    "\n",
    "            print(\"all_iou\" , self.all_iou)\n",
    "            print(\"mIOU\" , self.all_iou/self.gt_count)\n",
    "            \n",
    "            \n",
    "            plt.plot( recs ,prcs )            \n",
    "            #plt.xlim([0, 1])\n",
    "            #plt.ylim([0, 1])\n",
    "            plt.title(f\"PR_curve-{thresh} ap : {self.final_result_dict[i]['ap']}\")\n",
    "            plt.savefig(log_folder+f\"/_PR_curve-{thresh}-{subName}-ep{self.ep}.jpg\")\n",
    "            #self.writer.add_figure(f\"PR_curve-{thresh}-ep{self.ep}-{subName}.jpg\" , plt.figure())\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "        self.writer.close()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "def visualize_2d(us, v_tops , v_btms, imgs, u_grad=None  , title =None , do_sig_u =False , polys = None):\n",
    "    out_imgs=[]    \n",
    "    length =  imgs.shape[0] if torch.is_tensor(imgs) else  len(imgs)        \n",
    "\n",
    "    for i in range(length):\n",
    "        if polys  is not None and u_grad is not None:            \n",
    "            img =visualize_2d_single(us[i] , v_tops[i] ,v_btms[i] , imgs[i] , u_grad[i] , title ,do_sig_u , polys[i] )\n",
    "        elif u_grad is not None:            \n",
    "            img =visualize_2d_single(us[i] , v_tops[i] ,v_btms[i] , imgs[i] , u_grad[i] , title ,do_sig_u ,polys )\n",
    "        else:\n",
    "            img = visualize_2d_single(us[i] , v_tops[i] ,v_btms[i] , imgs[i] , None , title , do_sig_u , polys)\n",
    "\n",
    "        out_imgs.append(img)\n",
    "    return out_imgs\n",
    "\n",
    "\n",
    "def visualize_2d_single(us, v_tops , v_btms, imgs, u_grad=None , title=None , do_sig_u =False , poly =None):\n",
    "    us = us.cpu().detach().numpy().flatten()\n",
    "    v_tops = v_tops.cpu().detach().numpy().flatten()\n",
    "    v_btms = v_btms.cpu().detach().numpy().flatten()\n",
    "    \n",
    "    uvs=[]\n",
    "    for u, v_t , v_b in zip( us , v_tops ,v_btms):                    \n",
    "        uvs.append( (u , v_t) )\n",
    "        uvs.append( (u , v_b) )        \n",
    "        \n",
    "    img = imgs.permute(1,2,0).cpu().detach().numpy()\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    if(poly is not None):        \n",
    "        for doors in poly:            \n",
    "            for part_door in doors:            \n",
    "                part_door = np.array(part_door)                \n",
    "                part_door = part_door.reshape((-1 , 2)) * np.tile(np.array([1024 , 512]) , (part_door.size//2 , 1) )\n",
    "                part_door = part_door.astype('int32')               \n",
    "                img =  cv2.polylines(img, [part_door], True, (0,255,0), 2)\n",
    "        pass\n",
    "    \n",
    "    h,w,c = img.shape\n",
    "    img_size = [w,h]\n",
    "    for point in uvs:\n",
    "        #p = np.float32(point) * img_size % img_size            \n",
    "        p = np.float32(point) * img_size         \n",
    "        p = np.int32(p)\n",
    "        img = cv2.circle( img, tuple( (p[0] , p[1])), 5,(255,0,0) , thickness= -1)\n",
    "\n",
    "    # Preview Confidence map\n",
    "    if u_grad is not None:        \n",
    "        #dist_graph = u_grad[i]\n",
    "        fig = plt.figure()\n",
    "        #spec = gridspec.GridSpec(ncols=1, nrows=MAX_PREDICTION_COUNT+20,)\n",
    "        spec = gridspec.GridSpec(ncols=1, nrows=3,)\n",
    "        #fig, ax = plt.subplots((MAX_PREDICTION_COUNT+1) ,figsize=(10,5))\n",
    "        fig.tight_layout()\n",
    "        #for j in range(MAX_PREDICTION_COUNT):                       \n",
    "        #for j in range(u_grad.shape[0]):                       \n",
    "        if do_sig_u ==True:\n",
    "            u_grad = torch.sigmoid( u_grad)\n",
    "        dist_graph = u_grad.repeat((10,1)).cpu().detach().numpy()            \n",
    "            \n",
    "        ax0 = fig.add_subplot(spec[0])\n",
    "        ax0.imshow(dist_graph , cmap=\"gray\" )\n",
    "        ax0.axis(\"off\")        \n",
    "        #ax[MAX_PREDICTION_COUNT].imshow(img ,aspect='auto'  )        \n",
    "        #plt.axis('off')\n",
    "        ax0 = fig.add_subplot(spec[1:])\n",
    "        ax0.imshow(img , aspect='auto' )\n",
    "        ax0.axis(\"off\")        \n",
    "        \n",
    "        if(title is not None):\n",
    "            fig.suptitle(title)\n",
    "        plt.show()\n",
    "        return fig_to_img(fig)\n",
    "    else:\n",
    "        plt.title(title)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        return img\n",
    "    pass\n",
    "\n",
    "from PIL import Image\n",
    "def fig_to_img(fig):    \n",
    "    img = np.asarray(fig.canvas.buffer_rgba())\n",
    "    return img\n",
    "\n",
    "def uv_to_3d(u , v_top , v_btm , d_top , d_btm):\n",
    "    # input [b , n]    \n",
    "    points_3d=[]    \n",
    "    for b in range((len(u))) :              \n",
    "        u = u[b]*360 - 180\n",
    "        vt = v_top[b]*180 - 90\n",
    "        vb = v_btm[b]*180 - 90\n",
    "\n",
    "        u=u* 0.01745329252\n",
    "        vt=vt* 0.01745329252        \n",
    "        vb=vb* 0.01745329252        \n",
    "\n",
    "        #points_3d.append((u,v_top))        \n",
    "        #points_3d.append((u,v_btm))\n",
    "        #for _u , _vt , _vb, _dt, _db in zip(  )\n",
    "        '''\n",
    "        for uvd in zip(u,v , depth[0]):\n",
    "            x = uv[2] * cos(uv[1]) * cos(uv[0])\n",
    "            y = uv[2] * sin(uv[1])\n",
    "            z = uv[2] * cos(uv[1]) * sin(uv[0])\n",
    "            points_3d.append((x,y,z))\n",
    "            print(\"uv\" , uv  , x , y , z)\n",
    "            pass\n",
    "    draw_3d(points_3d)\n",
    "        '''\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(net, path , epoch =0 , ap = 0):\n",
    "    state_dict = {\n",
    "        #'args': args.__dict__,\n",
    "        'kwargs': {\n",
    "            'backbone': net.backbone,\n",
    "            'use_rnn': net.use_rnn,\n",
    "        },\n",
    "        'state_dict': net.state_dict(),\n",
    "        'epoch':epoch,\n",
    "        'ap':ap\n",
    "    }\n",
    "    torch.save(state_dict, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_u (batched_u , title = \"Visulize u\"):\n",
    "    for _g in batched_u:\n",
    "        if(_g.is_cuda):\n",
    "            _g = _g.detach().cpu().numpy()\n",
    "        else:\n",
    "            _g = _g.numpy()\n",
    "        _g = np.tile(_g , (1,1))        \n",
    "        plt.title(title)\n",
    "        plt.imshow(_g)\n",
    "        plt.show()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#device ='cpu'\n",
    "#state_dict = torch.load('./output/horizonn256u20d20.pth', map_location='cpu')\n",
    "#net.load_state_dict(state_dict['state_dict'])\n",
    "train_dataset = CustomDataset( f\"./output/{TEST_DATASET_NAME}\" ,debug_doTrans=False   )  \n",
    "train_dataset_noTrans = CustomDataset( f\"./output/{TRAIN_DATASET_NAME}\"  ,debug_doTrans=False )  \n",
    "#train_dataset = CustomDataset( f\"./output/testerr.json\"   )  \n",
    "BATCH_SIZE = 3\n",
    "train_dataloader = DataLoader(train_dataset,  BATCH_SIZE , shuffle=True, drop_last =True)\n",
    "train_dataloader_noTrans = DataLoader(train_dataset_noTrans,  1 , shuffle=False, drop_last =True)\n",
    "#_t_loader = DataLoader(_t_dataset, BATCH_SIZE , shuffle=True, drop_last =True)\n",
    "\n",
    "data = next(iter(train_dataloader))\n",
    "data2 = next(iter(train_dataloader_noTrans))\n",
    "\n",
    "\n",
    "#data = next(iter(eval_dataloader))\n",
    "for k, v in data.items():    \n",
    "    data[k]=data[k].to(device)\n",
    "\n",
    "for k, v in data2.items():    \n",
    "    data2[k]=data2[k].to(device)\n",
    "'''\n",
    "'''\n",
    "out = predict(data)   #[b , max_count , 5 ]\n",
    "out = torch.transpose(out , 1 , 2) #[b , 5 , max_count ]\n",
    "\n",
    "\n",
    "\n",
    "pack_gt = (data['u'] , data['v_top'] , data['v_btm'] , data['du'] , data['dv_top'] , data['dv_btm'] , data['u_grad'] )\n",
    "pack_gt = torch.cat(pack_gt , 1)\n",
    "\n",
    "pack_gt2 = (data2['u'] , data2['v_top'] , data2['v_btm'] , data2['du'] , data2['dv_top'] , data2['dv_btm'] , data2['u_grad'] )\n",
    "pack_gt2 = torch.cat(pack_gt2 , 1)\n",
    "\n",
    "\n",
    "b, _ = pack_gt.shape\n",
    "#out = torch.rand(b , 7 , MAX_PREDICTION_COUNT ).to(device)\n",
    "\n",
    "pack_gt = pack_gt.reshape((b , 7 , -1 ))\n",
    "pack_gt = encode(pack_gt)\n",
    "\n",
    "pack_gt2 = pack_gt2.reshape((b , 7 , -1 ))\n",
    "pack_gt2 = encode(pack_gt2)\n",
    "#print(\"encoded pack_gt\" , pack_gt)\n",
    "\n",
    "(matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,\\\n",
    "(matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm,),gt_idxs  = match_gt(pack_gt , out )\n",
    "\n",
    "#print(\"matched_gt_u\",matched_gt_u)\n",
    "#print(\"matched_prd_u\",matched_prd_u)\n",
    "\n",
    "# [debug] :\n",
    "#show_u(matched_gt_u , \"gt u\")\n",
    "#show_u(matched_prd_u , \"pred u\")\n",
    "\n",
    "_loss = cal_loss(\n",
    "    (matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm,),\n",
    "    (matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,\n",
    "    gt_idxs\n",
    "    )\n",
    "print(_loss)\n",
    "#u,vt,vb =  decode(out)\n",
    "#real_gt_u = u_interval_to_real_u(matched_prd_u)\n",
    "#u,vt,vb,u_grad =  decode((matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,CONFIDENCE_THRESHOLD)\n",
    "pred_u,pred_vt,pred_vb,pred_u_grad =  decode((matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm) , CONFIDENCE_THRESHOLD , True)\n",
    "u,vt,vb,u_grad =  decode((matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) , 0.5 , True)\n",
    "#u,vt,vb,scores =  decode((matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm) ,0.05 )\n",
    "\n",
    "#u_grad = get_grad_u_keep_batch(data['u'] , width= 256)\n",
    "\n",
    "#print(\"u\",u)\n",
    "#print(\"vt\",vt)\n",
    "#print(\"vb\",vb)\n",
    "\n",
    "print(\"raw pred boxu\" , pred_u)\n",
    "with torch.no_grad():\n",
    "    boxu , boxvt , boxvb = rearrange_decoded(u, vt , vb)\n",
    "    pred_boxu , pred_boxvt , pred_boxvb = rearrange_decoded(pred_u, pred_vt , pred_vb)\n",
    "    #print(\"gt  boxu\" , boxu)\n",
    "    #print(\"pred boxu\" , pred_boxu)\n",
    "\n",
    "    gt_polys =[]\n",
    "    pred_polys =[]\n",
    "\n",
    "    for bu,bvt,bvb ,img in zip(boxu , boxvt , boxvb ,data['image']):    \n",
    "        _u = bu.reshape(-1,2)\n",
    "        _vt = bvt.reshape(-1,2)\n",
    "        _vb = bvb.reshape(-1,2)    \n",
    "\n",
    "        img = img.permute(1,2,0).cpu().detach().numpy()\n",
    "        img = np.ascontiguousarray(img)\n",
    "        p =to_distorted_box(_u , _vt , _vb , img)\n",
    "        gt_polys.append(p)\n",
    "\n",
    "    for bu,bvt,bvb ,img in zip(pred_boxu , pred_boxvt , pred_boxvb ,data['image']):    \n",
    "        _u = bu.reshape(-1,2)\n",
    "        _vt = bvt.reshape(-1,2)\n",
    "        _vb = bvb.reshape(-1,2)    \n",
    "\n",
    "        img = img.permute(1,2,0).cpu().detach().numpy()\n",
    "        img = np.ascontiguousarray(img)\n",
    "        p = to_distorted_box(_u , _vt , _vb , img)\n",
    "        pred_polys.append(p)\n",
    "    \n",
    "    for b in range(BATCH_SIZE) :\n",
    "        iou_matrix = get_iou_matrix_distored(gt_polys[b] , pred_polys[b])\n",
    "        print(\"final iou_matrix\",iou_matrix)\n",
    "\n",
    "    plt_imgs = visualize_2d(         \n",
    "        pred_u,\n",
    "        pred_vt,\n",
    "        pred_vb,\n",
    "        data['image'],\n",
    "        #matched_prd_u,\n",
    "        matched_prd_u,\n",
    "        #u_grad\n",
    "        #get_grad_u_keep_batch(real_gt_u)    \n",
    "        polys= pred_polys\n",
    "    )\n",
    "    plt_imgs = visualize_2d( \n",
    "        #matched_gt_u,\n",
    "        u,\n",
    "        #scores,\n",
    "        vt,\n",
    "        vb,\n",
    "        data['image'],\n",
    "        #matched_prd_u,\n",
    "        data['u_grad']   ,\n",
    "        #u_grad\n",
    "        #get_grad_u_keep_batch(real_gt_u)    \n",
    "        polys= gt_polys\n",
    "    )\n",
    "\n",
    "'''\n",
    "#=============== [ DEBUG] =================\n",
    "(matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,\\\n",
    "(matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm,),gt_idxs  = match_gt(pack_gt2 , out )\n",
    "\n",
    "u,vt,vb,u_grad =  decode((matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) , 0.5)\n",
    "plt_imgs = visualize_2d( \n",
    "    #matched_gt_u,\n",
    "    u,\n",
    "    #scores,\n",
    "    vt,\n",
    "    vb,\n",
    "    data2['image'],\n",
    "    #matched_prd_u,\n",
    "    data2['u_grad']   ,\n",
    "    #u_grad\n",
    "    #get_grad_u_keep_batch(real_gt_u)    \n",
    ")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import ops\n",
    "#data = next(iter(train_dataloader))\n",
    "#print(data)\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "#optimizer = optim.Adam(net.parameters(),lr= 0.00015)\n",
    "\n",
    "def train_loop(dataloader ):\n",
    "    #it_count =1    \n",
    "    global ep_count\n",
    "    global MAX_LOG_IT_COUNT\n",
    "    global MAX_LOG_GAP\n",
    "    global log_folder\n",
    "    #global it_count    \n",
    "    it_count = 0    \n",
    "    for data in tqdm(dataloader , desc=\"ep \"+str(ep_count) , ):       \n",
    "\n",
    "        #data =  next(iter(dataloader))                \n",
    "        for k, v in data.items():    \n",
    "            data[k]=data[k].to(device)       \n",
    "            \n",
    "        '''\n",
    "        '''\n",
    "        pack_gt = (data['u'] , data['v_top'] , data['v_btm'] , data['du'] , data['dv_top'] , data['dv_btm'] , data['u_grad']  )        \n",
    "        pack_gt = torch.cat(pack_gt , 1)\n",
    "        b, _ = pack_gt.shape\n",
    "        pack_gt = pack_gt.reshape((b,7,-1))\n",
    "        #pack_gt = pack_gt.reshape((b,6,-1))\n",
    "        pack_gt = encode(pack_gt)        \n",
    "        \n",
    "        #out_u, out_v_top , out_v_btm , out_d_top , out_d_btm = predict(data)        \n",
    "        #peak_out_data = filter_peak_data( (out_u, out_v_top , out_v_btm , out_d_top , out_d_btm ) ) # [b , 5 , max_door_count]\n",
    "        out = predict(data)   #[b , max_count , 5 ]\n",
    "        out = torch.transpose(out , 1 , 2) #[b , 5 , max_count ]        \n",
    "        try :\n",
    "            (matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,\\\n",
    "            (matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm,),gt_idxs  = match_gt(pack_gt , out )\n",
    "        except Exception as error:\n",
    "            print (\"Error! : \" , error)        \n",
    "            import datetime            \n",
    "            current_time = datetime.datetime.now()\n",
    "            current_time_str = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            log = {\n",
    "                'timestamp': current_time_str ,\n",
    "                'u' : pack_gt[0],\n",
    "                'v_top' : pack_gt[1],\n",
    "                'v_btm' : pack_gt[2],\n",
    "                'du' : pack_gt[3],\n",
    "                'dv_top' : pack_gt[4],\n",
    "                'dv_btm' : pack_gt[5],\n",
    "            }\n",
    "            print(log)            \n",
    "            json_object = json.dumps(log, indent=4)                        \n",
    "            with open(f\"./output/error_log-{TRAIN_NAME}.json\", \"w\") as outfile:\n",
    "                outfile.write(json_object)\n",
    "\n",
    "        losses = cal_loss(                \n",
    "            (matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm,) ,         \n",
    "            (matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,\n",
    "            gt_idxs\n",
    "        )        \n",
    "        it_loss = sum(l for l in losses.values())              \n",
    "\n",
    "        #for k, v in losses.items():    \n",
    "            #writer.add_scalar('loss/'+k, v.item(),it_count)\n",
    "        for k, v in losses.items():    \n",
    "            writer.add_scalars('loss/'+k ,{'train':v.item()} , it_count + ep_count*len(dataloader))\n",
    "\n",
    "        if((ep_count % MAX_LOG_GAP==0 ) and (it_count < MAX_LOG_IT_COUNT)):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                u,vt,vb,scores =  decode((matched_prd_u ,matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm), CONFIDENCE_THRESHOLD )\n",
    "                plt_imgs = visualize_2d( \n",
    "                    u,\n",
    "                    vt ,\n",
    "                    vb , \n",
    "                    data['image'],\n",
    "                    matched_prd_u,\n",
    "                    #None,#u_grad\n",
    "                    \"Trainging Prediction\",\n",
    "                    True\n",
    "                \n",
    "                )\n",
    "                for im in plt_imgs:\n",
    "                    #writer.add_image('vis/pred/train-ep'+str(ep_count), im , dataformats=\"HWC\")\n",
    "                    cv2.imwrite(log_folder + f\"/pred-train-ep{ep_count}-{it_count}.png\" , im )\n",
    "\n",
    "                u,vt,vb,scores =  decode((matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm),0.5 )\n",
    "                plt_imgs = visualize_2d( \n",
    "                    u,\n",
    "                    vt ,\n",
    "                    vb , \n",
    "                    data['image'],\n",
    "                    #None , #u_grad\n",
    "                    data['u_grad'] ,\n",
    "                    \"GT\"\n",
    "                )\n",
    "                for im in plt_imgs:\n",
    "                    #writer.add_image('vis/gt/train-ep'+str(ep_count), im , dataformats=\"HWC\")\n",
    "                    cv2.imwrite(log_folder+f\"/gt-train-ep{ep_count}-{it_count}.png\" , im )\n",
    "        it_count+=1        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        it_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 3.0, norm_type='inf')\n",
    "        optimizer.step()\n",
    "\n",
    "        '''\n",
    "        if(it_count >21):\n",
    "            break        \n",
    "        '''\n",
    "def eval_loop( dataloader ):\n",
    "    global ep_count\n",
    "    global log_folder\n",
    "    global MAX_LOG_IT_COUNT\n",
    "    global MAX_LOG_GAP\n",
    "    it_count    =0\n",
    "    net.eval()\n",
    "\n",
    "    pr_helper = PR_Eval_Helper(writer=writer ,ep= ep_count)\n",
    "    #all_max_iou_pre_reg=[]\n",
    "    #gt_count=0\n",
    "\n",
    "    for data in tqdm(dataloader , desc=\"ep \"+str(ep_count) , ):    \n",
    "                \n",
    "        for k, v in data.items():    \n",
    "            data[k]=data[k].to(device)            \n",
    "        \n",
    "        pack_gt = (data['u'] , data['v_top'] , data['v_btm'] , data['du'] , data['dv_top'] , data['dv_btm'] , data['u_grad']  )        \n",
    "        pack_gt = torch.cat(pack_gt , 1)\n",
    "        b, _ = pack_gt.shape\n",
    "        pack_gt = pack_gt.reshape((b,7,-1))\n",
    "        pack_gt = encode(pack_gt)\n",
    "\n",
    "        out = predict(data)   #[b , max_count , 6 ]\n",
    "        out = torch.transpose(out , 1 , 2) #[b , 6 , max_count ]\n",
    "        pack_out = (out[:,0] , out[:,1],out[:,2],out[:,3],out[:,4],out[:,5])\n",
    "\n",
    "        # ====== Eval Loss ======\n",
    "        (matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,\\\n",
    "        (matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm,),gt_idxs  = match_gt(pack_gt , out )\n",
    "\n",
    "        losses = cal_loss(                \n",
    "            (matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm,) ,         \n",
    "            (matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,\n",
    "            gt_idxs\n",
    "        )        \n",
    "        it_loss = sum(l for l in losses.values())              \n",
    "\n",
    "        for k, v in losses.items():    \n",
    "            writer.add_scalars('loss/'+k ,{'test' :v.item()} ,it_count + ep_count*len(dataloader))\n",
    "\n",
    "\n",
    "        pred_u,pred_vt,pred_vb , pred_scores =  decode(pack_out , CONFIDENCE_THRESHOLD , True)                    \n",
    "        gt_u,gt_vt,gt_vb,gt_u_grad =  decode((matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) , 0.5 , True)\n",
    "        \n",
    "        pred_poly = uv_to_distorted_box(pred_u,pred_vt,pred_vb)\n",
    "        gt_poly = uv_to_distorted_box(gt_u,gt_vt,gt_vb)\n",
    "      \n",
    "        # ========= PR Curve ===========\n",
    "        #for pred , gt ,score in zip(pred_bboxes , gt_bboxes , scores  ): # each image                                    \n",
    "        for pred , gt  in zip(pred_poly , gt_poly ): # each image                                    \n",
    "            pr_helper.eval_batch_pr(pred , gt , None , ep_count)\n",
    "\n",
    "        #if((ep_count % MAX_LOG_GAP==0 ) and (it_count < MAX_LOG_IT_COUNT)):    \n",
    "                \n",
    "        if(it_count < MAX_LOG_IT_COUNT):        \n",
    "            plt_imgs = visualize_2d( \n",
    "                pred_u,\n",
    "                pred_vt ,\n",
    "                pred_vb , \n",
    "                data['image'],\n",
    "                #None,#u_grad,\n",
    "                out[:,0],                \n",
    "                \"inf\",\n",
    "                True,\n",
    "                pred_poly\n",
    "            )\n",
    "            for im in plt_imgs:\n",
    "                #writer.add_image('vis/pred/eval-ep'+str(ep_count), im , dataformats=\"HWC\")\n",
    "                cv2.imwrite(log_folder+f\"/inf-eval-ep{ep_count}-{it_count}.png\" , im )\n",
    "\n",
    "            plt_imgs = visualize_2d( \n",
    "                gt_u,\n",
    "                gt_vt ,\n",
    "                gt_vb , \n",
    "                data['image'],\n",
    "                #None,\n",
    "                data['u_grad'],\n",
    "                \"gt\",\n",
    "                False,\n",
    "                gt_poly\n",
    "            )\n",
    "            for im in plt_imgs:\n",
    "                #writer.add_image('vis/gt/eval-ep'+str(ep_count), im , dataformats=\"HWC\")\n",
    "                cv2.imwrite(log_folder+f\"/gt-eval-ep{ep_count}-{it_count}.png\" , im )\n",
    "        '''\n",
    "        if(it_count >2):\n",
    "            break     \n",
    "        '''        \n",
    "        it_count +=1\n",
    "\n",
    "    p , r ,auc = pr_helper.get_all_pr()\n",
    "    #pr_helper.write_tensorboard()\n",
    "    ap_50 = pr_helper.final_result_dict[1]['ap']   \n",
    "\n",
    "    #return p , r ,auc\n",
    "    return ap_50\n",
    "\n",
    "\n",
    "'''\n",
    "ep_count = 1\n",
    "MAX_LOG_IT_COUNT=5\n",
    "\n",
    "with torch.no_grad():                \n",
    "    eval_loop(train_dataloader)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [ONLY DO EVAl]\n",
    "ep_count=LOADED_EPOCH\n",
    "#eval_dataset = CustomDataset( f\"./output/test_visiable_horizon_unique_w0.01_4_fixedbug.json\"   ) \n",
    "eval_dataset = CustomDataset( f\"./output/{TEST_DATASET_NAME}\" ,debug_doTrans=False ) \n",
    "eval_dataloader = DataLoader(eval_dataset, BATCH_SIZE , shuffle=False, drop_last =True)\n",
    "with torch.no_grad():                \n",
    "    auc =eval_loop(eval_dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Eval Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE=6\n",
    "#TRAIN_NAME = \"hor-big-826-20-n256-1\"\n",
    "#TRAIN_NAME = \"n20_big20-r3\"\n",
    "\n",
    "#train_dataset = CustomDataset( \"./output/train_visiable_horizon_big_20.json\"   )  \n",
    "#eval_dataset = CustomDataset( \"./output/test_visiable_horizon_big_10.json\"   ) \n",
    "\n",
    "train_dataset = CustomDataset( f\"./output/{TRAIN_DATASET_NAME}\" , debug_doTrans= True  )  \n",
    "eval_dataset = CustomDataset( f\"./output/{TEST_DATASET_NAME}\"  ,debug_doTrans=False ) \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE , shuffle=True, drop_last =True)\n",
    "eval_dataloader = DataLoader(eval_dataset, BATCH_SIZE , shuffle=False, drop_last =True)\n",
    "\n",
    "'''\n",
    "'''\n",
    "assert os.environ[\"CUDA_VISIBLE_DEVICES\"]==\"2\" , f\"WRONG MECHAIN\"\n",
    "if(os.environ[\"CUDA_VISIBLE_DEVICES\"]!=\"2\"):\n",
    "    MAX_EPOCH=0\n",
    "\n",
    "  \n",
    "ep_count = 1 if LOADED_EPOCH is None else LOADED_EPOCH+1\n",
    "#train_loop()\n",
    "\n",
    "while True:\n",
    "#for i in range(1):    \n",
    "    \n",
    "    # ======= Train EPOCH =======\n",
    "    net.train()\n",
    "    train_loop(train_dataloader)    \n",
    "    try:\n",
    "        # ======= Eval EPOCH =======    \n",
    "        if(ep_count % EVAL_GAP ==0):\n",
    "            print(\"=========== EVAL =========\")\n",
    "            net.eval()\n",
    "            with torch.no_grad():        \n",
    "                auc = eval_loop(eval_dataloader)\n",
    "                \n",
    "                if(auc > save_auc) :\n",
    "                    path = './output/'+ TRAIN_NAME  +'.pth'\n",
    "                    save_model(net , path , ep_count , auc)\n",
    "                    save_auc = auc\n",
    "                if(ep_count % 5 == 0):\n",
    "                    path = './output/'+ '1017_wAug_all_bk.pth'\n",
    "                    save_model(net , path , ep_count , auc)\n",
    "                '''\n",
    "                '''\n",
    "    except Exception as error:\n",
    "        print (\"Error! : \" , error)        \n",
    "        import datetime            \n",
    "        current_time = datetime.datetime.now()\n",
    "        current_time_str = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        log = {\n",
    "            'timestamp': current_time_str ,\n",
    "            'error': error.__dict__,            \n",
    "        }\n",
    "        print(log)            \n",
    "        json_object = json.dumps(log, indent=4)                        \n",
    "        with open(f\"./output/error_log-{TRAIN_NAME}.json\", \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "\n",
    "    ep_count+=1\n",
    "\n",
    "#save_model(net,f'./output/{TRAIN_NAME}-final.pth' , ep_count , save_auc )\n",
    "'''\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_list = [90 ,70, 50, 40 ]\n",
    "c_list = [0 , 0.1 , 0.96]\n",
    "r_list = [0 , 3 , 5 , 10]\n",
    "\n",
    "train_dataset = CustomDataset( f\"./output/{TRAIN_DATASET_NAME}\"   )  \n",
    "eval_dataset = CustomDataset( f\"./output/{TEST_DATASET_NAME}\"   ) \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE , shuffle=True, drop_last =True)\n",
    "eval_dataloader = DataLoader(eval_dataset, BATCH_SIZE , shuffle=False, drop_last =True)\n",
    "\n",
    "\n",
    "assert os.environ[\"CUDA_VISIBLE_DEVICES\"]==\"2\" , f\"WRONG MECHAIN\"\n",
    "if(os.environ[\"CUDA_VISIBLE_DEVICES\"]!=\"2\"):\n",
    "    MAX_EPOCH=0\n",
    "    \n",
    "\n",
    "\n",
    "ep_count = 1 if LOADED_EPOCH is None else LOADED_EPOCH\n",
    "def train_eval_cycle(max_epoch):\n",
    "    global ep_count\n",
    "    ep_count = 1\n",
    "    e100_auc = 0\n",
    "    for i in range(max_epoch):    \n",
    "        # ======= Train EPOCH =======\n",
    "        net.train()\n",
    "        train_loop(train_dataloader)\n",
    "        # ======= Eval EPOCH =======    \n",
    "        if(ep_count % EVAL_GAP ==0):\n",
    "            print(\"=========== EVAL =========\")\n",
    "            net.eval()\n",
    "            with torch.no_grad():        \n",
    "                e100_auc = eval_loop(eval_dataloader) \n",
    "\n",
    "                '''\n",
    "                if(auc > save_auc) :\n",
    "                    path = './output/'+ TRAIN_NAME  +'.pth'\n",
    "                    save_model(net , path , ep_count , auc)\n",
    "                    save_auc = auc\n",
    "                '''\n",
    "        ep_count+=1\n",
    "    save_model(net,f'./output/{TRAIN_NAME}.pth' , max_epoch , e100_auc )\n",
    "\n",
    "\n",
    "'''\n",
    "for n in n_list:\n",
    "    for c in c_list:\n",
    "        for r in r_list:\n",
    "            reset_ncr_config(n,c,r)\n",
    "            reset_model()\n",
    "            torch.cuda.empty_cache()\n",
    "            train_eval_cycle(100)\n",
    "            \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(net,f'./output/{TRAIN_NAME}-with_aug_ep166.pth' , 166 , 0.6612 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():  \n",
    "    eval_loop(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "#save_path = './output/horizonn256u20d20.pth'\n",
    "#save_model(net,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#state_dict = torch.load(save_path, map_location='cpu')\n",
    "#net.load_state_dict(state_dict['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "'''\n",
    "target_imgs = [\n",
    "\"/0310/panos/floor_01_partial_room_11_pano_23.jpg\",\n",
    "\"1357/panos/floor_01_partial_room_08_pano_31.jpg\",\n",
    "\"0636/panos/floor_02_partial_room_03_pano_74.jpg\",\n",
    "\"0548/panos/floor_01_partial_room_12_pano_25.jpg\",\n",
    "\"1060/panos/floor_01_partial_room_03_pano_42.jpg\",\n",
    "\"0530/panos/floor_01_partial_room_15_pano_11.jpg\"\n",
    "]\n",
    "'''\n",
    "import torch\n",
    "import json\n",
    "from horizon_model_direct import HorizonNet\n",
    "'''\n",
    "train_dataset = CustomDataset( f\"./output/{TRAIN_DATASET_NAME}\"   )  \n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE , shuffle=True, drop_last =True)\n",
    "tr_data = next(iter(train_dataloader))\n",
    "\n",
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device= 'cpu'\n",
    "net = HorizonNet('resnet50', True , MAX_PREDICTION_COUNT).to(device)   # For server (small memory)\n",
    "\n",
    "'''\n",
    "fake_imgs = torch.rand((6,3,512,1024))\n",
    "f = open('./error_log-n90-c0.1-r10-0912-all-ep1.json') \n",
    "data = json.load(f)\n",
    "for k, v in data.items():    \n",
    "    data[k]=torch.tensor( data[k]).to(device) \n",
    "'''\n",
    "print(data.keys())\n",
    "out = net(fake_imgs)   #[b , max_count , 5 ]\n",
    "#out = predict(data)   #[b , max_count , 5 ]\n",
    "out = torch.transpose(out , 1 , 2) #[b , 5 , max_count ]\n",
    "'''\n",
    "\n",
    "#pack_gt = (data['u'] , data['v_top'] , data['v_btm'] , data['du'] , data['dv_top'] , data['dv_btm'] , data['u_grad'])    \n",
    "print(torch.rand_like(data['u']) .shape)\n",
    "print(data['v_top'].shape)\n",
    "print(out.shape)\n",
    "pack_gt = (data['u'] , data['v_top'] , data['v_btm'] , data['du'] , data['dv_top'] , data['dv_btm'] , torch.rand_like(data['u']) )    \n",
    "pack_gt = torch.cat(pack_gt , 1)\n",
    "b, _ = pack_gt.shape\n",
    "#pack_gt = pack_gt.reshape((b , 7 , -1 ))\n",
    "pack_gt = pack_gt.reshape((b ,  6 , -1 ))\n",
    "'''\n",
    "pack_gt = encode(pack_gt)\n",
    "print(pack_gt)\n",
    "'''\n",
    "\n",
    "(matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,\\\n",
    "(matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm,),gt_idxs  = match_gt(pack_gt , out )\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "(matched_gt_u , matched_gt_vtop , matched_gt_vbtm , matched_gt_du ,matched_gt_dvtop ,matched_gt_dvbtm) ,\\\n",
    "(matched_prd_u , matched_prd_vtop ,matched_prd_vbtm , matched_prd_du ,matched_prd_dvtop , matched_prd_dvbtm,),gt_idxs  = match_gt(pack_gt , out )\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "target_imgs = [\n",
    "\"0310/panos/floor_01_partial_room_11_pano_23.jpg\",\n",
    "\"1357/panos/floor_01_partial_room_08_pano_31.jpg\",\n",
    "\"0636/panos/floor_02_partial_room_03_pano_74.jpg\",\n",
    "\"0548/panos/floor_01_partial_room_12_pano_25.jpg\",\n",
    "\"1060/panos/floor_01_partial_room_03_pano_42.jpg\",\n",
    "\"0530/panos/floor_01_partial_room_15_pano_11.jpg\"\n",
    "]\n",
    "'''\n",
    "\n",
    "target_imgs = [\n",
    "    '0661/panos/floor_01_partial_room_01_pano_26.jpg',\n",
    "    '0544/panos/floor_01_partial_room_21_pano_48.jpg',\n",
    "    '1369/panos/floor_01_partial_room_04_pano_9.jpg',\n",
    "    '0883/panos/floor_01_partial_room_09_pano_28.jpg',\n",
    "    '0404/panos/floor_01_partial_room_13_pano_6.jpg',\n",
    "    '0142/panos/floor_01_partial_room_05_pano_30.jpg',\n",
    "    '0823/panos/floor_01_partial_room_05_pano_14.jpg',\n",
    "    '0313/panos/floor_01_partial_room_13_pano_33.jpg',\n",
    "    '0167/panos/floor_01_partial_room_17_pano_49.jpg',\n",
    "    '0831/panos/floor_01_partial_room_08_pano_10.jpg',\n",
    "    '1222/panos/floor_01_partial_room_04_pano_43.jpg',\n",
    "    '0868/panos/floor_02_partial_room_07_pano_58.jpg',\n",
    "    '0513/panos/floor_01_partial_room_09_pano_9.jpg',\n",
    "    '0672/panos/floor_01_partial_room_04_pano_4.jpg',\n",
    "    '0840/panos/floor_02_partial_room_09_pano_55.jpg',\n",
    "    '0467/panos/floor_01_partial_room_04_pano_32.jpg',\n",
    "    '0968/panos/floor_01_partial_room_03_pano_19.jpg', \n",
    "    '0350/panos/floor_02_partial_room_05_pano_70.jpg', \n",
    "    '0759/panos/floor_01_partial_room_03_pano_11.jpg', \n",
    "    '1345/panos/floor_01_partial_room_01_pano_8.jpg']\n",
    "f= open(\"./output/train_visiable_horizon_unique_w0.01_all_fixedbug.json\")\n",
    "all_json = json.load(f)\n",
    "target = [a for a in all_json if a['image'] in target_imgs]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(target))\n",
    "json_object = json.dumps(target, indent=4)                        \n",
    "with open(f\"./output/testerr.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
