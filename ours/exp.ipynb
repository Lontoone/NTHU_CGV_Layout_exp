{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\layout\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Layout\\NTHU_CGV_Layout_exp\\ours\n",
      "d:\\Projects\\Layout\\NTHU_CGV_Layout_exp\\Horizon_and_SAM\\Horizon\n"
     ]
    }
   ],
   "source": [
    "# Data Loader\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import json\n",
    "from CustomDataset import * \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from config import *\n",
    "from file_helper import *\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "#=================================\n",
    "#             Augmentation\n",
    "#=================================\n",
    "\n",
    "def gauss_noise_tensor(img):\n",
    "    rand = torch.rand(1)[0]\n",
    "    if rand < 0.5 and Horizon_AUG:\n",
    "        sigma = rand *0.125\n",
    "        out = img + sigma * torch.randn_like(img)\n",
    "        return out\n",
    "    return img\n",
    "\n",
    "def blank(img):    \n",
    "    return img\n",
    "\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self , train_dir , test_dir , batch_size = 2, num_workers = 0 , img_size=[IMG_WIDTH, IMG_HEIGHT] , use_aug = True ,padding_count = 24 ,c =0.1 ):\n",
    "        super().__init__()\n",
    "        self.train_dir = train_dir\n",
    "        self.test_dir = test_dir\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_size = img_size      \n",
    "        self.use_aug = use_aug\n",
    "        self.padding_count  = padding_count\n",
    "        self.c = c\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download dataset\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # Create dataset...          \n",
    "                \n",
    "        self.entire_dataset = CustomDataset(self.train_dir  , use_aug= self.use_aug , padding_count= self.padding_count , c=self.c)\n",
    "        self.train_ds , self.val_ds = random_split(self.entire_dataset , [0.9, 0.1])        \n",
    "        self.test_ds = CustomDataset(self.test_dir  , use_aug= False)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # ToDo: Reture Dataloader...\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=False)\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "dm = CustomDataModule ( train_dir= f\"../anno/test_visiable_10_no_cross.json\" ,\n",
    "                       test_dir= f\"../anno/test_visiable_10_no_cross.json\" , padding_count=256\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "def unpad_data( x :[Tensor] ) :\n",
    "\tnon_zero_indices = torch.nonzero(x)\n",
    "\t#print(non_zero_indices)\n",
    "\t# Get the non-zero values\n",
    "\tnon_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "\tunique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "\t#print(\"unique\" , unique)\n",
    "\t# Print the result\n",
    "\t#print(non_zero_values)\n",
    "\tnon_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "\t#print(\"split non_zero_values\" , non_zero_values)\n",
    "\treturn non_zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type              | Params\n",
      "-----------------------------------------------------------\n",
      "0 | backbone             | Resnet            | 23.5 M\n",
      "1 | transformer          | TransformerModel  | 20.4 M\n",
      "2 | vqt_box_head         | Linear            | 1.3 K \n",
      "3 | vqt_cls_head         | Linear            | 257   \n",
      "4 | reduce_height_module | GlobalHeightStage | 45.5 M\n",
      "5 | v_reproj             | Conv2d            | 262 K \n",
      "-----------------------------------------------------------\n",
      "89.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "89.6 M    Total params\n",
      "179.221   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s, loss=1.7, v_num=55]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  20%|██        | 1/5 [00:02<00:10,  2.57s/it, loss=1.74, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  40%|████      | 2/5 [00:04<00:07,  2.47s/it, loss=1.68, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  60%|██████    | 3/5 [00:07<00:04,  2.44s/it, loss=1.7, v_num=55] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  80%|████████  | 4/5 [00:09<00:02,  2.43s/it, loss=1.68, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s, loss=1.68, v_num=55]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  20%|██        | 1/5 [00:02<00:10,  2.70s/it, loss=1.77, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  40%|████      | 2/5 [00:05<00:07,  2.53s/it, loss=1.74, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  60%|██████    | 3/5 [00:08<00:05,  2.91s/it, loss=1.78, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  80%|████████  | 4/5 [00:11<00:02,  2.80s/it, loss=1.74, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:   0%|          | 0/5 [00:00<?, ?it/s, loss=1.72, v_num=55]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  20%|██        | 1/5 [00:02<00:10,  2.57s/it, loss=1.74, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  40%|████      | 2/5 [00:05<00:07,  2.52s/it, loss=1.7, v_num=55] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  60%|██████    | 3/5 [00:07<00:04,  2.46s/it, loss=1.67, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  80%|████████  | 4/5 [00:09<00:02,  2.42s/it, loss=1.67, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:   0%|          | 0/5 [00:00<?, ?it/s, loss=1.66, v_num=55]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  20%|██        | 1/5 [00:02<00:10,  2.58s/it, loss=1.67, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  40%|████      | 2/5 [00:05<00:07,  2.52s/it, loss=1.67, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  60%|██████    | 3/5 [00:07<00:04,  2.46s/it, loss=1.65, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  80%|████████  | 4/5 [00:09<00:02,  2.42s/it, loss=1.66, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:   0%|          | 0/5 [00:00<?, ?it/s, loss=1.61, v_num=55]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  20%|██        | 1/5 [00:02<00:09,  2.48s/it, loss=1.61, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  40%|████      | 2/5 [00:04<00:07,  2.41s/it, loss=1.62, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  60%|██████    | 3/5 [00:07<00:04,  2.44s/it, loss=1.64, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  80%|████████  | 4/5 [00:09<00:02,  2.41s/it, loss=1.65, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:   0%|          | 0/5 [00:00<?, ?it/s, loss=1.62, v_num=55]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:  20%|██        | 1/5 [00:02<00:10,  2.51s/it, loss=1.62, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:  40%|████      | 2/5 [00:04<00:07,  2.43s/it, loss=1.68, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:  60%|██████    | 3/5 [00:07<00:04,  2.41s/it, loss=1.68, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:  80%|████████  | 4/5 [00:09<00:02,  2.40s/it, loss=1.69, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:   0%|          | 0/5 [00:00<?, ?it/s, loss=1.56, v_num=55]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:  20%|██        | 1/5 [00:02<00:10,  2.71s/it, loss=1.51, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:  40%|████      | 2/5 [00:05<00:07,  2.51s/it, loss=1.52, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:  60%|██████    | 3/5 [00:07<00:04,  2.45s/it, loss=1.58, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:  80%|████████  | 4/5 [00:09<00:02,  2.42s/it, loss=1.59, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40:   0%|          | 0/5 [00:00<?, ?it/s, loss=1.6, v_num=55]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40:  20%|██        | 1/5 [00:02<00:09,  2.36s/it, loss=1.6, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40:  40%|████      | 2/5 [00:04<00:07,  2.33s/it, loss=1.61, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40:  60%|██████    | 3/5 [00:06<00:04,  2.33s/it, loss=1.65, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40:  80%|████████  | 4/5 [00:09<00:02,  2.41s/it, loss=1.61, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45:   0%|          | 0/5 [00:00<?, ?it/s, loss=1.58, v_num=55]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45:  20%|██        | 1/5 [00:02<00:09,  2.50s/it, loss=1.61, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45:  40%|████      | 2/5 [00:04<00:07,  2.40s/it, loss=1.64, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45:  60%|██████    | 3/5 [00:07<00:04,  2.39s/it, loss=1.6, v_num=55] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45:  80%|████████  | 4/5 [00:09<00:02,  2.37s/it, loss=1.58, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50:   0%|          | 0/5 [00:00<?, ?it/s, loss=1.59, v_num=55]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50:  20%|██        | 1/5 [00:02<00:10,  2.56s/it, loss=1.62, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50:  40%|████      | 2/5 [00:04<00:07,  2.42s/it, loss=1.58, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50:  60%|██████    | 3/5 [00:07<00:04,  2.40s/it, loss=1.57, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50:  80%|████████  | 4/5 [00:09<00:02,  2.39s/it, loss=1.58, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 5/5 [00:11<00:00,  2.27s/it, loss=1.59, v_num=55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=51` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 5/5 [00:16<00:00,  3.39s/it, loss=1.59, v_num=55]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Any\n",
    "import pytorch_lightning as pl\n",
    "from config import *\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import MLP\n",
    "import math\n",
    "from torch import Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Model\n",
    "class ConvCompressH(nn.Module):\n",
    "    ''' Reduce feature height by factor of two '''\n",
    "    def __init__(self, in_c, out_c, ks=3):\n",
    "        super(ConvCompressH, self).__init__()\n",
    "        assert ks % 2 == 1\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=ks, stride=(2, 1), padding=ks//2),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class GlobalHeightConv(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(GlobalHeightConv, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            ConvCompressH(in_c, in_c//2),\n",
    "            ConvCompressH(in_c//2, in_c//2),\n",
    "            ConvCompressH(in_c//2, in_c//4),\n",
    "            ConvCompressH(in_c//4, out_c),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, out_w):\n",
    "        x = self.layer(x)\n",
    "        assert out_w % x.shape[3] == 0\n",
    "        factor = out_w // x.shape[3]\n",
    "        x = torch.cat([x[..., -1:], x, x[..., :1]], 3)\n",
    "        x = F.interpolate(x, size=(x.shape[2], out_w + 2 * factor), mode='bilinear', align_corners=False)\n",
    "        x = x[..., factor:-factor]        \n",
    "        return x\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self, backbone='resnet50', pretrained=True):\n",
    "        super(Resnet, self).__init__()\n",
    "        #assert backbone in ENCODER_RESNET\n",
    "        self.encoder = getattr(models, backbone)(pretrained=pretrained)\n",
    "        del self.encoder.fc, self.encoder.avgpool\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        x = self.encoder.relu(x)\n",
    "        x = self.encoder.maxpool(x)\n",
    "\n",
    "        x = self.encoder.layer1(x);  features.append(x)  # 1/4\n",
    "        x = self.encoder.layer2(x);  features.append(x)  # 1/8\n",
    "        x = self.encoder.layer3(x);  features.append(x)  # 1/16\n",
    "        x = self.encoder.layer4(x);  features.append(x)  # 1/32\n",
    "        return features\n",
    "\n",
    "    def list_blocks(self):\n",
    "        lst = [m for m in self.encoder.children()]\n",
    "        block0 = lst[:4]\n",
    "        block1 = lst[4:5]\n",
    "        block2 = lst[5:6]\n",
    "        block3 = lst[6:7]\n",
    "        block4 = lst[7:8]\n",
    "        return block0, block1, block2, block3, block4\n",
    "    \n",
    "class GlobalHeightStage(nn.Module):\n",
    "    def __init__(self, c1, c2, c3, c4, out_scale=8 , pretrain_weight= \"\"):\n",
    "        ''' Process 4 blocks from encoder to single multiscale features '''\n",
    "        super(GlobalHeightStage, self).__init__()\n",
    "        self.cs = c1, c2, c3, c4\n",
    "        self.out_scale = out_scale\n",
    "        self.ghc_lst = nn.ModuleList([\n",
    "            GlobalHeightConv(c1, c1//out_scale),\n",
    "            GlobalHeightConv(c2, c2//out_scale),\n",
    "            GlobalHeightConv(c3, c3//out_scale),\n",
    "            GlobalHeightConv(c4, c4//out_scale),\n",
    "        ])\n",
    "\n",
    "        if(pretrain_weight!=\"\"):\n",
    "\n",
    "            pass\n",
    "\n",
    "    def forward(self, conv_list, out_w):\n",
    "        assert len(conv_list) == 4\n",
    "        bs = conv_list[0].shape[0]\n",
    "        feature = torch.cat([\n",
    "            f(x, out_w).reshape(bs, -1, out_w)\n",
    "            for f, x, out_c in zip(self.ghc_lst, conv_list, self.cs)\n",
    "        ], dim=1)\n",
    "        return feature\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int , d_hid: int, nlayers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.enc_embedding = nn.Embedding(num_embeddings=ntoken ,embedding_dim= d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model , nhead , d_hid , dropout) \n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "\n",
    "        self.dec_embedding = nn.Embedding(num_embeddings=ntoken ,embedding_dim= d_model)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead , dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder( self.decoder_layer  , num_layers=6)\n",
    "\n",
    "        '''\n",
    "        #self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "        self.init_weights()\n",
    "        '''\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        #self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        #src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        #print(\"trans forward \" , src.shape)\n",
    "        #src = self.pos_encoder(src)\n",
    "        #output = self.transformer_encoder(src, src_mask)\n",
    "        #output = self.linear(output)\n",
    "        #return output\n",
    "        #print(\"self.ntoken\" , self.ntoken)\n",
    "        #print(\"self.src\" , src.shape)\n",
    "        pos_idx = torch.arange(self.ntoken , device=src.device)\n",
    "\n",
    "        pos_src = self.enc_embedding(pos_idx)* math.sqrt(self.d_model) + src\n",
    "        pos_enc_src = self.encoder(pos_src )\n",
    "\n",
    "        pos_dec_src = self.dec_embedding(pos_idx)* math.sqrt(self.d_model) + pos_enc_src\n",
    "        #pos_dec_src = self.dec_embedding( torch.tensor([0,1],device=src.device) )* math.sqrt(self.d_model) + pos_enc_src\n",
    "        dec_src = self.decoder( pos_enc_src  ,pos_dec_src)\n",
    "\n",
    "        #print(\"dec_src\" , dec_src.shape)    # [batch , token , hidden]\n",
    "        return dec_src\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class VerticalQueryTransformer(pl.LightningModule):    \n",
    "    def __init__(self  ,  max_predict_count = 24 , hidden_out = 128 , class_num = 1 , log_folder = \"__test\" , num_classes = 1 , load_weight =\"\"):\n",
    "        #print(\" input_size\" ,  input_size)\n",
    "        super().__init__()\n",
    "        self.backbone = Resnet()\n",
    "        self.out_scale = 8\n",
    "        self.step_cols = 4        \n",
    "        self.hidden_size = hidden_out\n",
    "        self.max_predict_count = max_predict_count\n",
    "        self.num_classes  = num_classes \n",
    "\n",
    "        self.transformer = TransformerModel( ntoken= max_predict_count , d_model=hidden_out , nhead=8 , d_hid= 2048,nlayers=6 )\n",
    "\n",
    "        #self.box_head= nn.Linear( hidden_out , 6 )        \n",
    "        self.vqt_box_head= nn.Linear( hidden_out , 5 )        \n",
    "        self.vqt_cls_head= nn.Linear( hidden_out , class_num )        \n",
    "        self.confidence_threshold = 0.5\n",
    "\n",
    "        # loss\n",
    "        self.box_cost = 1\n",
    "        self.cls_cost = 20\n",
    "\n",
    "        self.log_folder = create_folder(os.path.join(os.getcwd() , \"output\" , log_folder))\n",
    "        #self.box_head.bias.data = torch.nn.Parameter(torch.tensor([0.3,0.2,0.2,0.3]))\n",
    "        #self.box_head.weight.data.fill_(0)\n",
    "        \n",
    "        # Inference channels number from each block of the encoder\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 320, 190)\n",
    "            c1, c2, c3, c4 = [b.shape[1] for b in self.backbone(dummy)] # resnet feature channel數\n",
    "            #print(\"c1, c2, c3, c4\" , c1, c2, c3, c4)\n",
    "            c_last = (c1*8 + c2*4 + c3*2 + c4*1) // self.out_scale            \n",
    "        self.reduce_height_module = GlobalHeightStage(c1, c2, c3, c4 , out_scale=self.out_scale , pretrain_weight= \"\")\n",
    "        self.v_reproj = nn.Conv2d(1024 , self.max_predict_count,kernel_size=1)\n",
    "        \n",
    "        if(load_weight != \"\"):\n",
    "            checkpoint = torch.load(load_weight ,  map_location=\"cpu\")\n",
    "            pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in self.state_dict()}\n",
    "            self.load_state_dict(pretrained_dict , strict=False)\n",
    "            '''\n",
    "            for i, (name, param) in enumerate(self.named_parameters()):\n",
    "                if \"reduce_height_module\" in name:\n",
    "                    param.requires_grad = False\n",
    "            '''\n",
    "            pass\n",
    "\n",
    "\n",
    "    def forward(self ,x ):\n",
    "        features = self.backbone(x) # [4 , c , h, w]\n",
    "        \n",
    "        reduced_feats  = self.reduce_height_module(features , x.shape[3]//self.step_cols ) # [b , 1024 ,  256] width = 1024 , 256d latent code each.\n",
    "        \n",
    "        #print(\"reduced_feats out\" , reduced_feats.shape) # [b , 1024 , 256]        \n",
    "        reduced_feats = self.v_reproj(reduced_feats.view(reduced_feats.shape[0] , 1024 , 256,-1)).view(reduced_feats.shape[0]  , 256 , 256)\n",
    "        #print(\"reduced_feats\" , reduced_feats.shape)\n",
    "        output = self.transformer(reduced_feats)  # (b , max_pred , 90 )\n",
    "\n",
    "        batch_size = output.shape[0]\n",
    "        output = output.view( batch_size , self.max_predict_count , -1 ) # (b , 90 , 64*out_scale )        \n",
    "\n",
    "        out_box = self.vqt_box_head(output)\n",
    "        out_cls = self.vqt_cls_head(output).view(batch_size , self.max_predict_count , -1 )\n",
    "        \n",
    "        #print(\"out_box\" , out_box.shape) \n",
    "        #print(\"out_cls\" , out_cls.shape) \n",
    "        return out_box , out_cls\n",
    "    \n",
    "    #@torch.no_grad()\n",
    "    def find_match(self, gt , pred):\n",
    "        #print(\"gt\"  , gt)\n",
    "        gt_vec = torch.stack(gt).permute(1,0)\n",
    "        pred_vec = torch.stack(pred).permute(1,0)\n",
    "        loss_dist = torch.cdist(gt_vec , pred_vec)\n",
    "        \n",
    "        each_gt_pred_best_idx = torch.argmax(loss_dist, 0 )      \n",
    "        #print(\"each_gt_pred_best_idx\" , each_gt_pred_best_idx)          \n",
    "\n",
    "        return gt_vec[each_gt_pred_best_idx] , pred_vec\n",
    "        \n",
    "    def pack_visualize(self, gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , dv_btm_b ):\n",
    "        \n",
    "        sizes = [len(t) for t in gt_u_b]               \n",
    "        if isinstance(gt_u_b, torch.Tensor):\n",
    "            us = gt_u_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            us[1::2]+=gt_du_b.flatten()\n",
    "            us = torch.split(us.view(-1,2) , sizes)\n",
    "\n",
    "            tops = gt_vtop_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            tops[1::2]=gt_dvtop_b.flatten()\n",
    "            tops = torch.split(tops.view(-1,2) , sizes)\n",
    "\n",
    "            btms = gt_vbtm_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            btms[1::2]=dv_btm_b.flatten()\n",
    "            btms = torch.split(btms.view(-1,2) , sizes)\n",
    "\n",
    "        elif isinstance(gt_u_b, tuple) and all(isinstance(t, torch.Tensor) for t in gt_u_b):        \n",
    "            us = torch.cat(gt_u_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            us[1::2]+=torch.cat(gt_du_b).view(-1)\n",
    "            us = torch.split(us.view(-1,2) , sizes)\n",
    "\n",
    "            tops = torch.cat(gt_vtop_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            tops[1::2]=torch.cat(gt_dvtop_b).view(-1)\n",
    "            tops = torch.split(tops.view(-1,2) , sizes)\n",
    "\n",
    "            btms = torch.cat(gt_vbtm_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            btms[1::2]=torch.cat(dv_btm_b).view(-1)\n",
    "            btms = torch.split(btms.view(-1,2) , sizes)\n",
    "        else:\n",
    "            assert(\"Wrong Type.\")\n",
    "        \n",
    "        return us , tops ,btms\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def training_step(self , input_b ,batch_idx , optimizer_idx):\n",
    "        \n",
    "        img = input_b['image']        \n",
    "\n",
    "        img = input_b['image']\n",
    "        #h,w = img.shape[1:3]\n",
    "        out_box , out_cls = self.forward(img)  # [ batch , n , 5]\n",
    "        #out_cls =  out_cls.view(batch_size * self.max_predict_count , self.num_classes) # [batch size * max count , num_classes]\n",
    "        #print(\"out_box\" , out_box.shape)\n",
    "        #print(\"out_cls\" , out_cls.shape)\n",
    "        batch_size = out_box.shape[0]\n",
    "               \n",
    "        gt_u_b = unpad_data( input_b['u'])          \n",
    "        gt_vtop_b =unpad_data(input_b['v_top'])\n",
    "        gt_vbtm_b = unpad_data (input_b['v_btm'])\n",
    "        gt_du_b = unpad_data(input_b['du'])\n",
    "        gt_dvtop_b = unpad_data(input_b['dv_top'])\n",
    "        gt_dv_btm_b = unpad_data(input_b['dv_btm'])\n",
    "        \n",
    "\n",
    "        total_loss = 0\n",
    "        b_cnt = 0\n",
    "        for u,vtop,vbtm,du,dvtop, dvbtm , pred ,cls_b,gt_cls in zip(gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , gt_dv_btm_b , out_box , out_cls , input_b['u_grad']):\n",
    "            # match            \n",
    "            gt_box =  torch.vstack([vtop,vbtm,du ,dvtop , dvbtm]).permute(1,0)\n",
    "            \n",
    "            box_loss = torch.cdist( pred , gt_box , p=1)\n",
    "            cls_loss = - F.softmax( cls_b , -1)\n",
    "            cost_matrix = box_loss*self.box_cost + cls_loss * self.cls_cost\n",
    "            cost_matrix = cost_matrix.detach().cpu().numpy()\n",
    "            \n",
    "            row_idx  , col_idx = linear_sum_assignment(cost_matrix)\n",
    "            #print(\"idx\" , row_idx , col_idx)\n",
    "            #print(\"matched out\" , pred[row_idx])\n",
    "            #print(\"matched gt\" , gt_box[col_idx])\n",
    "            \n",
    "            matched_cls_loss = F.binary_cross_entropy_with_logits(cls_b[row_idx].view(-1) , gt_cls[col_idx])\n",
    "            total_loss += F.l1_loss(pred[row_idx] ,  gt_box[col_idx]) + F.binary_cross_entropy_with_logits(cls_b.view(-1), gt_cls  ) + matched_cls_loss\n",
    "            #total_loss += F.l1_loss(pred[row_idx] ,  gt_box[col_idx]) + matched_cls_loss\n",
    "\n",
    "            #cls_loss = F.binary_cross_entropy_with_logits( out_cls.view(-1 , self.max_predict_count) ,)\n",
    "            #matched_gt , matched_pred =  self.find_match( (u,vtop,vbtm,du,dvtop,btm , input_b['u_grad']),(pred[:,0],pred[:,1],pred[:,2],pred[:,3],pred[:,4],pred[:,5]))\n",
    "            with torch.no_grad():\n",
    "                #if self.current_epoch % 5 == 0  :                \n",
    "                if self.current_epoch % 5 == 0 and self.current_epoch > 0 :                \n",
    "                    save_path = create_folder( os.path.join(self.log_folder , f\"gt_ep_{self.current_epoch}-{b_cnt}\" ))\n",
    "                    gt_us , gt_tops , gt_btms = self.pack_visualize(u.view(1 , -1 ) , vtop , vbtm , du , dvtop , dvbtm )\n",
    "                    #print(\"gt_us , gt_tops , gt_btms\" , gt_us , gt_tops , gt_btms)\n",
    "                    vis_imgs = visualize_2d_single(gt_us , gt_tops , gt_btms , u_grad = gt_cls.view(1 , -1 ), imgs= img[b_cnt] , title=\"GT\",save_path=save_path )                \n",
    "                    \n",
    "                    save_path = create_folder( os.path.join(self.log_folder , f\"pred_ep_{self.current_epoch}-{b_cnt}\" ))\n",
    "                    pred_u = row_idx / self.max_predict_count\n",
    "                    pred_u = torch.from_numpy(pred_u.flatten()[np.newaxis,...]).to(u.device)\n",
    "                    #print(\"pred_u\" , pred_u)\n",
    "                    pred_us , pred_tops , pred_btms = self.pack_visualize(pred_u, pred[row_idx,0],pred[row_idx,1],pred[row_idx,2],pred[row_idx,3],pred[row_idx,4] )\n",
    "                    #print(\"pred_us , pred_tops , pred_btms\" , pred_us , pred_tops , pred_btms)\n",
    "                    vis_imgs = visualize_2d_single(pred_us , pred_tops , pred_btms , u_grad = cls_b.view(1 , -1 ) , imgs=  img[b_cnt] , title=\"Pred\" , save_path= save_path  )\n",
    "                    \n",
    "            #print(\"matched_gt\" , matched_gt)\n",
    "            #print(\"matched_pred\" , matched_pred)\n",
    "            #print(\"===================\")\n",
    "            #l1_loss += F.l1_loss(matched_gt , matched_pred)\n",
    "            b_cnt+=1\n",
    "            pass\n",
    "        '''\n",
    "        gt_vtop_b =torch.cat(unpad_data(input_b['v_top'])).view( -1)\n",
    "        gt_vbtm_b = torch.cat(unpad_data (input_b['v_btm'])).view( -1)\n",
    "        gt_du_b = torch.cat(unpad_data(input_b['du'])).view( -1)\n",
    "        gt_dvtop_b = torch.cat(unpad_data(input_b['dv_top'])).view( -1)\n",
    "        gt_dv_btm_b = torch.cat(unpad_data(input_b['dv_btm'])).view( -1)\n",
    "        gt_box =  torch.vstack(\n",
    "            [gt_vtop_b,\n",
    "             gt_vbtm_b,\n",
    "             gt_du_b ,\n",
    "             gt_dvtop_b , \n",
    "             gt_dv_btm_b\n",
    "             ]).permute(1,0)\n",
    "        print(\"gt_box\" , gt_box)\n",
    "\n",
    "        # cost matrix        \n",
    "        out_cls =  out_cls.view(batch_size * self.max_predict_count , self.num_classes) # [batch size * max count , num_classes]\n",
    "        out_box = out_box.view(-1 , 5 ) # [batch size * max count , 5]\n",
    "\n",
    "        gt_cls = torch.cat([ v for v  in input_b['u_grad']])\n",
    "        #gt_box = gt_box\n",
    "        print(\"input_b['u_grad']\" , input_b['u_grad'].shape)\n",
    "\n",
    "        box_loss = torch.cdist(out_box ,gt_box  , p =1)\n",
    "        print(\"box_loss\" , box_loss.shape)\n",
    "        cls_loss = -F.softmax(out_cls , -1)\n",
    "\n",
    "        cost_matrix = box_loss * self.box_cost + cls_loss*self.cls_cost \n",
    "        print(\"cost_matrix\" , cost_matrix.shape)\n",
    "        cost_matrix = cost_matrix.view(batch_size, self.max_predict_count, -1).detach().cpu().numpy()\n",
    "        sizes = [self.max_predict_count for v in input_b['u_grad']]       \n",
    "        print(\"cost_matrix\" , cost_matrix.shape)\n",
    "\n",
    "        #indices =[]\n",
    "        matched_gt_idx=[]\n",
    "        matched_out_idx=[]\n",
    "        for i , cost_b in enumerate(cost_matrix):\n",
    "            c = linear_sum_assignment(cost_b)\n",
    "            print(\"c\" , c)\n",
    "            #indices.append(c[0] )\n",
    "            matched_out_idx.append(c[0])\n",
    "            matched_gt_idx.append(c[1])\n",
    "\n",
    "            #_idx = torch.tensor(c[0])\n",
    "            #print(\"_idx\",_idx)\n",
    "            #print(\"gt box\" , gt_box[ _idx])\n",
    "            #print(\"matched box\" , out_box[ _idx] )\n",
    "        #indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n",
    "        #print(\"indices\" , indices)\n",
    "        #indices = [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "        #print(\"indices\" , indices)\n",
    "\n",
    "        #matched_gt = \n",
    "        box_loss = 0\n",
    "        cls_loss = 0\n",
    "        i=0\n",
    "        for gt_idx , out_idx in zip( matched_gt_idx , matched_out_idx):\n",
    "            box_loss+= F.l1_loss( gt_box[gt_idx+ self.max_predict_count*i ] , out_box[out_idx + self.max_predict_count*i ] )\n",
    "            i+=1\n",
    "            pass\n",
    "        '''\n",
    "\n",
    "\n",
    "        # match cls\n",
    "        '''\n",
    "        out_idx = torch.argwhere(out_cls.view(-1) > self.confidence_threshold ).view(-1)\n",
    "        batch_idx = torch.floor(out_idx / self.max_predict_count).to(torch.int64)\n",
    "        batch_split_cnt = torch.unique(batch_idx , return_counts=True)[1]\n",
    "        print(\"batch_split_cnt\" , batch_split_cnt)\n",
    "\n",
    "        out_idx = torch.argwhere(out_cls > self.confidence_threshold )\n",
    "        #print(\"out_idx\" ,  out_idx.shape , out_idx)\n",
    "        \n",
    "        out_box_over_threshold = out_box[out_idx[:, 0],out_idx[:, 0],:]\n",
    "        #print(\"out_box_over_threshold\" ,  out_box_over_threshold.shape)\n",
    "        pred_box = torch.zeros( out_box_over_threshold.shape[0] , 6,device=img.device)        \n",
    "        pred_box[:,1:] = out_box_over_threshold\n",
    "        pred_box [:, 0 ]  = (out_idx[:,1]+0.5) / self.max_predict_count\n",
    "        #print(\"pred_box\" ,  pred_box.shape)\n",
    "\n",
    "        batch_split_cnt = torch.unique(out_idx[:,0] , return_counts=True)[1]\n",
    "        #print(\"batch_split_cnt\" , batch_split_cnt)\n",
    "        pred_box_b = []        \n",
    "        prev = 0\n",
    "        for idx_cnt_b in batch_split_cnt:\n",
    "            pred_box_b.append(pred_box[prev : idx_cnt_b])\n",
    "            prev +=idx_cnt_b            \n",
    "            pass\n",
    "        '''\n",
    "        '''\n",
    "        #pred_box = out_box.view(-1,6)[out_idx]\n",
    "        out_box_over_threshold = out_box.view(-1,5)[out_idx]\n",
    "        pred_box = torch.zeros( out_box_over_threshold.shape[0] , 6)        \n",
    "        pred_box [:,1:]   = out_box_over_threshold\n",
    "        pred_box [:, 0 ]  = (out_idx+0.5) / w \n",
    "        #pred_box = out_box.view(-1,5)[out_idx]  # no u\n",
    "        pred_box = torch.split(pred_box , tuple(batch_split_cnt) )\n",
    "        print(\"pred_box\" , len(pred_box) , pred_box[0].shape)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        \n",
    "        # Log every 5 epochs\n",
    "        with torch.no_grad():\n",
    "            #if self.current_epoch % 5 == 0 and self.current_epoch > 0 :                \n",
    "            if self.current_epoch % 5 == 0  :                \n",
    "                save_path = create_folder( os.path.join(self.log_folder , f\"ep_{self.current_epoch}\" ))\n",
    "                gt_us , gt_tops , gt_btms = self.pack_visualize(gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , gt_dv_btm_b )\n",
    "                vis_imgs = visualize_2d(gt_us , gt_tops , gt_btms , u_grad = input_b['u_grad'] , imgs= img , title=\"GT\",save_path=save_path )                \n",
    "\n",
    "                pred_us , pred_tops , pred_btms = self.pack_visualize(out_box[:,0],out_box[:,1],out_box[:,2],out_box[:,3],out_box[:,4],out_box[:,5] )\n",
    "                vis_imgs = visualize_2d(pred_us , pred_tops , pred_btms , u_grad = out_cls.view(batch_size , -1) , imgs= img , title=\"Pred\" , save_path= save_path )\n",
    "                \n",
    "        print(\"gt_u\" , gt_u_b)\n",
    "        print(\"gt_vtop\" , gt_vtop_b)\n",
    "        print(\"gt_vbtm\" , gt_vbtm_b)\n",
    "        print(\"gt_du\" , gt_du_b)\n",
    "        print(\"gt_dvtop\" , gt_dvtop_b)\n",
    "        print(\"dv_btm\" , dv_btm_b)\n",
    "\n",
    "        l1_loss = 0\n",
    "        for u,vtop,vbtm,du,dvtop,btm , pred in zip(gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , gt_dv_btm_b , pred_box_b):\n",
    "            # match\n",
    "            #print(\"pred\" , pred.shape)\n",
    "            #print(\"gt u\" , u.shape)\n",
    "            cls_loss = F.binary_cross_entropy_with_logits( out_cls.view(-1 , self.max_predict_count) ,)\n",
    "            matched_gt , matched_pred =  self.find_match( (u,vtop,vbtm,du,dvtop,btm , input_b['u_grad']),(pred[:,0],pred[:,1],pred[:,2],pred[:,3],pred[:,4],pred[:,5]))\n",
    "           \n",
    "            print(\"matched_gt\" , matched_gt)\n",
    "            print(\"matched_pred\" , matched_pred)\n",
    "            print(\"===================\")\n",
    "            l1_loss += F.l1_loss(matched_gt , matched_pred)\n",
    "            pass\n",
    "\n",
    "        # loss:\n",
    "        #box_loss = \n",
    "\n",
    "        #cls_loss = F.binary_cross_entropy_with_logits( out_cls.view(-1 , self.max_predict_count) , input_b['u_grad'])\n",
    "        total_loss = l1_loss + cls_loss \n",
    "        '''\n",
    "        return total_loss\n",
    "        pass    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        backbone_opt = optim.Adam(self.backbone.parameters() , lr=0.00035)\n",
    "        transforms_opt = optim.Adam(self.transformer.parameters() , lr=0.000035)\n",
    "\n",
    "        return [backbone_opt , transforms_opt] , []\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "# Unit testing...\n",
    "\n",
    "# Test\n",
    "dm = CustomDataModule ( train_dir= f\"../anno/test_visiable_10_no_cross.json\" ,\n",
    "                        test_dir= f\"../anno/test_visiable_10_no_cross.json\" , padding_count=256 , use_aug=False , c= 0.95\n",
    "                       )\n",
    "m = VerticalQueryTransformer(max_predict_count = 256 , hidden_out=256 , load_weight=\"D:/OneDrive/OneDrive - NTHU/Layout/Horizon/0912_all_bk.pth\")\n",
    "#img = torch.randn((3,3,1024,512))\n",
    "#o = m(img)\n",
    "\n",
    "#print(o)\n",
    "trainer = pl.Trainer(accelerator='gpu' , devices=1 ,min_epochs=1, max_epochs=51 , precision=16 , fast_dev_run=False )\n",
    "trainer.fit(m , dm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['v_head.weight', 'v_head.bias', 'du_head.weight', 'du_head.bias', 'cls_head.weight', 'cls_head.bias', 'feature_extractor.encoder.conv1.1.weight', 'feature_extractor.encoder.bn1.weight', 'feature_extractor.encoder.bn1.bias', 'feature_extractor.encoder.bn1.running_mean', 'feature_extractor.encoder.bn1.running_var', 'feature_extractor.encoder.bn1.num_batches_tracked', 'feature_extractor.encoder.layer1.0.conv1.weight', 'feature_extractor.encoder.layer1.0.bn1.weight', 'feature_extractor.encoder.layer1.0.bn1.bias', 'feature_extractor.encoder.layer1.0.bn1.running_mean', 'feature_extractor.encoder.layer1.0.bn1.running_var', 'feature_extractor.encoder.layer1.0.bn1.num_batches_tracked', 'feature_extractor.encoder.layer1.0.conv2.1.weight', 'feature_extractor.encoder.layer1.0.bn2.weight', 'feature_extractor.encoder.layer1.0.bn2.bias', 'feature_extractor.encoder.layer1.0.bn2.running_mean', 'feature_extractor.encoder.layer1.0.bn2.running_var', 'feature_extractor.encoder.layer1.0.bn2.num_batches_tracked', 'feature_extractor.encoder.layer1.0.conv3.weight', 'feature_extractor.encoder.layer1.0.bn3.weight', 'feature_extractor.encoder.layer1.0.bn3.bias', 'feature_extractor.encoder.layer1.0.bn3.running_mean', 'feature_extractor.encoder.layer1.0.bn3.running_var', 'feature_extractor.encoder.layer1.0.bn3.num_batches_tracked', 'feature_extractor.encoder.layer1.0.downsample.0.weight', 'feature_extractor.encoder.layer1.0.downsample.1.weight', 'feature_extractor.encoder.layer1.0.downsample.1.bias', 'feature_extractor.encoder.layer1.0.downsample.1.running_mean', 'feature_extractor.encoder.layer1.0.downsample.1.running_var', 'feature_extractor.encoder.layer1.0.downsample.1.num_batches_tracked', 'feature_extractor.encoder.layer1.1.conv1.weight', 'feature_extractor.encoder.layer1.1.bn1.weight', 'feature_extractor.encoder.layer1.1.bn1.bias', 'feature_extractor.encoder.layer1.1.bn1.running_mean', 'feature_extractor.encoder.layer1.1.bn1.running_var', 'feature_extractor.encoder.layer1.1.bn1.num_batches_tracked', 'feature_extractor.encoder.layer1.1.conv2.1.weight', 'feature_extractor.encoder.layer1.1.bn2.weight', 'feature_extractor.encoder.layer1.1.bn2.bias', 'feature_extractor.encoder.layer1.1.bn2.running_mean', 'feature_extractor.encoder.layer1.1.bn2.running_var', 'feature_extractor.encoder.layer1.1.bn2.num_batches_tracked', 'feature_extractor.encoder.layer1.1.conv3.weight', 'feature_extractor.encoder.layer1.1.bn3.weight', 'feature_extractor.encoder.layer1.1.bn3.bias', 'feature_extractor.encoder.layer1.1.bn3.running_mean', 'feature_extractor.encoder.layer1.1.bn3.running_var', 'feature_extractor.encoder.layer1.1.bn3.num_batches_tracked', 'feature_extractor.encoder.layer1.2.conv1.weight', 'feature_extractor.encoder.layer1.2.bn1.weight', 'feature_extractor.encoder.layer1.2.bn1.bias', 'feature_extractor.encoder.layer1.2.bn1.running_mean', 'feature_extractor.encoder.layer1.2.bn1.running_var', 'feature_extractor.encoder.layer1.2.bn1.num_batches_tracked', 'feature_extractor.encoder.layer1.2.conv2.1.weight', 'feature_extractor.encoder.layer1.2.bn2.weight', 'feature_extractor.encoder.layer1.2.bn2.bias', 'feature_extractor.encoder.layer1.2.bn2.running_mean', 'feature_extractor.encoder.layer1.2.bn2.running_var', 'feature_extractor.encoder.layer1.2.bn2.num_batches_tracked', 'feature_extractor.encoder.layer1.2.conv3.weight', 'feature_extractor.encoder.layer1.2.bn3.weight', 'feature_extractor.encoder.layer1.2.bn3.bias', 'feature_extractor.encoder.layer1.2.bn3.running_mean', 'feature_extractor.encoder.layer1.2.bn3.running_var', 'feature_extractor.encoder.layer1.2.bn3.num_batches_tracked', 'feature_extractor.encoder.layer2.0.conv1.weight', 'feature_extractor.encoder.layer2.0.bn1.weight', 'feature_extractor.encoder.layer2.0.bn1.bias', 'feature_extractor.encoder.layer2.0.bn1.running_mean', 'feature_extractor.encoder.layer2.0.bn1.running_var', 'feature_extractor.encoder.layer2.0.bn1.num_batches_tracked', 'feature_extractor.encoder.layer2.0.conv2.1.weight', 'feature_extractor.encoder.layer2.0.bn2.weight', 'feature_extractor.encoder.layer2.0.bn2.bias', 'feature_extractor.encoder.layer2.0.bn2.running_mean', 'feature_extractor.encoder.layer2.0.bn2.running_var', 'feature_extractor.encoder.layer2.0.bn2.num_batches_tracked', 'feature_extractor.encoder.layer2.0.conv3.weight', 'feature_extractor.encoder.layer2.0.bn3.weight', 'feature_extractor.encoder.layer2.0.bn3.bias', 'feature_extractor.encoder.layer2.0.bn3.running_mean', 'feature_extractor.encoder.layer2.0.bn3.running_var', 'feature_extractor.encoder.layer2.0.bn3.num_batches_tracked', 'feature_extractor.encoder.layer2.0.downsample.0.weight', 'feature_extractor.encoder.layer2.0.downsample.1.weight', 'feature_extractor.encoder.layer2.0.downsample.1.bias', 'feature_extractor.encoder.layer2.0.downsample.1.running_mean', 'feature_extractor.encoder.layer2.0.downsample.1.running_var', 'feature_extractor.encoder.layer2.0.downsample.1.num_batches_tracked', 'feature_extractor.encoder.layer2.1.conv1.weight', 'feature_extractor.encoder.layer2.1.bn1.weight', 'feature_extractor.encoder.layer2.1.bn1.bias', 'feature_extractor.encoder.layer2.1.bn1.running_mean', 'feature_extractor.encoder.layer2.1.bn1.running_var', 'feature_extractor.encoder.layer2.1.bn1.num_batches_tracked', 'feature_extractor.encoder.layer2.1.conv2.1.weight', 'feature_extractor.encoder.layer2.1.bn2.weight', 'feature_extractor.encoder.layer2.1.bn2.bias', 'feature_extractor.encoder.layer2.1.bn2.running_mean', 'feature_extractor.encoder.layer2.1.bn2.running_var', 'feature_extractor.encoder.layer2.1.bn2.num_batches_tracked', 'feature_extractor.encoder.layer2.1.conv3.weight', 'feature_extractor.encoder.layer2.1.bn3.weight', 'feature_extractor.encoder.layer2.1.bn3.bias', 'feature_extractor.encoder.layer2.1.bn3.running_mean', 'feature_extractor.encoder.layer2.1.bn3.running_var', 'feature_extractor.encoder.layer2.1.bn3.num_batches_tracked', 'feature_extractor.encoder.layer2.2.conv1.weight', 'feature_extractor.encoder.layer2.2.bn1.weight', 'feature_extractor.encoder.layer2.2.bn1.bias', 'feature_extractor.encoder.layer2.2.bn1.running_mean', 'feature_extractor.encoder.layer2.2.bn1.running_var', 'feature_extractor.encoder.layer2.2.bn1.num_batches_tracked', 'feature_extractor.encoder.layer2.2.conv2.1.weight', 'feature_extractor.encoder.layer2.2.bn2.weight', 'feature_extractor.encoder.layer2.2.bn2.bias', 'feature_extractor.encoder.layer2.2.bn2.running_mean', 'feature_extractor.encoder.layer2.2.bn2.running_var', 'feature_extractor.encoder.layer2.2.bn2.num_batches_tracked', 'feature_extractor.encoder.layer2.2.conv3.weight', 'feature_extractor.encoder.layer2.2.bn3.weight', 'feature_extractor.encoder.layer2.2.bn3.bias', 'feature_extractor.encoder.layer2.2.bn3.running_mean', 'feature_extractor.encoder.layer2.2.bn3.running_var', 'feature_extractor.encoder.layer2.2.bn3.num_batches_tracked', 'feature_extractor.encoder.layer2.3.conv1.weight', 'feature_extractor.encoder.layer2.3.bn1.weight', 'feature_extractor.encoder.layer2.3.bn1.bias', 'feature_extractor.encoder.layer2.3.bn1.running_mean', 'feature_extractor.encoder.layer2.3.bn1.running_var', 'feature_extractor.encoder.layer2.3.bn1.num_batches_tracked', 'feature_extractor.encoder.layer2.3.conv2.1.weight', 'feature_extractor.encoder.layer2.3.bn2.weight', 'feature_extractor.encoder.layer2.3.bn2.bias', 'feature_extractor.encoder.layer2.3.bn2.running_mean', 'feature_extractor.encoder.layer2.3.bn2.running_var', 'feature_extractor.encoder.layer2.3.bn2.num_batches_tracked', 'feature_extractor.encoder.layer2.3.conv3.weight', 'feature_extractor.encoder.layer2.3.bn3.weight', 'feature_extractor.encoder.layer2.3.bn3.bias', 'feature_extractor.encoder.layer2.3.bn3.running_mean', 'feature_extractor.encoder.layer2.3.bn3.running_var', 'feature_extractor.encoder.layer2.3.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.0.conv1.weight', 'feature_extractor.encoder.layer3.0.bn1.weight', 'feature_extractor.encoder.layer3.0.bn1.bias', 'feature_extractor.encoder.layer3.0.bn1.running_mean', 'feature_extractor.encoder.layer3.0.bn1.running_var', 'feature_extractor.encoder.layer3.0.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.0.conv2.1.weight', 'feature_extractor.encoder.layer3.0.bn2.weight', 'feature_extractor.encoder.layer3.0.bn2.bias', 'feature_extractor.encoder.layer3.0.bn2.running_mean', 'feature_extractor.encoder.layer3.0.bn2.running_var', 'feature_extractor.encoder.layer3.0.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.0.conv3.weight', 'feature_extractor.encoder.layer3.0.bn3.weight', 'feature_extractor.encoder.layer3.0.bn3.bias', 'feature_extractor.encoder.layer3.0.bn3.running_mean', 'feature_extractor.encoder.layer3.0.bn3.running_var', 'feature_extractor.encoder.layer3.0.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.0.downsample.0.weight', 'feature_extractor.encoder.layer3.0.downsample.1.weight', 'feature_extractor.encoder.layer3.0.downsample.1.bias', 'feature_extractor.encoder.layer3.0.downsample.1.running_mean', 'feature_extractor.encoder.layer3.0.downsample.1.running_var', 'feature_extractor.encoder.layer3.0.downsample.1.num_batches_tracked', 'feature_extractor.encoder.layer3.1.conv1.weight', 'feature_extractor.encoder.layer3.1.bn1.weight', 'feature_extractor.encoder.layer3.1.bn1.bias', 'feature_extractor.encoder.layer3.1.bn1.running_mean', 'feature_extractor.encoder.layer3.1.bn1.running_var', 'feature_extractor.encoder.layer3.1.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.1.conv2.1.weight', 'feature_extractor.encoder.layer3.1.bn2.weight', 'feature_extractor.encoder.layer3.1.bn2.bias', 'feature_extractor.encoder.layer3.1.bn2.running_mean', 'feature_extractor.encoder.layer3.1.bn2.running_var', 'feature_extractor.encoder.layer3.1.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.1.conv3.weight', 'feature_extractor.encoder.layer3.1.bn3.weight', 'feature_extractor.encoder.layer3.1.bn3.bias', 'feature_extractor.encoder.layer3.1.bn3.running_mean', 'feature_extractor.encoder.layer3.1.bn3.running_var', 'feature_extractor.encoder.layer3.1.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.2.conv1.weight', 'feature_extractor.encoder.layer3.2.bn1.weight', 'feature_extractor.encoder.layer3.2.bn1.bias', 'feature_extractor.encoder.layer3.2.bn1.running_mean', 'feature_extractor.encoder.layer3.2.bn1.running_var', 'feature_extractor.encoder.layer3.2.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.2.conv2.1.weight', 'feature_extractor.encoder.layer3.2.bn2.weight', 'feature_extractor.encoder.layer3.2.bn2.bias', 'feature_extractor.encoder.layer3.2.bn2.running_mean', 'feature_extractor.encoder.layer3.2.bn2.running_var', 'feature_extractor.encoder.layer3.2.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.2.conv3.weight', 'feature_extractor.encoder.layer3.2.bn3.weight', 'feature_extractor.encoder.layer3.2.bn3.bias', 'feature_extractor.encoder.layer3.2.bn3.running_mean', 'feature_extractor.encoder.layer3.2.bn3.running_var', 'feature_extractor.encoder.layer3.2.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.3.conv1.weight', 'feature_extractor.encoder.layer3.3.bn1.weight', 'feature_extractor.encoder.layer3.3.bn1.bias', 'feature_extractor.encoder.layer3.3.bn1.running_mean', 'feature_extractor.encoder.layer3.3.bn1.running_var', 'feature_extractor.encoder.layer3.3.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.3.conv2.1.weight', 'feature_extractor.encoder.layer3.3.bn2.weight', 'feature_extractor.encoder.layer3.3.bn2.bias', 'feature_extractor.encoder.layer3.3.bn2.running_mean', 'feature_extractor.encoder.layer3.3.bn2.running_var', 'feature_extractor.encoder.layer3.3.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.3.conv3.weight', 'feature_extractor.encoder.layer3.3.bn3.weight', 'feature_extractor.encoder.layer3.3.bn3.bias', 'feature_extractor.encoder.layer3.3.bn3.running_mean', 'feature_extractor.encoder.layer3.3.bn3.running_var', 'feature_extractor.encoder.layer3.3.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.4.conv1.weight', 'feature_extractor.encoder.layer3.4.bn1.weight', 'feature_extractor.encoder.layer3.4.bn1.bias', 'feature_extractor.encoder.layer3.4.bn1.running_mean', 'feature_extractor.encoder.layer3.4.bn1.running_var', 'feature_extractor.encoder.layer3.4.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.4.conv2.1.weight', 'feature_extractor.encoder.layer3.4.bn2.weight', 'feature_extractor.encoder.layer3.4.bn2.bias', 'feature_extractor.encoder.layer3.4.bn2.running_mean', 'feature_extractor.encoder.layer3.4.bn2.running_var', 'feature_extractor.encoder.layer3.4.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.4.conv3.weight', 'feature_extractor.encoder.layer3.4.bn3.weight', 'feature_extractor.encoder.layer3.4.bn3.bias', 'feature_extractor.encoder.layer3.4.bn3.running_mean', 'feature_extractor.encoder.layer3.4.bn3.running_var', 'feature_extractor.encoder.layer3.4.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.5.conv1.weight', 'feature_extractor.encoder.layer3.5.bn1.weight', 'feature_extractor.encoder.layer3.5.bn1.bias', 'feature_extractor.encoder.layer3.5.bn1.running_mean', 'feature_extractor.encoder.layer3.5.bn1.running_var', 'feature_extractor.encoder.layer3.5.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.5.conv2.1.weight', 'feature_extractor.encoder.layer3.5.bn2.weight', 'feature_extractor.encoder.layer3.5.bn2.bias', 'feature_extractor.encoder.layer3.5.bn2.running_mean', 'feature_extractor.encoder.layer3.5.bn2.running_var', 'feature_extractor.encoder.layer3.5.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.5.conv3.weight', 'feature_extractor.encoder.layer3.5.bn3.weight', 'feature_extractor.encoder.layer3.5.bn3.bias', 'feature_extractor.encoder.layer3.5.bn3.running_mean', 'feature_extractor.encoder.layer3.5.bn3.running_var', 'feature_extractor.encoder.layer3.5.bn3.num_batches_tracked', 'feature_extractor.encoder.layer4.0.conv1.weight', 'feature_extractor.encoder.layer4.0.bn1.weight', 'feature_extractor.encoder.layer4.0.bn1.bias', 'feature_extractor.encoder.layer4.0.bn1.running_mean', 'feature_extractor.encoder.layer4.0.bn1.running_var', 'feature_extractor.encoder.layer4.0.bn1.num_batches_tracked', 'feature_extractor.encoder.layer4.0.conv2.1.weight', 'feature_extractor.encoder.layer4.0.bn2.weight', 'feature_extractor.encoder.layer4.0.bn2.bias', 'feature_extractor.encoder.layer4.0.bn2.running_mean', 'feature_extractor.encoder.layer4.0.bn2.running_var', 'feature_extractor.encoder.layer4.0.bn2.num_batches_tracked', 'feature_extractor.encoder.layer4.0.conv3.weight', 'feature_extractor.encoder.layer4.0.bn3.weight', 'feature_extractor.encoder.layer4.0.bn3.bias', 'feature_extractor.encoder.layer4.0.bn3.running_mean', 'feature_extractor.encoder.layer4.0.bn3.running_var', 'feature_extractor.encoder.layer4.0.bn3.num_batches_tracked', 'feature_extractor.encoder.layer4.0.downsample.0.weight', 'feature_extractor.encoder.layer4.0.downsample.1.weight', 'feature_extractor.encoder.layer4.0.downsample.1.bias', 'feature_extractor.encoder.layer4.0.downsample.1.running_mean', 'feature_extractor.encoder.layer4.0.downsample.1.running_var', 'feature_extractor.encoder.layer4.0.downsample.1.num_batches_tracked', 'feature_extractor.encoder.layer4.1.conv1.weight', 'feature_extractor.encoder.layer4.1.bn1.weight', 'feature_extractor.encoder.layer4.1.bn1.bias', 'feature_extractor.encoder.layer4.1.bn1.running_mean', 'feature_extractor.encoder.layer4.1.bn1.running_var', 'feature_extractor.encoder.layer4.1.bn1.num_batches_tracked', 'feature_extractor.encoder.layer4.1.conv2.1.weight', 'feature_extractor.encoder.layer4.1.bn2.weight', 'feature_extractor.encoder.layer4.1.bn2.bias', 'feature_extractor.encoder.layer4.1.bn2.running_mean', 'feature_extractor.encoder.layer4.1.bn2.running_var', 'feature_extractor.encoder.layer4.1.bn2.num_batches_tracked', 'feature_extractor.encoder.layer4.1.conv3.weight', 'feature_extractor.encoder.layer4.1.bn3.weight', 'feature_extractor.encoder.layer4.1.bn3.bias', 'feature_extractor.encoder.layer4.1.bn3.running_mean', 'feature_extractor.encoder.layer4.1.bn3.running_var', 'feature_extractor.encoder.layer4.1.bn3.num_batches_tracked', 'feature_extractor.encoder.layer4.2.conv1.weight', 'feature_extractor.encoder.layer4.2.bn1.weight', 'feature_extractor.encoder.layer4.2.bn1.bias', 'feature_extractor.encoder.layer4.2.bn1.running_mean', 'feature_extractor.encoder.layer4.2.bn1.running_var', 'feature_extractor.encoder.layer4.2.bn1.num_batches_tracked', 'feature_extractor.encoder.layer4.2.conv2.1.weight', 'feature_extractor.encoder.layer4.2.bn2.weight', 'feature_extractor.encoder.layer4.2.bn2.bias', 'feature_extractor.encoder.layer4.2.bn2.running_mean', 'feature_extractor.encoder.layer4.2.bn2.running_var', 'feature_extractor.encoder.layer4.2.bn2.num_batches_tracked', 'feature_extractor.encoder.layer4.2.conv3.weight', 'feature_extractor.encoder.layer4.2.bn3.weight', 'feature_extractor.encoder.layer4.2.bn3.bias', 'feature_extractor.encoder.layer4.2.bn3.running_mean', 'feature_extractor.encoder.layer4.2.bn3.running_var', 'feature_extractor.encoder.layer4.2.bn3.num_batches_tracked', 'reduce_height_module.ghc_lst.0.layer.0.layers.0.1.weight', 'reduce_height_module.ghc_lst.0.layer.0.layers.0.1.bias', 'reduce_height_module.ghc_lst.0.layer.0.layers.1.weight', 'reduce_height_module.ghc_lst.0.layer.0.layers.1.bias', 'reduce_height_module.ghc_lst.0.layer.0.layers.1.running_mean', 'reduce_height_module.ghc_lst.0.layer.0.layers.1.running_var', 'reduce_height_module.ghc_lst.0.layer.0.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.0.layer.1.layers.0.1.weight', 'reduce_height_module.ghc_lst.0.layer.1.layers.0.1.bias', 'reduce_height_module.ghc_lst.0.layer.1.layers.1.weight', 'reduce_height_module.ghc_lst.0.layer.1.layers.1.bias', 'reduce_height_module.ghc_lst.0.layer.1.layers.1.running_mean', 'reduce_height_module.ghc_lst.0.layer.1.layers.1.running_var', 'reduce_height_module.ghc_lst.0.layer.1.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.0.layer.2.layers.0.1.weight', 'reduce_height_module.ghc_lst.0.layer.2.layers.0.1.bias', 'reduce_height_module.ghc_lst.0.layer.2.layers.1.weight', 'reduce_height_module.ghc_lst.0.layer.2.layers.1.bias', 'reduce_height_module.ghc_lst.0.layer.2.layers.1.running_mean', 'reduce_height_module.ghc_lst.0.layer.2.layers.1.running_var', 'reduce_height_module.ghc_lst.0.layer.2.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.0.layer.3.layers.0.1.weight', 'reduce_height_module.ghc_lst.0.layer.3.layers.0.1.bias', 'reduce_height_module.ghc_lst.0.layer.3.layers.1.weight', 'reduce_height_module.ghc_lst.0.layer.3.layers.1.bias', 'reduce_height_module.ghc_lst.0.layer.3.layers.1.running_mean', 'reduce_height_module.ghc_lst.0.layer.3.layers.1.running_var', 'reduce_height_module.ghc_lst.0.layer.3.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.1.layer.0.layers.0.1.weight', 'reduce_height_module.ghc_lst.1.layer.0.layers.0.1.bias', 'reduce_height_module.ghc_lst.1.layer.0.layers.1.weight', 'reduce_height_module.ghc_lst.1.layer.0.layers.1.bias', 'reduce_height_module.ghc_lst.1.layer.0.layers.1.running_mean', 'reduce_height_module.ghc_lst.1.layer.0.layers.1.running_var', 'reduce_height_module.ghc_lst.1.layer.0.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.1.layer.1.layers.0.1.weight', 'reduce_height_module.ghc_lst.1.layer.1.layers.0.1.bias', 'reduce_height_module.ghc_lst.1.layer.1.layers.1.weight', 'reduce_height_module.ghc_lst.1.layer.1.layers.1.bias', 'reduce_height_module.ghc_lst.1.layer.1.layers.1.running_mean', 'reduce_height_module.ghc_lst.1.layer.1.layers.1.running_var', 'reduce_height_module.ghc_lst.1.layer.1.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.1.layer.2.layers.0.1.weight', 'reduce_height_module.ghc_lst.1.layer.2.layers.0.1.bias', 'reduce_height_module.ghc_lst.1.layer.2.layers.1.weight', 'reduce_height_module.ghc_lst.1.layer.2.layers.1.bias', 'reduce_height_module.ghc_lst.1.layer.2.layers.1.running_mean', 'reduce_height_module.ghc_lst.1.layer.2.layers.1.running_var', 'reduce_height_module.ghc_lst.1.layer.2.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.1.layer.3.layers.0.1.weight', 'reduce_height_module.ghc_lst.1.layer.3.layers.0.1.bias', 'reduce_height_module.ghc_lst.1.layer.3.layers.1.weight', 'reduce_height_module.ghc_lst.1.layer.3.layers.1.bias', 'reduce_height_module.ghc_lst.1.layer.3.layers.1.running_mean', 'reduce_height_module.ghc_lst.1.layer.3.layers.1.running_var', 'reduce_height_module.ghc_lst.1.layer.3.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.2.layer.0.layers.0.1.weight', 'reduce_height_module.ghc_lst.2.layer.0.layers.0.1.bias', 'reduce_height_module.ghc_lst.2.layer.0.layers.1.weight', 'reduce_height_module.ghc_lst.2.layer.0.layers.1.bias', 'reduce_height_module.ghc_lst.2.layer.0.layers.1.running_mean', 'reduce_height_module.ghc_lst.2.layer.0.layers.1.running_var', 'reduce_height_module.ghc_lst.2.layer.0.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.2.layer.1.layers.0.1.weight', 'reduce_height_module.ghc_lst.2.layer.1.layers.0.1.bias', 'reduce_height_module.ghc_lst.2.layer.1.layers.1.weight', 'reduce_height_module.ghc_lst.2.layer.1.layers.1.bias', 'reduce_height_module.ghc_lst.2.layer.1.layers.1.running_mean', 'reduce_height_module.ghc_lst.2.layer.1.layers.1.running_var', 'reduce_height_module.ghc_lst.2.layer.1.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.2.layer.2.layers.0.1.weight', 'reduce_height_module.ghc_lst.2.layer.2.layers.0.1.bias', 'reduce_height_module.ghc_lst.2.layer.2.layers.1.weight', 'reduce_height_module.ghc_lst.2.layer.2.layers.1.bias', 'reduce_height_module.ghc_lst.2.layer.2.layers.1.running_mean', 'reduce_height_module.ghc_lst.2.layer.2.layers.1.running_var', 'reduce_height_module.ghc_lst.2.layer.2.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.2.layer.3.layers.0.1.weight', 'reduce_height_module.ghc_lst.2.layer.3.layers.0.1.bias', 'reduce_height_module.ghc_lst.2.layer.3.layers.1.weight', 'reduce_height_module.ghc_lst.2.layer.3.layers.1.bias', 'reduce_height_module.ghc_lst.2.layer.3.layers.1.running_mean', 'reduce_height_module.ghc_lst.2.layer.3.layers.1.running_var', 'reduce_height_module.ghc_lst.2.layer.3.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.3.layer.0.layers.0.1.weight', 'reduce_height_module.ghc_lst.3.layer.0.layers.0.1.bias', 'reduce_height_module.ghc_lst.3.layer.0.layers.1.weight', 'reduce_height_module.ghc_lst.3.layer.0.layers.1.bias', 'reduce_height_module.ghc_lst.3.layer.0.layers.1.running_mean', 'reduce_height_module.ghc_lst.3.layer.0.layers.1.running_var', 'reduce_height_module.ghc_lst.3.layer.0.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.3.layer.1.layers.0.1.weight', 'reduce_height_module.ghc_lst.3.layer.1.layers.0.1.bias', 'reduce_height_module.ghc_lst.3.layer.1.layers.1.weight', 'reduce_height_module.ghc_lst.3.layer.1.layers.1.bias', 'reduce_height_module.ghc_lst.3.layer.1.layers.1.running_mean', 'reduce_height_module.ghc_lst.3.layer.1.layers.1.running_var', 'reduce_height_module.ghc_lst.3.layer.1.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.3.layer.2.layers.0.1.weight', 'reduce_height_module.ghc_lst.3.layer.2.layers.0.1.bias', 'reduce_height_module.ghc_lst.3.layer.2.layers.1.weight', 'reduce_height_module.ghc_lst.3.layer.2.layers.1.bias', 'reduce_height_module.ghc_lst.3.layer.2.layers.1.running_mean', 'reduce_height_module.ghc_lst.3.layer.2.layers.1.running_var', 'reduce_height_module.ghc_lst.3.layer.2.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.3.layer.3.layers.0.1.weight', 'reduce_height_module.ghc_lst.3.layer.3.layers.0.1.bias', 'reduce_height_module.ghc_lst.3.layer.3.layers.1.weight', 'reduce_height_module.ghc_lst.3.layer.3.layers.1.bias', 'reduce_height_module.ghc_lst.3.layer.3.layers.1.running_mean', 'reduce_height_module.ghc_lst.3.layer.3.layers.1.running_var', 'reduce_height_module.ghc_lst.3.layer.3.layers.1.num_batches_tracked', 'bi_rnn.weight_ih_l0', 'bi_rnn.weight_hh_l0', 'bi_rnn.bias_ih_l0', 'bi_rnn.bias_hh_l0', 'bi_rnn.weight_ih_l0_reverse', 'bi_rnn.weight_hh_l0_reverse', 'bi_rnn.bias_ih_l0_reverse', 'bi_rnn.bias_hh_l0_reverse', 'bi_rnn.weight_ih_l1', 'bi_rnn.weight_hh_l1', 'bi_rnn.bias_ih_l1', 'bi_rnn.bias_hh_l1', 'bi_rnn.weight_ih_l1_reverse', 'bi_rnn.weight_hh_l1_reverse', 'bi_rnn.bias_ih_l1_reverse', 'bi_rnn.bias_hh_l1_reverse', 'linear.weight', 'linear.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['backbone.encoder.conv1.weight', 'backbone.encoder.bn1.weight', 'backbone.encoder.bn1.bias', 'backbone.encoder.bn1.running_mean', 'backbone.encoder.bn1.running_var', 'backbone.encoder.layer1.0.conv1.weight', 'backbone.encoder.layer1.0.bn1.weight', 'backbone.encoder.layer1.0.bn1.bias', 'backbone.encoder.layer1.0.bn1.running_mean', 'backbone.encoder.layer1.0.bn1.running_var', 'backbone.encoder.layer1.0.conv2.weight', 'backbone.encoder.layer1.0.bn2.weight', 'backbone.encoder.layer1.0.bn2.bias', 'backbone.encoder.layer1.0.bn2.running_mean', 'backbone.encoder.layer1.0.bn2.running_var', 'backbone.encoder.layer1.0.conv3.weight', 'backbone.encoder.layer1.0.bn3.weight', 'backbone.encoder.layer1.0.bn3.bias', 'backbone.encoder.layer1.0.bn3.running_mean', 'backbone.encoder.layer1.0.bn3.running_var', 'backbone.encoder.layer1.0.downsample.0.weight', 'backbone.encoder.layer1.0.downsample.1.weight', 'backbone.encoder.layer1.0.downsample.1.bias', 'backbone.encoder.layer1.0.downsample.1.running_mean', 'backbone.encoder.layer1.0.downsample.1.running_var', 'backbone.encoder.layer1.1.conv1.weight', 'backbone.encoder.layer1.1.bn1.weight', 'backbone.encoder.layer1.1.bn1.bias', 'backbone.encoder.layer1.1.bn1.running_mean', 'backbone.encoder.layer1.1.bn1.running_var', 'backbone.encoder.layer1.1.conv2.weight', 'backbone.encoder.layer1.1.bn2.weight', 'backbone.encoder.layer1.1.bn2.bias', 'backbone.encoder.layer1.1.bn2.running_mean', 'backbone.encoder.layer1.1.bn2.running_var', 'backbone.encoder.layer1.1.conv3.weight', 'backbone.encoder.layer1.1.bn3.weight', 'backbone.encoder.layer1.1.bn3.bias', 'backbone.encoder.layer1.1.bn3.running_mean', 'backbone.encoder.layer1.1.bn3.running_var', 'backbone.encoder.layer1.2.conv1.weight', 'backbone.encoder.layer1.2.bn1.weight', 'backbone.encoder.layer1.2.bn1.bias', 'backbone.encoder.layer1.2.bn1.running_mean', 'backbone.encoder.layer1.2.bn1.running_var', 'backbone.encoder.layer1.2.conv2.weight', 'backbone.encoder.layer1.2.bn2.weight', 'backbone.encoder.layer1.2.bn2.bias', 'backbone.encoder.layer1.2.bn2.running_mean', 'backbone.encoder.layer1.2.bn2.running_var', 'backbone.encoder.layer1.2.conv3.weight', 'backbone.encoder.layer1.2.bn3.weight', 'backbone.encoder.layer1.2.bn3.bias', 'backbone.encoder.layer1.2.bn3.running_mean', 'backbone.encoder.layer1.2.bn3.running_var', 'backbone.encoder.layer2.0.conv1.weight', 'backbone.encoder.layer2.0.bn1.weight', 'backbone.encoder.layer2.0.bn1.bias', 'backbone.encoder.layer2.0.bn1.running_mean', 'backbone.encoder.layer2.0.bn1.running_var', 'backbone.encoder.layer2.0.conv2.weight', 'backbone.encoder.layer2.0.bn2.weight', 'backbone.encoder.layer2.0.bn2.bias', 'backbone.encoder.layer2.0.bn2.running_mean', 'backbone.encoder.layer2.0.bn2.running_var', 'backbone.encoder.layer2.0.conv3.weight', 'backbone.encoder.layer2.0.bn3.weight', 'backbone.encoder.layer2.0.bn3.bias', 'backbone.encoder.layer2.0.bn3.running_mean', 'backbone.encoder.layer2.0.bn3.running_var', 'backbone.encoder.layer2.0.downsample.0.weight', 'backbone.encoder.layer2.0.downsample.1.weight', 'backbone.encoder.layer2.0.downsample.1.bias', 'backbone.encoder.layer2.0.downsample.1.running_mean', 'backbone.encoder.layer2.0.downsample.1.running_var', 'backbone.encoder.layer2.1.conv1.weight', 'backbone.encoder.layer2.1.bn1.weight', 'backbone.encoder.layer2.1.bn1.bias', 'backbone.encoder.layer2.1.bn1.running_mean', 'backbone.encoder.layer2.1.bn1.running_var', 'backbone.encoder.layer2.1.conv2.weight', 'backbone.encoder.layer2.1.bn2.weight', 'backbone.encoder.layer2.1.bn2.bias', 'backbone.encoder.layer2.1.bn2.running_mean', 'backbone.encoder.layer2.1.bn2.running_var', 'backbone.encoder.layer2.1.conv3.weight', 'backbone.encoder.layer2.1.bn3.weight', 'backbone.encoder.layer2.1.bn3.bias', 'backbone.encoder.layer2.1.bn3.running_mean', 'backbone.encoder.layer2.1.bn3.running_var', 'backbone.encoder.layer2.2.conv1.weight', 'backbone.encoder.layer2.2.bn1.weight', 'backbone.encoder.layer2.2.bn1.bias', 'backbone.encoder.layer2.2.bn1.running_mean', 'backbone.encoder.layer2.2.bn1.running_var', 'backbone.encoder.layer2.2.conv2.weight', 'backbone.encoder.layer2.2.bn2.weight', 'backbone.encoder.layer2.2.bn2.bias', 'backbone.encoder.layer2.2.bn2.running_mean', 'backbone.encoder.layer2.2.bn2.running_var', 'backbone.encoder.layer2.2.conv3.weight', 'backbone.encoder.layer2.2.bn3.weight', 'backbone.encoder.layer2.2.bn3.bias', 'backbone.encoder.layer2.2.bn3.running_mean', 'backbone.encoder.layer2.2.bn3.running_var', 'backbone.encoder.layer2.3.conv1.weight', 'backbone.encoder.layer2.3.bn1.weight', 'backbone.encoder.layer2.3.bn1.bias', 'backbone.encoder.layer2.3.bn1.running_mean', 'backbone.encoder.layer2.3.bn1.running_var', 'backbone.encoder.layer2.3.conv2.weight', 'backbone.encoder.layer2.3.bn2.weight', 'backbone.encoder.layer2.3.bn2.bias', 'backbone.encoder.layer2.3.bn2.running_mean', 'backbone.encoder.layer2.3.bn2.running_var', 'backbone.encoder.layer2.3.conv3.weight', 'backbone.encoder.layer2.3.bn3.weight', 'backbone.encoder.layer2.3.bn3.bias', 'backbone.encoder.layer2.3.bn3.running_mean', 'backbone.encoder.layer2.3.bn3.running_var', 'backbone.encoder.layer3.0.conv1.weight', 'backbone.encoder.layer3.0.bn1.weight', 'backbone.encoder.layer3.0.bn1.bias', 'backbone.encoder.layer3.0.bn1.running_mean', 'backbone.encoder.layer3.0.bn1.running_var', 'backbone.encoder.layer3.0.conv2.weight', 'backbone.encoder.layer3.0.bn2.weight', 'backbone.encoder.layer3.0.bn2.bias', 'backbone.encoder.layer3.0.bn2.running_mean', 'backbone.encoder.layer3.0.bn2.running_var', 'backbone.encoder.layer3.0.conv3.weight', 'backbone.encoder.layer3.0.bn3.weight', 'backbone.encoder.layer3.0.bn3.bias', 'backbone.encoder.layer3.0.bn3.running_mean', 'backbone.encoder.layer3.0.bn3.running_var', 'backbone.encoder.layer3.0.downsample.0.weight', 'backbone.encoder.layer3.0.downsample.1.weight', 'backbone.encoder.layer3.0.downsample.1.bias', 'backbone.encoder.layer3.0.downsample.1.running_mean', 'backbone.encoder.layer3.0.downsample.1.running_var', 'backbone.encoder.layer3.1.conv1.weight', 'backbone.encoder.layer3.1.bn1.weight', 'backbone.encoder.layer3.1.bn1.bias', 'backbone.encoder.layer3.1.bn1.running_mean', 'backbone.encoder.layer3.1.bn1.running_var', 'backbone.encoder.layer3.1.conv2.weight', 'backbone.encoder.layer3.1.bn2.weight', 'backbone.encoder.layer3.1.bn2.bias', 'backbone.encoder.layer3.1.bn2.running_mean', 'backbone.encoder.layer3.1.bn2.running_var', 'backbone.encoder.layer3.1.conv3.weight', 'backbone.encoder.layer3.1.bn3.weight', 'backbone.encoder.layer3.1.bn3.bias', 'backbone.encoder.layer3.1.bn3.running_mean', 'backbone.encoder.layer3.1.bn3.running_var', 'backbone.encoder.layer3.2.conv1.weight', 'backbone.encoder.layer3.2.bn1.weight', 'backbone.encoder.layer3.2.bn1.bias', 'backbone.encoder.layer3.2.bn1.running_mean', 'backbone.encoder.layer3.2.bn1.running_var', 'backbone.encoder.layer3.2.conv2.weight', 'backbone.encoder.layer3.2.bn2.weight', 'backbone.encoder.layer3.2.bn2.bias', 'backbone.encoder.layer3.2.bn2.running_mean', 'backbone.encoder.layer3.2.bn2.running_var', 'backbone.encoder.layer3.2.conv3.weight', 'backbone.encoder.layer3.2.bn3.weight', 'backbone.encoder.layer3.2.bn3.bias', 'backbone.encoder.layer3.2.bn3.running_mean', 'backbone.encoder.layer3.2.bn3.running_var', 'backbone.encoder.layer3.3.conv1.weight', 'backbone.encoder.layer3.3.bn1.weight', 'backbone.encoder.layer3.3.bn1.bias', 'backbone.encoder.layer3.3.bn1.running_mean', 'backbone.encoder.layer3.3.bn1.running_var', 'backbone.encoder.layer3.3.conv2.weight', 'backbone.encoder.layer3.3.bn2.weight', 'backbone.encoder.layer3.3.bn2.bias', 'backbone.encoder.layer3.3.bn2.running_mean', 'backbone.encoder.layer3.3.bn2.running_var', 'backbone.encoder.layer3.3.conv3.weight', 'backbone.encoder.layer3.3.bn3.weight', 'backbone.encoder.layer3.3.bn3.bias', 'backbone.encoder.layer3.3.bn3.running_mean', 'backbone.encoder.layer3.3.bn3.running_var', 'backbone.encoder.layer3.4.conv1.weight', 'backbone.encoder.layer3.4.bn1.weight', 'backbone.encoder.layer3.4.bn1.bias', 'backbone.encoder.layer3.4.bn1.running_mean', 'backbone.encoder.layer3.4.bn1.running_var', 'backbone.encoder.layer3.4.conv2.weight', 'backbone.encoder.layer3.4.bn2.weight', 'backbone.encoder.layer3.4.bn2.bias', 'backbone.encoder.layer3.4.bn2.running_mean', 'backbone.encoder.layer3.4.bn2.running_var', 'backbone.encoder.layer3.4.conv3.weight', 'backbone.encoder.layer3.4.bn3.weight', 'backbone.encoder.layer3.4.bn3.bias', 'backbone.encoder.layer3.4.bn3.running_mean', 'backbone.encoder.layer3.4.bn3.running_var', 'backbone.encoder.layer3.5.conv1.weight', 'backbone.encoder.layer3.5.bn1.weight', 'backbone.encoder.layer3.5.bn1.bias', 'backbone.encoder.layer3.5.bn1.running_mean', 'backbone.encoder.layer3.5.bn1.running_var', 'backbone.encoder.layer3.5.conv2.weight', 'backbone.encoder.layer3.5.bn2.weight', 'backbone.encoder.layer3.5.bn2.bias', 'backbone.encoder.layer3.5.bn2.running_mean', 'backbone.encoder.layer3.5.bn2.running_var', 'backbone.encoder.layer3.5.conv3.weight', 'backbone.encoder.layer3.5.bn3.weight', 'backbone.encoder.layer3.5.bn3.bias', 'backbone.encoder.layer3.5.bn3.running_mean', 'backbone.encoder.layer3.5.bn3.running_var', 'backbone.encoder.layer4.0.conv1.weight', 'backbone.encoder.layer4.0.bn1.weight', 'backbone.encoder.layer4.0.bn1.bias', 'backbone.encoder.layer4.0.bn1.running_mean', 'backbone.encoder.layer4.0.bn1.running_var', 'backbone.encoder.layer4.0.conv2.weight', 'backbone.encoder.layer4.0.bn2.weight', 'backbone.encoder.layer4.0.bn2.bias', 'backbone.encoder.layer4.0.bn2.running_mean', 'backbone.encoder.layer4.0.bn2.running_var', 'backbone.encoder.layer4.0.conv3.weight', 'backbone.encoder.layer4.0.bn3.weight', 'backbone.encoder.layer4.0.bn3.bias', 'backbone.encoder.layer4.0.bn3.running_mean', 'backbone.encoder.layer4.0.bn3.running_var', 'backbone.encoder.layer4.0.downsample.0.weight', 'backbone.encoder.layer4.0.downsample.1.weight', 'backbone.encoder.layer4.0.downsample.1.bias', 'backbone.encoder.layer4.0.downsample.1.running_mean', 'backbone.encoder.layer4.0.downsample.1.running_var', 'backbone.encoder.layer4.1.conv1.weight', 'backbone.encoder.layer4.1.bn1.weight', 'backbone.encoder.layer4.1.bn1.bias', 'backbone.encoder.layer4.1.bn1.running_mean', 'backbone.encoder.layer4.1.bn1.running_var', 'backbone.encoder.layer4.1.conv2.weight', 'backbone.encoder.layer4.1.bn2.weight', 'backbone.encoder.layer4.1.bn2.bias', 'backbone.encoder.layer4.1.bn2.running_mean', 'backbone.encoder.layer4.1.bn2.running_var', 'backbone.encoder.layer4.1.conv3.weight', 'backbone.encoder.layer4.1.bn3.weight', 'backbone.encoder.layer4.1.bn3.bias', 'backbone.encoder.layer4.1.bn3.running_mean', 'backbone.encoder.layer4.1.bn3.running_var', 'backbone.encoder.layer4.2.conv1.weight', 'backbone.encoder.layer4.2.bn1.weight', 'backbone.encoder.layer4.2.bn1.bias', 'backbone.encoder.layer4.2.bn1.running_mean', 'backbone.encoder.layer4.2.bn1.running_var', 'backbone.encoder.layer4.2.conv2.weight', 'backbone.encoder.layer4.2.bn2.weight', 'backbone.encoder.layer4.2.bn2.bias', 'backbone.encoder.layer4.2.bn2.running_mean', 'backbone.encoder.layer4.2.bn2.running_var', 'backbone.encoder.layer4.2.conv3.weight', 'backbone.encoder.layer4.2.bn3.weight', 'backbone.encoder.layer4.2.bn3.bias', 'backbone.encoder.layer4.2.bn3.running_mean', 'backbone.encoder.layer4.2.bn3.running_var', 'transformer.enc_embedding.weight', 'transformer.encoder_layer.self_attn.in_proj_weight', 'transformer.encoder_layer.self_attn.in_proj_bias', 'transformer.encoder_layer.self_attn.out_proj.weight', 'transformer.encoder_layer.self_attn.out_proj.bias', 'transformer.encoder_layer.linear1.weight', 'transformer.encoder_layer.linear1.bias', 'transformer.encoder_layer.linear2.weight', 'transformer.encoder_layer.linear2.bias', 'transformer.encoder_layer.norm1.weight', 'transformer.encoder_layer.norm1.bias', 'transformer.encoder_layer.norm2.weight', 'transformer.encoder_layer.norm2.bias', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.dec_embedding.weight', 'transformer.decoder_layer.self_attn.in_proj_weight', 'transformer.decoder_layer.self_attn.in_proj_bias', 'transformer.decoder_layer.self_attn.out_proj.weight', 'transformer.decoder_layer.self_attn.out_proj.bias', 'transformer.decoder_layer.multihead_attn.in_proj_weight', 'transformer.decoder_layer.multihead_attn.in_proj_bias', 'transformer.decoder_layer.multihead_attn.out_proj.weight', 'transformer.decoder_layer.multihead_attn.out_proj.bias', 'transformer.decoder_layer.linear1.weight', 'transformer.decoder_layer.linear1.bias', 'transformer.decoder_layer.linear2.weight', 'transformer.decoder_layer.linear2.bias', 'transformer.decoder_layer.norm1.weight', 'transformer.decoder_layer.norm1.bias', 'transformer.decoder_layer.norm2.weight', 'transformer.decoder_layer.norm2.bias', 'transformer.decoder_layer.norm3.weight', 'transformer.decoder_layer.norm3.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.out_proj.weight', 'transformer.decoder.layers.1.self_attn.out_proj.bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'transformer.decoder.layers.1.linear1.weight', 'transformer.decoder.layers.1.linear1.bias', 'transformer.decoder.layers.1.linear2.weight', 'transformer.decoder.layers.1.linear2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.layers.2.self_attn.in_proj_weight', 'transformer.decoder.layers.2.self_attn.in_proj_bias', 'transformer.decoder.layers.2.self_attn.out_proj.weight', 'transformer.decoder.layers.2.self_attn.out_proj.bias', 'transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'transformer.decoder.layers.2.multihead_attn.out_proj.weight', 'transformer.decoder.layers.2.multihead_attn.out_proj.bias', 'transformer.decoder.layers.2.linear1.weight', 'transformer.decoder.layers.2.linear1.bias', 'transformer.decoder.layers.2.linear2.weight', 'transformer.decoder.layers.2.linear2.bias', 'transformer.decoder.layers.2.norm1.weight', 'transformer.decoder.layers.2.norm1.bias', 'transformer.decoder.layers.2.norm2.weight', 'transformer.decoder.layers.2.norm2.bias', 'transformer.decoder.layers.2.norm3.weight', 'transformer.decoder.layers.2.norm3.bias', 'transformer.decoder.layers.3.self_attn.in_proj_weight', 'transformer.decoder.layers.3.self_attn.in_proj_bias', 'transformer.decoder.layers.3.self_attn.out_proj.weight', 'transformer.decoder.layers.3.self_attn.out_proj.bias', 'transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'transformer.decoder.layers.3.multihead_attn.out_proj.weight', 'transformer.decoder.layers.3.multihead_attn.out_proj.bias', 'transformer.decoder.layers.3.linear1.weight', 'transformer.decoder.layers.3.linear1.bias', 'transformer.decoder.layers.3.linear2.weight', 'transformer.decoder.layers.3.linear2.bias', 'transformer.decoder.layers.3.norm1.weight', 'transformer.decoder.layers.3.norm1.bias', 'transformer.decoder.layers.3.norm2.weight', 'transformer.decoder.layers.3.norm2.bias', 'transformer.decoder.layers.3.norm3.weight', 'transformer.decoder.layers.3.norm3.bias', 'transformer.decoder.layers.4.self_attn.in_proj_weight', 'transformer.decoder.layers.4.self_attn.in_proj_bias', 'transformer.decoder.layers.4.self_attn.out_proj.weight', 'transformer.decoder.layers.4.self_attn.out_proj.bias', 'transformer.decoder.layers.4.multihead_attn.in_proj_weight', 'transformer.decoder.layers.4.multihead_attn.in_proj_bias', 'transformer.decoder.layers.4.multihead_attn.out_proj.weight', 'transformer.decoder.layers.4.multihead_attn.out_proj.bias', 'transformer.decoder.layers.4.linear1.weight', 'transformer.decoder.layers.4.linear1.bias', 'transformer.decoder.layers.4.linear2.weight', 'transformer.decoder.layers.4.linear2.bias', 'transformer.decoder.layers.4.norm1.weight', 'transformer.decoder.layers.4.norm1.bias', 'transformer.decoder.layers.4.norm2.weight', 'transformer.decoder.layers.4.norm2.bias', 'transformer.decoder.layers.4.norm3.weight', 'transformer.decoder.layers.4.norm3.bias', 'transformer.decoder.layers.5.self_attn.in_proj_weight', 'transformer.decoder.layers.5.self_attn.in_proj_bias', 'transformer.decoder.layers.5.self_attn.out_proj.weight', 'transformer.decoder.layers.5.self_attn.out_proj.bias', 'transformer.decoder.layers.5.multihead_attn.in_proj_weight', 'transformer.decoder.layers.5.multihead_attn.in_proj_bias', 'transformer.decoder.layers.5.multihead_attn.out_proj.weight', 'transformer.decoder.layers.5.multihead_attn.out_proj.bias', 'transformer.decoder.layers.5.linear1.weight', 'transformer.decoder.layers.5.linear1.bias', 'transformer.decoder.layers.5.linear2.weight', 'transformer.decoder.layers.5.linear2.bias', 'transformer.decoder.layers.5.norm1.weight', 'transformer.decoder.layers.5.norm1.bias', 'transformer.decoder.layers.5.norm2.weight', 'transformer.decoder.layers.5.norm2.bias', 'transformer.decoder.layers.5.norm3.weight', 'transformer.decoder.layers.5.norm3.bias', 'vqt_box_head.weight', 'vqt_box_head.bias', 'vqt_cls_head.weight', 'vqt_cls_head.bias', 'reduce_height_module.ghc_lst.0.layer.0.layers.0.weight', 'reduce_height_module.ghc_lst.0.layer.0.layers.0.bias', 'reduce_height_module.ghc_lst.0.layer.1.layers.0.weight', 'reduce_height_module.ghc_lst.0.layer.1.layers.0.bias', 'reduce_height_module.ghc_lst.0.layer.2.layers.0.weight', 'reduce_height_module.ghc_lst.0.layer.2.layers.0.bias', 'reduce_height_module.ghc_lst.0.layer.3.layers.0.weight', 'reduce_height_module.ghc_lst.0.layer.3.layers.0.bias', 'reduce_height_module.ghc_lst.1.layer.0.layers.0.weight', 'reduce_height_module.ghc_lst.1.layer.0.layers.0.bias', 'reduce_height_module.ghc_lst.1.layer.1.layers.0.weight', 'reduce_height_module.ghc_lst.1.layer.1.layers.0.bias', 'reduce_height_module.ghc_lst.1.layer.2.layers.0.weight', 'reduce_height_module.ghc_lst.1.layer.2.layers.0.bias', 'reduce_height_module.ghc_lst.1.layer.3.layers.0.weight', 'reduce_height_module.ghc_lst.1.layer.3.layers.0.bias', 'reduce_height_module.ghc_lst.2.layer.0.layers.0.weight', 'reduce_height_module.ghc_lst.2.layer.0.layers.0.bias', 'reduce_height_module.ghc_lst.2.layer.1.layers.0.weight', 'reduce_height_module.ghc_lst.2.layer.1.layers.0.bias', 'reduce_height_module.ghc_lst.2.layer.2.layers.0.weight', 'reduce_height_module.ghc_lst.2.layer.2.layers.0.bias', 'reduce_height_module.ghc_lst.2.layer.3.layers.0.weight', 'reduce_height_module.ghc_lst.2.layer.3.layers.0.bias', 'reduce_height_module.ghc_lst.3.layer.0.layers.0.weight', 'reduce_height_module.ghc_lst.3.layer.0.layers.0.bias', 'reduce_height_module.ghc_lst.3.layer.1.layers.0.weight', 'reduce_height_module.ghc_lst.3.layer.1.layers.0.bias', 'reduce_height_module.ghc_lst.3.layer.2.layers.0.weight', 'reduce_height_module.ghc_lst.3.layer.2.layers.0.bias', 'reduce_height_module.ghc_lst.3.layer.3.layers.0.weight', 'reduce_height_module.ghc_lst.3.layer.3.layers.0.bias', 'v_reproj.weight', 'v_reproj.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "horizon_path =r\"D:/OneDrive/OneDrive - NTHU/Layout/Horizon/0912_all_bk.pth\"\n",
    "#models_dict = torch.load_s\n",
    "checkpoint = torch.load(horizon_path ,  map_location=\"cpu\")\n",
    "print(checkpoint['state_dict'].keys())\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in m.state_dict()}\n",
    "m.load_state_dict(pretrained_dict , strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.arange(5)\n",
    "b=torch.arange(5)\n",
    "c=torch.arange(5)\n",
    "\n",
    "d = torch.vstack([a,b,c]).permute(1,0)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "a = torch.tensor([ [0,1,2]  ,  [0,3,5] , [1,0,5] ]).to(torch.float32)\n",
    "b = torch.tensor([ [0,1,2] , [1,0,5] ]).to(torch.float32)\n",
    "\n",
    "cost = torch.cdist(b,a)\n",
    "print(cost)\n",
    "row , col = linear_sum_assignment(cost,)\n",
    "print(row)\n",
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.7605, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7730, 0.5752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7057, 0.5861, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8386, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8304, 0.7823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7034, 0.5994, 0.5691, 0.5652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.6996, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8305, 0.7819, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8238, 0.7839, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "# Get the indices of non-zero elements\n",
    "non_zero_indices = torch.nonzero(x)\n",
    "print(non_zero_indices)\n",
    "# Get the non-zero values\n",
    "non_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "unique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "print(\"unique\" , unique)\n",
    "# Print the result\n",
    "print(non_zero_values)\n",
    "non_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "print(\"split non_zero_values\" , non_zero_values)\n",
    "\n",
    "def unpad_data( x :[Tensor] ) :\n",
    "\tnon_zero_indices = torch.nonzero(x)\n",
    "\tprint(non_zero_indices)\n",
    "\t# Get the non-zero values\n",
    "\tnon_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "\tunique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "\tprint(\"unique\" , unique)\n",
    "\t# Print the result\n",
    "\tprint(non_zero_values)\n",
    "\tnon_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "\tprint(\"split non_zero_values\" , non_zero_values)\n",
    "\treturn non_zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.58 , 0.6] , [0.4] ] , )\n",
    "b = torch.tensor([0.1 , 0.2] , )\n",
    "\n",
    "c = a.repeat(2)\n",
    "print(a.repeat(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.rand(1)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Layout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
