{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\layout\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Layout\\NTHU_CGV_Layout_exp\\ours\n",
      "d:\\Projects\\Layout\\NTHU_CGV_Layout_exp\\Horizon_and_SAM\\Horizon\n"
     ]
    }
   ],
   "source": [
    "# Data Loader\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import json\n",
    "from CustomDataset import * \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from config import *\n",
    "from file_helper import *\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "#=================================\n",
    "#             Augmentation\n",
    "#=================================\n",
    "\n",
    "def gauss_noise_tensor(img):\n",
    "    rand = torch.rand(1)[0]\n",
    "    if rand < 0.5 and Horizon_AUG:\n",
    "        sigma = rand *0.125\n",
    "        out = img + sigma * torch.randn_like(img)\n",
    "        return out\n",
    "    return img\n",
    "\n",
    "def blank(img):    \n",
    "    return img\n",
    "\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self , train_dir , test_dir , batch_size = 2, num_workers = 0 , img_size=[IMG_WIDTH, IMG_HEIGHT] , use_aug = True ,padding_count = 24 ,c =0.1 ):\n",
    "        super().__init__()\n",
    "        self.train_dir = train_dir\n",
    "        self.test_dir = test_dir\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_size = img_size      \n",
    "        self.use_aug = use_aug\n",
    "        self.padding_count  = padding_count\n",
    "        self.c = c\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download dataset\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # Create dataset...          \n",
    "                \n",
    "        self.entire_dataset = CustomDataset(self.train_dir  , use_aug= self.use_aug , padding_count= self.padding_count , c=self.c)\n",
    "        self.train_ds , self.val_ds = random_split(self.entire_dataset , [0.9, 0.1])        \n",
    "        self.test_ds = CustomDataset(self.test_dir  , use_aug= False)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # ToDo: Reture Dataloader...\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=False)\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "dm = CustomDataModule ( train_dir= f\"../anno/test_visiable_10_no_cross.json\" ,\n",
    "                       test_dir= f\"../anno/test_visiable_10_no_cross.json\" , padding_count=256\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "def unpad_data( x :[Tensor] ) :\n",
    "\tnon_zero_indices = torch.nonzero(x)\n",
    "\t#print(non_zero_indices)\n",
    "\t# Get the non-zero values\n",
    "\tnon_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "\tunique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "\t#print(\"unique\" , unique)\n",
    "\t# Print the result\n",
    "\t#print(non_zero_values)\n",
    "\tnon_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "\t#print(\"split non_zero_values\" , non_zero_values)\n",
    "\treturn non_zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position torch.Size([1024, 1])\n",
      "div_term torch.Size([256])\n",
      "pe torch.Size([1024, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type                    | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | backbone             | Resnet                  | 23.5 M\n",
      "1 | fixed_pe             | PositionalEncoding      | 0     \n",
      "2 | transformer          | TransformerEncoderLayer | 1.3 M \n",
      "3 | vqt_box_head         | Linear                  | 1.3 K \n",
      "4 | vqt_cls_head         | Linear                  | 257   \n",
      "5 | v_reproj             | Conv2d                  | 262 K \n",
      "6 | reduce_height_module | GlobalHeightStage       | 45.5 M\n",
      "-----------------------------------------------------------------\n",
      "70.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "70.5 M    Total params\n",
      "141.076   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s, loss=3.55, v_num=84]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  20%|██        | 1/5 [00:02<00:08,  2.09s/it, loss=3.41, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  40%|████      | 2/5 [00:04<00:06,  2.00s/it, loss=3.38, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  60%|██████    | 3/5 [00:05<00:03,  1.98s/it, loss=3.19, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  80%|████████  | 4/5 [00:07<00:01,  1.97s/it, loss=3.15, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s, loss=2.52, v_num=84]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  20%|██        | 1/5 [00:02<00:08,  2.22s/it, loss=2.48, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  40%|████      | 2/5 [00:04<00:06,  2.18s/it, loss=2.46, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  60%|██████    | 3/5 [00:06<00:04,  2.17s/it, loss=2.42, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  80%|████████  | 4/5 [00:08<00:02,  2.14s/it, loss=2.45, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:   0%|          | 0/5 [00:00<?, ?it/s, loss=2.13, v_num=84]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  20%|██        | 1/5 [00:02<00:08,  2.10s/it, loss=2.19, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  40%|████      | 2/5 [00:04<00:06,  2.06s/it, loss=2.09, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  60%|██████    | 3/5 [00:06<00:04,  2.05s/it, loss=2.09, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  80%|████████  | 4/5 [00:08<00:02,  2.05s/it, loss=2.12, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 5/5 [00:02<00:00,  1.89it/s, loss=2.15, v_num=84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Any\n",
    "import pytorch_lightning as pl\n",
    "from config import *\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import MLP\n",
    "import math\n",
    "from torch import Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from VerticalCompressionNet import * \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: 256, dropout: float = 0.0, max_len: int = 1024):\n",
    "        super().__init__()        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)        \n",
    "        print(\"position\", position.shape)\n",
    "        div_term = torch.exp(torch.arange(0, d_model) * (-math.log(10000.0) / d_model))\n",
    "        #div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        print(\"div_term\", div_term.shape)\n",
    "        #pe = torch.zeros(max_len, 1, d_model)  # []\n",
    "        pe = torch.zeros(max_len, d_model)  # [ 1024 , 256 ]\n",
    "        print(\"pe\", pe.shape)\n",
    "        #pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        #pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        pe[: ,:] = torch.sin(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:    \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            #x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "            x: Tensor, shape ``[batch_size , seq_len , embedding_dim]``\n",
    "        \"\"\"\n",
    "        #x = x + self.pe[:x.size(0)]\n",
    "        # [batch size , 1024 , 256 ]\n",
    "        x = x + self.pe\n",
    "        return self.dropout(x) , self.pe\n",
    "        #return self.dropout(x)\n",
    "        #return self.pe[:0]\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int , d_hid: int, nlayers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.enc_embedding = nn.Embedding(num_embeddings=ntoken ,embedding_dim= d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model , nhead , d_hid , dropout) \n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        '''\n",
    "        self.dec_embedding = nn.Embedding(num_embeddings=ntoken ,embedding_dim= d_model)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead , dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder( self.decoder_layer  , num_layers=6)\n",
    "        ''' \n",
    "\n",
    "        '''\n",
    "        #self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "        self.init_weights()\n",
    "        '''\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        #self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        #src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        #print(\"trans forward \" , src.shape)\n",
    "        #src = self.pos_encoder(src)\n",
    "        #output = self.transformer_encoder(src, src_mask)\n",
    "        #output = self.linear(output)\n",
    "        #return output\n",
    "        #print(\"self.ntoken\" , self.ntoken)\n",
    "        #print(\"self.src\" , src.shape)\n",
    "        ''''''\n",
    "        pos_idx = torch.arange(self.ntoken , device=src.device)\n",
    "\n",
    "        pos_src = self.enc_embedding(pos_idx)* math.sqrt(self.d_model) + src\n",
    "        pos_enc_src = self.encoder(pos_src )\n",
    "\n",
    "        pos_dec_src = self.dec_embedding(pos_idx)* math.sqrt(self.d_model) + pos_enc_src\n",
    "        #pos_dec_src = self.dec_embedding( torch.tensor([0,1],device=src.device) )* math.sqrt(self.d_model) + pos_enc_src\n",
    "        dec_src = self.decoder( pos_enc_src  ,pos_dec_src)\n",
    "\n",
    "        #print(\"dec_src\" , dec_src.shape)    # [batch , token , hidden]\n",
    "        return dec_src\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class VerticalQueryTransformer(pl.LightningModule):    \n",
    "    def __init__(self  ,  max_predict_count = 24 , hidden_out = 128 , class_num = 1 , log_folder = \"__test\" , num_classes = 1 , backbone_trainable =False, load_weight =\"\" , top_k = 20):\n",
    "        #print(\" input_size\" ,  input_size)\n",
    "        super().__init__()\n",
    "        self.backbone = Resnet()\n",
    "        self.out_scale = 8\n",
    "        self.step_cols = 4        \n",
    "        self.hidden_size = hidden_out\n",
    "        self.max_predict_count = max_predict_count\n",
    "        self.num_classes  = num_classes \n",
    "        self.top_k_num = top_k        \n",
    "\n",
    "        self.fixed_pe = PositionalEncoding(hidden_out, 0.1 , 1024)\n",
    "\n",
    "        #self.transformer = TransformerModel( ntoken= max_predict_count , d_model=hidden_out , nhead=8 , d_hid= 2048,nlayers=6 )\n",
    "        self.transformer = nn.TransformerEncoderLayer(hidden_out ,  8 , 2048 , dropout= 0.1) \n",
    "\n",
    "        #self.box_head= nn.Linear( hidden_out , 6 )        \n",
    "        self.vqt_box_head= nn.Linear( hidden_out , 5 )        \n",
    "        self.vqt_cls_head= nn.Linear( hidden_out , class_num )        \n",
    "        self.confidence_threshold = 0.5\n",
    "\n",
    "        # loss\n",
    "        self.box_cost = 1\n",
    "        self.cls_cost = 20\n",
    "\n",
    "        self.log_folder = create_folder(os.path.join(os.getcwd() , \"output\" , log_folder))\n",
    "        #self.box_head.bias.data = torch.nn.Parameter(torch.tensor([0.3,0.2,0.2,0.3]))\n",
    "        #self.box_head.weight.data.fill_(0)\n",
    "        \n",
    "        # Inference channels number from each block of the encoder\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 320, 190)\n",
    "            c1, c2, c3, c4 = [b.shape[1] for b in self.backbone(dummy)] # resnet feature channel數\n",
    "            #print(\"c1, c2, c3, c4\" , c1, c2, c3, c4)\n",
    "            c_last = (c1*8 + c2*4 + c3*2 + c4*1) // self.out_scale            \n",
    "        self.v_reproj = nn.Conv2d(1024 , self.max_predict_count,kernel_size=1)\n",
    "        self.reduce_height_module = GlobalHeightStage(c1, c2, c3, c4 , out_scale=self.out_scale , pretrain_weight= load_weight , freeze_model= not backbone_trainable)\n",
    "\n",
    "\n",
    "    def forward(self ,x ):\n",
    "        features = self.backbone(x) # [4 , c , h, w]      \n",
    "        # vertical feature\n",
    "        reduced_feats  = self.reduce_height_module(features , x.shape[3]//self.step_cols ) # [b , 1024 ,  256] width = 1024 , 256d latent code each.\n",
    "        '''\n",
    "        plt.imshow(reduced_feats[0].detach().cpu().numpy())\n",
    "        plt.title(\"vertical feature\")\n",
    "        plt.show()        \n",
    "        print(\"reduced_feats\" , reduced_feats.shape)  # [b , 1024 , 256]\n",
    "        '''\n",
    "        \n",
    "        # Add fixed PE\n",
    "        embedded_feat , pe_pattern = self.fixed_pe(reduced_feats)\n",
    "        '''\n",
    "        print(\"pe_pattern \" , pe_pattern.shape)\n",
    "        plt.imshow(pe_pattern.detach().cpu().numpy())\n",
    "        plt.title(\"PE Pattern\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(embedded_feat[0].detach().cpu().numpy())\n",
    "        plt.title(\"PE embedded_feat\")\n",
    "        plt.show()\n",
    "        '''\n",
    "\n",
    "        #reduced_feats = self.v_reproj(reduced_feats.view(reduced_feats.shape[0] , 1024 , 256,-1)).view(reduced_feats.shape[0]  , 256 , 256)\n",
    "        #print(\"reduced_feats\" , reduced_feats.shape)\n",
    "        output = self.transformer(embedded_feat)  # (b , 1024 , 256 )\n",
    "        #print(\"encoder out \" , output.shape)\n",
    "        \n",
    "        batch_size = output.shape[0]\n",
    "        #output = output.view( batch_size , self.max_predict_count , -1 ) # (b , 90 , 64*out_scale )        \n",
    "\n",
    "        out_box = self.vqt_box_head(output)\n",
    "        out_cls = self.vqt_cls_head(output)#.view(batch_size , self.max_predict_count , -1 )\n",
    "\n",
    "        '''\n",
    "        '''\n",
    "        clampped_box = torch.zeros((batch_size , self.max_predict_count , 5)).to(output.device)\n",
    "        out_cls = F.interpolate(out_cls.view(batch_size,-1).unsqueeze(0) , self.max_predict_count)[0].view(batch_size , self.max_predict_count,1)\n",
    "        \n",
    "        clampped_box[:,:,0] =  F.interpolate( out_box[:,:,0].unsqueeze(0), self.max_predict_count)\n",
    "        clampped_box[:,:,1] =  F.interpolate( out_box[:,:,1].unsqueeze(0), self.max_predict_count)\n",
    "        clampped_box[:,:,2] =  F.interpolate( out_box[:,:,2].unsqueeze(0), self.max_predict_count)\n",
    "        clampped_box[:,:,3] =  F.interpolate( out_box[:,:,3].unsqueeze(0), self.max_predict_count)\n",
    "        clampped_box[:,:,4] =  F.interpolate( out_box[:,:,4].unsqueeze(0), self.max_predict_count)\n",
    "        \n",
    "        '''\n",
    "        plt.imshow(out_cls[0].repeat(10,1).detach().cpu().numpy())\n",
    "        plt.title(\"encoder cls output\")\n",
    "        plt.show()\n",
    "        top_k = torch.topk(out_cls ,self.top_k_num , dim= 1 )\n",
    "        top_k_idx = top_k[1].view(batch_size  , self.top_k_num)  # [b , top_k ]        \n",
    "        #print(\"top_k_idx\" , top_k_idx.shape)\n",
    "\n",
    "        topk_box = out_box.gather(1 , top_k_idx.unsqueeze(-1).repeat(1,1,5))\n",
    "        topk_cls = out_cls.gather(1 , top_k_idx.unsqueeze(-1).repeat(1,1,1))\n",
    "        \n",
    "        return topk_box , topk_cls , top_k_idx\n",
    "        '''\n",
    "        return clampped_box , out_cls\n",
    "    \n",
    "    #@torch.no_grad()\n",
    "    def find_match(self, gt , pred):\n",
    "        #print(\"gt\"  , gt)\n",
    "        gt_vec = torch.stack(gt).permute(1,0)\n",
    "        pred_vec = torch.stack(pred).permute(1,0)\n",
    "        loss_dist = torch.cdist(gt_vec , pred_vec)\n",
    "        \n",
    "        each_gt_pred_best_idx = torch.argmax(loss_dist, 0 )      \n",
    "        #print(\"each_gt_pred_best_idx\" , each_gt_pred_best_idx)          \n",
    "\n",
    "        return gt_vec[each_gt_pred_best_idx] , pred_vec\n",
    "        \n",
    "    def pack_visualize(self, gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , dv_btm_b ):\n",
    "        \n",
    "        sizes = [len(t) for t in gt_u_b]               \n",
    "        if isinstance(gt_u_b, torch.Tensor):\n",
    "            us = gt_u_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            us[1::2]+=gt_du_b.flatten()\n",
    "            us = torch.split(us.view(-1,2) , sizes)\n",
    "\n",
    "            tops = gt_vtop_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            tops[1::2]=gt_dvtop_b.flatten()\n",
    "            tops = torch.split(tops.view(-1,2) , sizes)\n",
    "\n",
    "            btms = gt_vbtm_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            btms[1::2]=dv_btm_b.flatten()\n",
    "            btms = torch.split(btms.view(-1,2) , sizes)\n",
    "\n",
    "        elif isinstance(gt_u_b, tuple) and all(isinstance(t, torch.Tensor) for t in gt_u_b):        \n",
    "            us = torch.cat(gt_u_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            us[1::2]+=torch.cat(gt_du_b).view(-1)\n",
    "            us = torch.split(us.view(-1,2) , sizes)\n",
    "\n",
    "            tops = torch.cat(gt_vtop_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            tops[1::2]=torch.cat(gt_dvtop_b).view(-1)\n",
    "            tops = torch.split(tops.view(-1,2) , sizes)\n",
    "\n",
    "            btms = torch.cat(gt_vbtm_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            btms[1::2]=torch.cat(dv_btm_b).view(-1)\n",
    "            btms = torch.split(btms.view(-1,2) , sizes)\n",
    "        else:\n",
    "            assert(\"Wrong Type.\")\n",
    "        \n",
    "        return us , tops ,btms\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def training_step(self , input_b ,batch_idx , optimizer_idx):\n",
    "        \n",
    "        img = input_b['image']\n",
    "        #h,w = img.shape[1:3]\n",
    "        out_box , out_cls  = self.forward(img)  # [ batch , top_k , 5]   , [ batch , top_k , 1] \n",
    "        \n",
    "        batch_size = out_box.shape[0]\n",
    "               \n",
    "        gt_u_b = unpad_data( input_b['u'])          \n",
    "        gt_vtop_b =unpad_data(input_b['v_top'])\n",
    "        gt_vbtm_b = unpad_data (input_b['v_btm'])\n",
    "        gt_du_b = unpad_data(input_b['du'])\n",
    "        gt_dvtop_b = unpad_data(input_b['dv_top'])\n",
    "        gt_dv_btm_b = unpad_data(input_b['dv_btm'])\n",
    "\n",
    "        #selected_gt_u_grad =  input_b['u_grad'].view(batch_size , 1024 , 1).gather(1 , top_k_idx.unsqueeze(-1).repeat(1,1,1)).view(batch_size , self.top_k_num)        \n",
    "\n",
    "        total_loss = 0\n",
    "        b_cnt = 0\n",
    "        for u,vtop,vbtm,du,dvtop, dvbtm , pred ,cls_b,gt_cls in zip(gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , gt_dv_btm_b , out_box , out_cls , input_b['u_grad']):\n",
    "        #for u,vtop,vbtm,du,dvtop, dvbtm , pred ,cls_b,gt_cls , k_idx in zip(gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , gt_dv_btm_b , out_box , out_cls ,selected_gt_u_grad , top_k_idx):\n",
    "            \n",
    "            # match            \n",
    "            gt_box =  torch.vstack([vtop,vbtm,du ,dvtop , dvbtm]).permute(1,0)\n",
    "            \n",
    "            box_loss = torch.cdist( pred , gt_box , p=1)\n",
    "            cls_loss = - F.softmax( cls_b , -1)\n",
    "\n",
    "            #cost_matrix = box_loss * self.box_cost + cls_loss * self.cls_cost\n",
    "            '''\n",
    "            cost_matrix =  torch.cdist( F.sigmoid(cls_b).view(-1,1) , gt_u_b.view(-1,1))            \n",
    "            cost_matrix = cost_matrix.detach().cpu().numpy()            \n",
    "            row_idx  , col_idx = linear_sum_assignment(cost_matrix)                      \n",
    "            '''\n",
    "            \n",
    "            row_idx = torch.round(u* self.max_predict_count ).to(torch.long).detach().cpu().numpy()           \n",
    "            col_idx = torch.argsort(u).detach().cpu().numpy() #torch.arange(u.shape[0]).detach().cpu().numpy()           \n",
    "\n",
    "            matched_cls_loss = F.binary_cross_entropy_with_logits(cls_b[row_idx].view(-1) , gt_cls[col_idx])\n",
    "            \n",
    "            total_loss += F.l1_loss(pred[row_idx] ,  gt_box[col_idx])*5 + F.binary_cross_entropy_with_logits(cls_b.view(-1), gt_cls  ) + matched_cls_loss\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                #if self.current_epoch % 5 == 0  :                \n",
    "                if self.current_epoch % 5 == 0 and self.current_epoch > 0 :                \n",
    "                    save_path =  os.path.join(self.log_folder , f\"gt_ep_{self.current_epoch}-{self.global_step}-{b_cnt}\" )\n",
    "                    gt_us , gt_tops , gt_btms = self.pack_visualize(u.view(1 , -1 ) , vtop , vbtm , du , dvtop , dvbtm )\n",
    "                    #print(\"gt_us , gt_tops , gt_btms\" , gt_us , gt_tops , gt_btms)\n",
    "                    vis_imgs = visualize_2d_single(gt_us , gt_tops , gt_btms , u_grad =  gt_cls.view(1 , -1 ), imgs= img[b_cnt] , title=\"GT\",save_path=save_path )                \n",
    "                    \n",
    "                    save_path =  os.path.join(self.log_folder , f\"pred_ep_{self.current_epoch}-{self.global_step}-{b_cnt}\" )\n",
    "                    pred_u = row_idx / self.max_predict_count                    \n",
    "                    #pred_u = k_idx[row_idx] / 1024\n",
    "                    #pred_u = pred_u.detach().cpu().numpy()\n",
    "                    #print(\"pred_u\" , pred_u)\n",
    "                    pred_u = torch.from_numpy(pred_u.flatten()[np.newaxis,...]).to(u.device)\n",
    "                    pred_us , pred_tops , pred_btms = self.pack_visualize(pred_u, pred[row_idx,0],pred[row_idx,1],pred[row_idx,2],pred[row_idx,3],pred[row_idx,4] )\n",
    "                    #print(\"pred_us , pred_tops , pred_btms\" , pred_us , pred_tops , pred_btms)\n",
    "                    vis_imgs = visualize_2d_single(pred_us , pred_tops , pred_btms , u_grad = F.sigmoid(cls_b).view(1 , -1 ) , imgs=  img[b_cnt] , title=\"Pred\" , save_path= save_path  )\n",
    "                    \n",
    "           \n",
    "            b_cnt+=1\n",
    "            pass        \n",
    "        return total_loss\n",
    "        pass    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        backbone_opt = optim.Adam(self.backbone.parameters() , lr=0.00035)\n",
    "        transforms_opt = optim.Adam(self.transformer.parameters() , lr=0.000035)\n",
    "\n",
    "        return [backbone_opt , transforms_opt] , []\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "# Unit testing...\n",
    "\n",
    "# Test\n",
    "dm = CustomDataModule ( train_dir= f\"../anno/test_visiable_10_no_cross.json\" ,\n",
    "                        test_dir= f\"../anno/test_visiable_10_no_cross.json\" , padding_count=256 , use_aug=False , c= 0.1\n",
    "                       )\n",
    "m = VerticalQueryTransformer(max_predict_count = 256 , hidden_out=256 , load_weight=\"D:/OneDrive/OneDrive - NTHU/Layout/Horizon/0912_all_bk.pth\"  , backbone_trainable=True, top_k=90)\n",
    "#img = torch.randn((3,3,1024,512))\n",
    "#o = m(img)\n",
    "\n",
    "#print(o)\n",
    "trainer = pl.Trainer(accelerator='gpu' , devices=1 ,min_epochs=1, max_epochs=51 , precision=16 , fast_dev_run=False )\n",
    "trainer.fit(m , dm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['v_head.weight', 'v_head.bias', 'du_head.weight', 'du_head.bias', 'cls_head.weight', 'cls_head.bias', 'feature_extractor.encoder.conv1.1.weight', 'feature_extractor.encoder.bn1.weight', 'feature_extractor.encoder.bn1.bias', 'feature_extractor.encoder.bn1.running_mean', 'feature_extractor.encoder.bn1.running_var', 'feature_extractor.encoder.bn1.num_batches_tracked', 'feature_extractor.encoder.layer1.0.conv1.weight', 'feature_extractor.encoder.layer1.0.bn1.weight', 'feature_extractor.encoder.layer1.0.bn1.bias', 'feature_extractor.encoder.layer1.0.bn1.running_mean', 'feature_extractor.encoder.layer1.0.bn1.running_var', 'feature_extractor.encoder.layer1.0.bn1.num_batches_tracked', 'feature_extractor.encoder.layer1.0.conv2.1.weight', 'feature_extractor.encoder.layer1.0.bn2.weight', 'feature_extractor.encoder.layer1.0.bn2.bias', 'feature_extractor.encoder.layer1.0.bn2.running_mean', 'feature_extractor.encoder.layer1.0.bn2.running_var', 'feature_extractor.encoder.layer1.0.bn2.num_batches_tracked', 'feature_extractor.encoder.layer1.0.conv3.weight', 'feature_extractor.encoder.layer1.0.bn3.weight', 'feature_extractor.encoder.layer1.0.bn3.bias', 'feature_extractor.encoder.layer1.0.bn3.running_mean', 'feature_extractor.encoder.layer1.0.bn3.running_var', 'feature_extractor.encoder.layer1.0.bn3.num_batches_tracked', 'feature_extractor.encoder.layer1.0.downsample.0.weight', 'feature_extractor.encoder.layer1.0.downsample.1.weight', 'feature_extractor.encoder.layer1.0.downsample.1.bias', 'feature_extractor.encoder.layer1.0.downsample.1.running_mean', 'feature_extractor.encoder.layer1.0.downsample.1.running_var', 'feature_extractor.encoder.layer1.0.downsample.1.num_batches_tracked', 'feature_extractor.encoder.layer1.1.conv1.weight', 'feature_extractor.encoder.layer1.1.bn1.weight', 'feature_extractor.encoder.layer1.1.bn1.bias', 'feature_extractor.encoder.layer1.1.bn1.running_mean', 'feature_extractor.encoder.layer1.1.bn1.running_var', 'feature_extractor.encoder.layer1.1.bn1.num_batches_tracked', 'feature_extractor.encoder.layer1.1.conv2.1.weight', 'feature_extractor.encoder.layer1.1.bn2.weight', 'feature_extractor.encoder.layer1.1.bn2.bias', 'feature_extractor.encoder.layer1.1.bn2.running_mean', 'feature_extractor.encoder.layer1.1.bn2.running_var', 'feature_extractor.encoder.layer1.1.bn2.num_batches_tracked', 'feature_extractor.encoder.layer1.1.conv3.weight', 'feature_extractor.encoder.layer1.1.bn3.weight', 'feature_extractor.encoder.layer1.1.bn3.bias', 'feature_extractor.encoder.layer1.1.bn3.running_mean', 'feature_extractor.encoder.layer1.1.bn3.running_var', 'feature_extractor.encoder.layer1.1.bn3.num_batches_tracked', 'feature_extractor.encoder.layer1.2.conv1.weight', 'feature_extractor.encoder.layer1.2.bn1.weight', 'feature_extractor.encoder.layer1.2.bn1.bias', 'feature_extractor.encoder.layer1.2.bn1.running_mean', 'feature_extractor.encoder.layer1.2.bn1.running_var', 'feature_extractor.encoder.layer1.2.bn1.num_batches_tracked', 'feature_extractor.encoder.layer1.2.conv2.1.weight', 'feature_extractor.encoder.layer1.2.bn2.weight', 'feature_extractor.encoder.layer1.2.bn2.bias', 'feature_extractor.encoder.layer1.2.bn2.running_mean', 'feature_extractor.encoder.layer1.2.bn2.running_var', 'feature_extractor.encoder.layer1.2.bn2.num_batches_tracked', 'feature_extractor.encoder.layer1.2.conv3.weight', 'feature_extractor.encoder.layer1.2.bn3.weight', 'feature_extractor.encoder.layer1.2.bn3.bias', 'feature_extractor.encoder.layer1.2.bn3.running_mean', 'feature_extractor.encoder.layer1.2.bn3.running_var', 'feature_extractor.encoder.layer1.2.bn3.num_batches_tracked', 'feature_extractor.encoder.layer2.0.conv1.weight', 'feature_extractor.encoder.layer2.0.bn1.weight', 'feature_extractor.encoder.layer2.0.bn1.bias', 'feature_extractor.encoder.layer2.0.bn1.running_mean', 'feature_extractor.encoder.layer2.0.bn1.running_var', 'feature_extractor.encoder.layer2.0.bn1.num_batches_tracked', 'feature_extractor.encoder.layer2.0.conv2.1.weight', 'feature_extractor.encoder.layer2.0.bn2.weight', 'feature_extractor.encoder.layer2.0.bn2.bias', 'feature_extractor.encoder.layer2.0.bn2.running_mean', 'feature_extractor.encoder.layer2.0.bn2.running_var', 'feature_extractor.encoder.layer2.0.bn2.num_batches_tracked', 'feature_extractor.encoder.layer2.0.conv3.weight', 'feature_extractor.encoder.layer2.0.bn3.weight', 'feature_extractor.encoder.layer2.0.bn3.bias', 'feature_extractor.encoder.layer2.0.bn3.running_mean', 'feature_extractor.encoder.layer2.0.bn3.running_var', 'feature_extractor.encoder.layer2.0.bn3.num_batches_tracked', 'feature_extractor.encoder.layer2.0.downsample.0.weight', 'feature_extractor.encoder.layer2.0.downsample.1.weight', 'feature_extractor.encoder.layer2.0.downsample.1.bias', 'feature_extractor.encoder.layer2.0.downsample.1.running_mean', 'feature_extractor.encoder.layer2.0.downsample.1.running_var', 'feature_extractor.encoder.layer2.0.downsample.1.num_batches_tracked', 'feature_extractor.encoder.layer2.1.conv1.weight', 'feature_extractor.encoder.layer2.1.bn1.weight', 'feature_extractor.encoder.layer2.1.bn1.bias', 'feature_extractor.encoder.layer2.1.bn1.running_mean', 'feature_extractor.encoder.layer2.1.bn1.running_var', 'feature_extractor.encoder.layer2.1.bn1.num_batches_tracked', 'feature_extractor.encoder.layer2.1.conv2.1.weight', 'feature_extractor.encoder.layer2.1.bn2.weight', 'feature_extractor.encoder.layer2.1.bn2.bias', 'feature_extractor.encoder.layer2.1.bn2.running_mean', 'feature_extractor.encoder.layer2.1.bn2.running_var', 'feature_extractor.encoder.layer2.1.bn2.num_batches_tracked', 'feature_extractor.encoder.layer2.1.conv3.weight', 'feature_extractor.encoder.layer2.1.bn3.weight', 'feature_extractor.encoder.layer2.1.bn3.bias', 'feature_extractor.encoder.layer2.1.bn3.running_mean', 'feature_extractor.encoder.layer2.1.bn3.running_var', 'feature_extractor.encoder.layer2.1.bn3.num_batches_tracked', 'feature_extractor.encoder.layer2.2.conv1.weight', 'feature_extractor.encoder.layer2.2.bn1.weight', 'feature_extractor.encoder.layer2.2.bn1.bias', 'feature_extractor.encoder.layer2.2.bn1.running_mean', 'feature_extractor.encoder.layer2.2.bn1.running_var', 'feature_extractor.encoder.layer2.2.bn1.num_batches_tracked', 'feature_extractor.encoder.layer2.2.conv2.1.weight', 'feature_extractor.encoder.layer2.2.bn2.weight', 'feature_extractor.encoder.layer2.2.bn2.bias', 'feature_extractor.encoder.layer2.2.bn2.running_mean', 'feature_extractor.encoder.layer2.2.bn2.running_var', 'feature_extractor.encoder.layer2.2.bn2.num_batches_tracked', 'feature_extractor.encoder.layer2.2.conv3.weight', 'feature_extractor.encoder.layer2.2.bn3.weight', 'feature_extractor.encoder.layer2.2.bn3.bias', 'feature_extractor.encoder.layer2.2.bn3.running_mean', 'feature_extractor.encoder.layer2.2.bn3.running_var', 'feature_extractor.encoder.layer2.2.bn3.num_batches_tracked', 'feature_extractor.encoder.layer2.3.conv1.weight', 'feature_extractor.encoder.layer2.3.bn1.weight', 'feature_extractor.encoder.layer2.3.bn1.bias', 'feature_extractor.encoder.layer2.3.bn1.running_mean', 'feature_extractor.encoder.layer2.3.bn1.running_var', 'feature_extractor.encoder.layer2.3.bn1.num_batches_tracked', 'feature_extractor.encoder.layer2.3.conv2.1.weight', 'feature_extractor.encoder.layer2.3.bn2.weight', 'feature_extractor.encoder.layer2.3.bn2.bias', 'feature_extractor.encoder.layer2.3.bn2.running_mean', 'feature_extractor.encoder.layer2.3.bn2.running_var', 'feature_extractor.encoder.layer2.3.bn2.num_batches_tracked', 'feature_extractor.encoder.layer2.3.conv3.weight', 'feature_extractor.encoder.layer2.3.bn3.weight', 'feature_extractor.encoder.layer2.3.bn3.bias', 'feature_extractor.encoder.layer2.3.bn3.running_mean', 'feature_extractor.encoder.layer2.3.bn3.running_var', 'feature_extractor.encoder.layer2.3.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.0.conv1.weight', 'feature_extractor.encoder.layer3.0.bn1.weight', 'feature_extractor.encoder.layer3.0.bn1.bias', 'feature_extractor.encoder.layer3.0.bn1.running_mean', 'feature_extractor.encoder.layer3.0.bn1.running_var', 'feature_extractor.encoder.layer3.0.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.0.conv2.1.weight', 'feature_extractor.encoder.layer3.0.bn2.weight', 'feature_extractor.encoder.layer3.0.bn2.bias', 'feature_extractor.encoder.layer3.0.bn2.running_mean', 'feature_extractor.encoder.layer3.0.bn2.running_var', 'feature_extractor.encoder.layer3.0.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.0.conv3.weight', 'feature_extractor.encoder.layer3.0.bn3.weight', 'feature_extractor.encoder.layer3.0.bn3.bias', 'feature_extractor.encoder.layer3.0.bn3.running_mean', 'feature_extractor.encoder.layer3.0.bn3.running_var', 'feature_extractor.encoder.layer3.0.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.0.downsample.0.weight', 'feature_extractor.encoder.layer3.0.downsample.1.weight', 'feature_extractor.encoder.layer3.0.downsample.1.bias', 'feature_extractor.encoder.layer3.0.downsample.1.running_mean', 'feature_extractor.encoder.layer3.0.downsample.1.running_var', 'feature_extractor.encoder.layer3.0.downsample.1.num_batches_tracked', 'feature_extractor.encoder.layer3.1.conv1.weight', 'feature_extractor.encoder.layer3.1.bn1.weight', 'feature_extractor.encoder.layer3.1.bn1.bias', 'feature_extractor.encoder.layer3.1.bn1.running_mean', 'feature_extractor.encoder.layer3.1.bn1.running_var', 'feature_extractor.encoder.layer3.1.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.1.conv2.1.weight', 'feature_extractor.encoder.layer3.1.bn2.weight', 'feature_extractor.encoder.layer3.1.bn2.bias', 'feature_extractor.encoder.layer3.1.bn2.running_mean', 'feature_extractor.encoder.layer3.1.bn2.running_var', 'feature_extractor.encoder.layer3.1.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.1.conv3.weight', 'feature_extractor.encoder.layer3.1.bn3.weight', 'feature_extractor.encoder.layer3.1.bn3.bias', 'feature_extractor.encoder.layer3.1.bn3.running_mean', 'feature_extractor.encoder.layer3.1.bn3.running_var', 'feature_extractor.encoder.layer3.1.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.2.conv1.weight', 'feature_extractor.encoder.layer3.2.bn1.weight', 'feature_extractor.encoder.layer3.2.bn1.bias', 'feature_extractor.encoder.layer3.2.bn1.running_mean', 'feature_extractor.encoder.layer3.2.bn1.running_var', 'feature_extractor.encoder.layer3.2.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.2.conv2.1.weight', 'feature_extractor.encoder.layer3.2.bn2.weight', 'feature_extractor.encoder.layer3.2.bn2.bias', 'feature_extractor.encoder.layer3.2.bn2.running_mean', 'feature_extractor.encoder.layer3.2.bn2.running_var', 'feature_extractor.encoder.layer3.2.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.2.conv3.weight', 'feature_extractor.encoder.layer3.2.bn3.weight', 'feature_extractor.encoder.layer3.2.bn3.bias', 'feature_extractor.encoder.layer3.2.bn3.running_mean', 'feature_extractor.encoder.layer3.2.bn3.running_var', 'feature_extractor.encoder.layer3.2.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.3.conv1.weight', 'feature_extractor.encoder.layer3.3.bn1.weight', 'feature_extractor.encoder.layer3.3.bn1.bias', 'feature_extractor.encoder.layer3.3.bn1.running_mean', 'feature_extractor.encoder.layer3.3.bn1.running_var', 'feature_extractor.encoder.layer3.3.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.3.conv2.1.weight', 'feature_extractor.encoder.layer3.3.bn2.weight', 'feature_extractor.encoder.layer3.3.bn2.bias', 'feature_extractor.encoder.layer3.3.bn2.running_mean', 'feature_extractor.encoder.layer3.3.bn2.running_var', 'feature_extractor.encoder.layer3.3.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.3.conv3.weight', 'feature_extractor.encoder.layer3.3.bn3.weight', 'feature_extractor.encoder.layer3.3.bn3.bias', 'feature_extractor.encoder.layer3.3.bn3.running_mean', 'feature_extractor.encoder.layer3.3.bn3.running_var', 'feature_extractor.encoder.layer3.3.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.4.conv1.weight', 'feature_extractor.encoder.layer3.4.bn1.weight', 'feature_extractor.encoder.layer3.4.bn1.bias', 'feature_extractor.encoder.layer3.4.bn1.running_mean', 'feature_extractor.encoder.layer3.4.bn1.running_var', 'feature_extractor.encoder.layer3.4.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.4.conv2.1.weight', 'feature_extractor.encoder.layer3.4.bn2.weight', 'feature_extractor.encoder.layer3.4.bn2.bias', 'feature_extractor.encoder.layer3.4.bn2.running_mean', 'feature_extractor.encoder.layer3.4.bn2.running_var', 'feature_extractor.encoder.layer3.4.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.4.conv3.weight', 'feature_extractor.encoder.layer3.4.bn3.weight', 'feature_extractor.encoder.layer3.4.bn3.bias', 'feature_extractor.encoder.layer3.4.bn3.running_mean', 'feature_extractor.encoder.layer3.4.bn3.running_var', 'feature_extractor.encoder.layer3.4.bn3.num_batches_tracked', 'feature_extractor.encoder.layer3.5.conv1.weight', 'feature_extractor.encoder.layer3.5.bn1.weight', 'feature_extractor.encoder.layer3.5.bn1.bias', 'feature_extractor.encoder.layer3.5.bn1.running_mean', 'feature_extractor.encoder.layer3.5.bn1.running_var', 'feature_extractor.encoder.layer3.5.bn1.num_batches_tracked', 'feature_extractor.encoder.layer3.5.conv2.1.weight', 'feature_extractor.encoder.layer3.5.bn2.weight', 'feature_extractor.encoder.layer3.5.bn2.bias', 'feature_extractor.encoder.layer3.5.bn2.running_mean', 'feature_extractor.encoder.layer3.5.bn2.running_var', 'feature_extractor.encoder.layer3.5.bn2.num_batches_tracked', 'feature_extractor.encoder.layer3.5.conv3.weight', 'feature_extractor.encoder.layer3.5.bn3.weight', 'feature_extractor.encoder.layer3.5.bn3.bias', 'feature_extractor.encoder.layer3.5.bn3.running_mean', 'feature_extractor.encoder.layer3.5.bn3.running_var', 'feature_extractor.encoder.layer3.5.bn3.num_batches_tracked', 'feature_extractor.encoder.layer4.0.conv1.weight', 'feature_extractor.encoder.layer4.0.bn1.weight', 'feature_extractor.encoder.layer4.0.bn1.bias', 'feature_extractor.encoder.layer4.0.bn1.running_mean', 'feature_extractor.encoder.layer4.0.bn1.running_var', 'feature_extractor.encoder.layer4.0.bn1.num_batches_tracked', 'feature_extractor.encoder.layer4.0.conv2.1.weight', 'feature_extractor.encoder.layer4.0.bn2.weight', 'feature_extractor.encoder.layer4.0.bn2.bias', 'feature_extractor.encoder.layer4.0.bn2.running_mean', 'feature_extractor.encoder.layer4.0.bn2.running_var', 'feature_extractor.encoder.layer4.0.bn2.num_batches_tracked', 'feature_extractor.encoder.layer4.0.conv3.weight', 'feature_extractor.encoder.layer4.0.bn3.weight', 'feature_extractor.encoder.layer4.0.bn3.bias', 'feature_extractor.encoder.layer4.0.bn3.running_mean', 'feature_extractor.encoder.layer4.0.bn3.running_var', 'feature_extractor.encoder.layer4.0.bn3.num_batches_tracked', 'feature_extractor.encoder.layer4.0.downsample.0.weight', 'feature_extractor.encoder.layer4.0.downsample.1.weight', 'feature_extractor.encoder.layer4.0.downsample.1.bias', 'feature_extractor.encoder.layer4.0.downsample.1.running_mean', 'feature_extractor.encoder.layer4.0.downsample.1.running_var', 'feature_extractor.encoder.layer4.0.downsample.1.num_batches_tracked', 'feature_extractor.encoder.layer4.1.conv1.weight', 'feature_extractor.encoder.layer4.1.bn1.weight', 'feature_extractor.encoder.layer4.1.bn1.bias', 'feature_extractor.encoder.layer4.1.bn1.running_mean', 'feature_extractor.encoder.layer4.1.bn1.running_var', 'feature_extractor.encoder.layer4.1.bn1.num_batches_tracked', 'feature_extractor.encoder.layer4.1.conv2.1.weight', 'feature_extractor.encoder.layer4.1.bn2.weight', 'feature_extractor.encoder.layer4.1.bn2.bias', 'feature_extractor.encoder.layer4.1.bn2.running_mean', 'feature_extractor.encoder.layer4.1.bn2.running_var', 'feature_extractor.encoder.layer4.1.bn2.num_batches_tracked', 'feature_extractor.encoder.layer4.1.conv3.weight', 'feature_extractor.encoder.layer4.1.bn3.weight', 'feature_extractor.encoder.layer4.1.bn3.bias', 'feature_extractor.encoder.layer4.1.bn3.running_mean', 'feature_extractor.encoder.layer4.1.bn3.running_var', 'feature_extractor.encoder.layer4.1.bn3.num_batches_tracked', 'feature_extractor.encoder.layer4.2.conv1.weight', 'feature_extractor.encoder.layer4.2.bn1.weight', 'feature_extractor.encoder.layer4.2.bn1.bias', 'feature_extractor.encoder.layer4.2.bn1.running_mean', 'feature_extractor.encoder.layer4.2.bn1.running_var', 'feature_extractor.encoder.layer4.2.bn1.num_batches_tracked', 'feature_extractor.encoder.layer4.2.conv2.1.weight', 'feature_extractor.encoder.layer4.2.bn2.weight', 'feature_extractor.encoder.layer4.2.bn2.bias', 'feature_extractor.encoder.layer4.2.bn2.running_mean', 'feature_extractor.encoder.layer4.2.bn2.running_var', 'feature_extractor.encoder.layer4.2.bn2.num_batches_tracked', 'feature_extractor.encoder.layer4.2.conv3.weight', 'feature_extractor.encoder.layer4.2.bn3.weight', 'feature_extractor.encoder.layer4.2.bn3.bias', 'feature_extractor.encoder.layer4.2.bn3.running_mean', 'feature_extractor.encoder.layer4.2.bn3.running_var', 'feature_extractor.encoder.layer4.2.bn3.num_batches_tracked', 'reduce_height_module.ghc_lst.0.layer.0.layers.0.1.weight', 'reduce_height_module.ghc_lst.0.layer.0.layers.0.1.bias', 'reduce_height_module.ghc_lst.0.layer.0.layers.1.weight', 'reduce_height_module.ghc_lst.0.layer.0.layers.1.bias', 'reduce_height_module.ghc_lst.0.layer.0.layers.1.running_mean', 'reduce_height_module.ghc_lst.0.layer.0.layers.1.running_var', 'reduce_height_module.ghc_lst.0.layer.0.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.0.layer.1.layers.0.1.weight', 'reduce_height_module.ghc_lst.0.layer.1.layers.0.1.bias', 'reduce_height_module.ghc_lst.0.layer.1.layers.1.weight', 'reduce_height_module.ghc_lst.0.layer.1.layers.1.bias', 'reduce_height_module.ghc_lst.0.layer.1.layers.1.running_mean', 'reduce_height_module.ghc_lst.0.layer.1.layers.1.running_var', 'reduce_height_module.ghc_lst.0.layer.1.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.0.layer.2.layers.0.1.weight', 'reduce_height_module.ghc_lst.0.layer.2.layers.0.1.bias', 'reduce_height_module.ghc_lst.0.layer.2.layers.1.weight', 'reduce_height_module.ghc_lst.0.layer.2.layers.1.bias', 'reduce_height_module.ghc_lst.0.layer.2.layers.1.running_mean', 'reduce_height_module.ghc_lst.0.layer.2.layers.1.running_var', 'reduce_height_module.ghc_lst.0.layer.2.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.0.layer.3.layers.0.1.weight', 'reduce_height_module.ghc_lst.0.layer.3.layers.0.1.bias', 'reduce_height_module.ghc_lst.0.layer.3.layers.1.weight', 'reduce_height_module.ghc_lst.0.layer.3.layers.1.bias', 'reduce_height_module.ghc_lst.0.layer.3.layers.1.running_mean', 'reduce_height_module.ghc_lst.0.layer.3.layers.1.running_var', 'reduce_height_module.ghc_lst.0.layer.3.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.1.layer.0.layers.0.1.weight', 'reduce_height_module.ghc_lst.1.layer.0.layers.0.1.bias', 'reduce_height_module.ghc_lst.1.layer.0.layers.1.weight', 'reduce_height_module.ghc_lst.1.layer.0.layers.1.bias', 'reduce_height_module.ghc_lst.1.layer.0.layers.1.running_mean', 'reduce_height_module.ghc_lst.1.layer.0.layers.1.running_var', 'reduce_height_module.ghc_lst.1.layer.0.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.1.layer.1.layers.0.1.weight', 'reduce_height_module.ghc_lst.1.layer.1.layers.0.1.bias', 'reduce_height_module.ghc_lst.1.layer.1.layers.1.weight', 'reduce_height_module.ghc_lst.1.layer.1.layers.1.bias', 'reduce_height_module.ghc_lst.1.layer.1.layers.1.running_mean', 'reduce_height_module.ghc_lst.1.layer.1.layers.1.running_var', 'reduce_height_module.ghc_lst.1.layer.1.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.1.layer.2.layers.0.1.weight', 'reduce_height_module.ghc_lst.1.layer.2.layers.0.1.bias', 'reduce_height_module.ghc_lst.1.layer.2.layers.1.weight', 'reduce_height_module.ghc_lst.1.layer.2.layers.1.bias', 'reduce_height_module.ghc_lst.1.layer.2.layers.1.running_mean', 'reduce_height_module.ghc_lst.1.layer.2.layers.1.running_var', 'reduce_height_module.ghc_lst.1.layer.2.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.1.layer.3.layers.0.1.weight', 'reduce_height_module.ghc_lst.1.layer.3.layers.0.1.bias', 'reduce_height_module.ghc_lst.1.layer.3.layers.1.weight', 'reduce_height_module.ghc_lst.1.layer.3.layers.1.bias', 'reduce_height_module.ghc_lst.1.layer.3.layers.1.running_mean', 'reduce_height_module.ghc_lst.1.layer.3.layers.1.running_var', 'reduce_height_module.ghc_lst.1.layer.3.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.2.layer.0.layers.0.1.weight', 'reduce_height_module.ghc_lst.2.layer.0.layers.0.1.bias', 'reduce_height_module.ghc_lst.2.layer.0.layers.1.weight', 'reduce_height_module.ghc_lst.2.layer.0.layers.1.bias', 'reduce_height_module.ghc_lst.2.layer.0.layers.1.running_mean', 'reduce_height_module.ghc_lst.2.layer.0.layers.1.running_var', 'reduce_height_module.ghc_lst.2.layer.0.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.2.layer.1.layers.0.1.weight', 'reduce_height_module.ghc_lst.2.layer.1.layers.0.1.bias', 'reduce_height_module.ghc_lst.2.layer.1.layers.1.weight', 'reduce_height_module.ghc_lst.2.layer.1.layers.1.bias', 'reduce_height_module.ghc_lst.2.layer.1.layers.1.running_mean', 'reduce_height_module.ghc_lst.2.layer.1.layers.1.running_var', 'reduce_height_module.ghc_lst.2.layer.1.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.2.layer.2.layers.0.1.weight', 'reduce_height_module.ghc_lst.2.layer.2.layers.0.1.bias', 'reduce_height_module.ghc_lst.2.layer.2.layers.1.weight', 'reduce_height_module.ghc_lst.2.layer.2.layers.1.bias', 'reduce_height_module.ghc_lst.2.layer.2.layers.1.running_mean', 'reduce_height_module.ghc_lst.2.layer.2.layers.1.running_var', 'reduce_height_module.ghc_lst.2.layer.2.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.2.layer.3.layers.0.1.weight', 'reduce_height_module.ghc_lst.2.layer.3.layers.0.1.bias', 'reduce_height_module.ghc_lst.2.layer.3.layers.1.weight', 'reduce_height_module.ghc_lst.2.layer.3.layers.1.bias', 'reduce_height_module.ghc_lst.2.layer.3.layers.1.running_mean', 'reduce_height_module.ghc_lst.2.layer.3.layers.1.running_var', 'reduce_height_module.ghc_lst.2.layer.3.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.3.layer.0.layers.0.1.weight', 'reduce_height_module.ghc_lst.3.layer.0.layers.0.1.bias', 'reduce_height_module.ghc_lst.3.layer.0.layers.1.weight', 'reduce_height_module.ghc_lst.3.layer.0.layers.1.bias', 'reduce_height_module.ghc_lst.3.layer.0.layers.1.running_mean', 'reduce_height_module.ghc_lst.3.layer.0.layers.1.running_var', 'reduce_height_module.ghc_lst.3.layer.0.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.3.layer.1.layers.0.1.weight', 'reduce_height_module.ghc_lst.3.layer.1.layers.0.1.bias', 'reduce_height_module.ghc_lst.3.layer.1.layers.1.weight', 'reduce_height_module.ghc_lst.3.layer.1.layers.1.bias', 'reduce_height_module.ghc_lst.3.layer.1.layers.1.running_mean', 'reduce_height_module.ghc_lst.3.layer.1.layers.1.running_var', 'reduce_height_module.ghc_lst.3.layer.1.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.3.layer.2.layers.0.1.weight', 'reduce_height_module.ghc_lst.3.layer.2.layers.0.1.bias', 'reduce_height_module.ghc_lst.3.layer.2.layers.1.weight', 'reduce_height_module.ghc_lst.3.layer.2.layers.1.bias', 'reduce_height_module.ghc_lst.3.layer.2.layers.1.running_mean', 'reduce_height_module.ghc_lst.3.layer.2.layers.1.running_var', 'reduce_height_module.ghc_lst.3.layer.2.layers.1.num_batches_tracked', 'reduce_height_module.ghc_lst.3.layer.3.layers.0.1.weight', 'reduce_height_module.ghc_lst.3.layer.3.layers.0.1.bias', 'reduce_height_module.ghc_lst.3.layer.3.layers.1.weight', 'reduce_height_module.ghc_lst.3.layer.3.layers.1.bias', 'reduce_height_module.ghc_lst.3.layer.3.layers.1.running_mean', 'reduce_height_module.ghc_lst.3.layer.3.layers.1.running_var', 'reduce_height_module.ghc_lst.3.layer.3.layers.1.num_batches_tracked', 'bi_rnn.weight_ih_l0', 'bi_rnn.weight_hh_l0', 'bi_rnn.bias_ih_l0', 'bi_rnn.bias_hh_l0', 'bi_rnn.weight_ih_l0_reverse', 'bi_rnn.weight_hh_l0_reverse', 'bi_rnn.bias_ih_l0_reverse', 'bi_rnn.bias_hh_l0_reverse', 'bi_rnn.weight_ih_l1', 'bi_rnn.weight_hh_l1', 'bi_rnn.bias_ih_l1', 'bi_rnn.bias_hh_l1', 'bi_rnn.weight_ih_l1_reverse', 'bi_rnn.weight_hh_l1_reverse', 'bi_rnn.bias_ih_l1_reverse', 'bi_rnn.bias_hh_l1_reverse', 'linear.weight', 'linear.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['backbone.encoder.conv1.weight', 'backbone.encoder.bn1.weight', 'backbone.encoder.bn1.bias', 'backbone.encoder.bn1.running_mean', 'backbone.encoder.bn1.running_var', 'backbone.encoder.layer1.0.conv1.weight', 'backbone.encoder.layer1.0.bn1.weight', 'backbone.encoder.layer1.0.bn1.bias', 'backbone.encoder.layer1.0.bn1.running_mean', 'backbone.encoder.layer1.0.bn1.running_var', 'backbone.encoder.layer1.0.conv2.weight', 'backbone.encoder.layer1.0.bn2.weight', 'backbone.encoder.layer1.0.bn2.bias', 'backbone.encoder.layer1.0.bn2.running_mean', 'backbone.encoder.layer1.0.bn2.running_var', 'backbone.encoder.layer1.0.conv3.weight', 'backbone.encoder.layer1.0.bn3.weight', 'backbone.encoder.layer1.0.bn3.bias', 'backbone.encoder.layer1.0.bn3.running_mean', 'backbone.encoder.layer1.0.bn3.running_var', 'backbone.encoder.layer1.0.downsample.0.weight', 'backbone.encoder.layer1.0.downsample.1.weight', 'backbone.encoder.layer1.0.downsample.1.bias', 'backbone.encoder.layer1.0.downsample.1.running_mean', 'backbone.encoder.layer1.0.downsample.1.running_var', 'backbone.encoder.layer1.1.conv1.weight', 'backbone.encoder.layer1.1.bn1.weight', 'backbone.encoder.layer1.1.bn1.bias', 'backbone.encoder.layer1.1.bn1.running_mean', 'backbone.encoder.layer1.1.bn1.running_var', 'backbone.encoder.layer1.1.conv2.weight', 'backbone.encoder.layer1.1.bn2.weight', 'backbone.encoder.layer1.1.bn2.bias', 'backbone.encoder.layer1.1.bn2.running_mean', 'backbone.encoder.layer1.1.bn2.running_var', 'backbone.encoder.layer1.1.conv3.weight', 'backbone.encoder.layer1.1.bn3.weight', 'backbone.encoder.layer1.1.bn3.bias', 'backbone.encoder.layer1.1.bn3.running_mean', 'backbone.encoder.layer1.1.bn3.running_var', 'backbone.encoder.layer1.2.conv1.weight', 'backbone.encoder.layer1.2.bn1.weight', 'backbone.encoder.layer1.2.bn1.bias', 'backbone.encoder.layer1.2.bn1.running_mean', 'backbone.encoder.layer1.2.bn1.running_var', 'backbone.encoder.layer1.2.conv2.weight', 'backbone.encoder.layer1.2.bn2.weight', 'backbone.encoder.layer1.2.bn2.bias', 'backbone.encoder.layer1.2.bn2.running_mean', 'backbone.encoder.layer1.2.bn2.running_var', 'backbone.encoder.layer1.2.conv3.weight', 'backbone.encoder.layer1.2.bn3.weight', 'backbone.encoder.layer1.2.bn3.bias', 'backbone.encoder.layer1.2.bn3.running_mean', 'backbone.encoder.layer1.2.bn3.running_var', 'backbone.encoder.layer2.0.conv1.weight', 'backbone.encoder.layer2.0.bn1.weight', 'backbone.encoder.layer2.0.bn1.bias', 'backbone.encoder.layer2.0.bn1.running_mean', 'backbone.encoder.layer2.0.bn1.running_var', 'backbone.encoder.layer2.0.conv2.weight', 'backbone.encoder.layer2.0.bn2.weight', 'backbone.encoder.layer2.0.bn2.bias', 'backbone.encoder.layer2.0.bn2.running_mean', 'backbone.encoder.layer2.0.bn2.running_var', 'backbone.encoder.layer2.0.conv3.weight', 'backbone.encoder.layer2.0.bn3.weight', 'backbone.encoder.layer2.0.bn3.bias', 'backbone.encoder.layer2.0.bn3.running_mean', 'backbone.encoder.layer2.0.bn3.running_var', 'backbone.encoder.layer2.0.downsample.0.weight', 'backbone.encoder.layer2.0.downsample.1.weight', 'backbone.encoder.layer2.0.downsample.1.bias', 'backbone.encoder.layer2.0.downsample.1.running_mean', 'backbone.encoder.layer2.0.downsample.1.running_var', 'backbone.encoder.layer2.1.conv1.weight', 'backbone.encoder.layer2.1.bn1.weight', 'backbone.encoder.layer2.1.bn1.bias', 'backbone.encoder.layer2.1.bn1.running_mean', 'backbone.encoder.layer2.1.bn1.running_var', 'backbone.encoder.layer2.1.conv2.weight', 'backbone.encoder.layer2.1.bn2.weight', 'backbone.encoder.layer2.1.bn2.bias', 'backbone.encoder.layer2.1.bn2.running_mean', 'backbone.encoder.layer2.1.bn2.running_var', 'backbone.encoder.layer2.1.conv3.weight', 'backbone.encoder.layer2.1.bn3.weight', 'backbone.encoder.layer2.1.bn3.bias', 'backbone.encoder.layer2.1.bn3.running_mean', 'backbone.encoder.layer2.1.bn3.running_var', 'backbone.encoder.layer2.2.conv1.weight', 'backbone.encoder.layer2.2.bn1.weight', 'backbone.encoder.layer2.2.bn1.bias', 'backbone.encoder.layer2.2.bn1.running_mean', 'backbone.encoder.layer2.2.bn1.running_var', 'backbone.encoder.layer2.2.conv2.weight', 'backbone.encoder.layer2.2.bn2.weight', 'backbone.encoder.layer2.2.bn2.bias', 'backbone.encoder.layer2.2.bn2.running_mean', 'backbone.encoder.layer2.2.bn2.running_var', 'backbone.encoder.layer2.2.conv3.weight', 'backbone.encoder.layer2.2.bn3.weight', 'backbone.encoder.layer2.2.bn3.bias', 'backbone.encoder.layer2.2.bn3.running_mean', 'backbone.encoder.layer2.2.bn3.running_var', 'backbone.encoder.layer2.3.conv1.weight', 'backbone.encoder.layer2.3.bn1.weight', 'backbone.encoder.layer2.3.bn1.bias', 'backbone.encoder.layer2.3.bn1.running_mean', 'backbone.encoder.layer2.3.bn1.running_var', 'backbone.encoder.layer2.3.conv2.weight', 'backbone.encoder.layer2.3.bn2.weight', 'backbone.encoder.layer2.3.bn2.bias', 'backbone.encoder.layer2.3.bn2.running_mean', 'backbone.encoder.layer2.3.bn2.running_var', 'backbone.encoder.layer2.3.conv3.weight', 'backbone.encoder.layer2.3.bn3.weight', 'backbone.encoder.layer2.3.bn3.bias', 'backbone.encoder.layer2.3.bn3.running_mean', 'backbone.encoder.layer2.3.bn3.running_var', 'backbone.encoder.layer3.0.conv1.weight', 'backbone.encoder.layer3.0.bn1.weight', 'backbone.encoder.layer3.0.bn1.bias', 'backbone.encoder.layer3.0.bn1.running_mean', 'backbone.encoder.layer3.0.bn1.running_var', 'backbone.encoder.layer3.0.conv2.weight', 'backbone.encoder.layer3.0.bn2.weight', 'backbone.encoder.layer3.0.bn2.bias', 'backbone.encoder.layer3.0.bn2.running_mean', 'backbone.encoder.layer3.0.bn2.running_var', 'backbone.encoder.layer3.0.conv3.weight', 'backbone.encoder.layer3.0.bn3.weight', 'backbone.encoder.layer3.0.bn3.bias', 'backbone.encoder.layer3.0.bn3.running_mean', 'backbone.encoder.layer3.0.bn3.running_var', 'backbone.encoder.layer3.0.downsample.0.weight', 'backbone.encoder.layer3.0.downsample.1.weight', 'backbone.encoder.layer3.0.downsample.1.bias', 'backbone.encoder.layer3.0.downsample.1.running_mean', 'backbone.encoder.layer3.0.downsample.1.running_var', 'backbone.encoder.layer3.1.conv1.weight', 'backbone.encoder.layer3.1.bn1.weight', 'backbone.encoder.layer3.1.bn1.bias', 'backbone.encoder.layer3.1.bn1.running_mean', 'backbone.encoder.layer3.1.bn1.running_var', 'backbone.encoder.layer3.1.conv2.weight', 'backbone.encoder.layer3.1.bn2.weight', 'backbone.encoder.layer3.1.bn2.bias', 'backbone.encoder.layer3.1.bn2.running_mean', 'backbone.encoder.layer3.1.bn2.running_var', 'backbone.encoder.layer3.1.conv3.weight', 'backbone.encoder.layer3.1.bn3.weight', 'backbone.encoder.layer3.1.bn3.bias', 'backbone.encoder.layer3.1.bn3.running_mean', 'backbone.encoder.layer3.1.bn3.running_var', 'backbone.encoder.layer3.2.conv1.weight', 'backbone.encoder.layer3.2.bn1.weight', 'backbone.encoder.layer3.2.bn1.bias', 'backbone.encoder.layer3.2.bn1.running_mean', 'backbone.encoder.layer3.2.bn1.running_var', 'backbone.encoder.layer3.2.conv2.weight', 'backbone.encoder.layer3.2.bn2.weight', 'backbone.encoder.layer3.2.bn2.bias', 'backbone.encoder.layer3.2.bn2.running_mean', 'backbone.encoder.layer3.2.bn2.running_var', 'backbone.encoder.layer3.2.conv3.weight', 'backbone.encoder.layer3.2.bn3.weight', 'backbone.encoder.layer3.2.bn3.bias', 'backbone.encoder.layer3.2.bn3.running_mean', 'backbone.encoder.layer3.2.bn3.running_var', 'backbone.encoder.layer3.3.conv1.weight', 'backbone.encoder.layer3.3.bn1.weight', 'backbone.encoder.layer3.3.bn1.bias', 'backbone.encoder.layer3.3.bn1.running_mean', 'backbone.encoder.layer3.3.bn1.running_var', 'backbone.encoder.layer3.3.conv2.weight', 'backbone.encoder.layer3.3.bn2.weight', 'backbone.encoder.layer3.3.bn2.bias', 'backbone.encoder.layer3.3.bn2.running_mean', 'backbone.encoder.layer3.3.bn2.running_var', 'backbone.encoder.layer3.3.conv3.weight', 'backbone.encoder.layer3.3.bn3.weight', 'backbone.encoder.layer3.3.bn3.bias', 'backbone.encoder.layer3.3.bn3.running_mean', 'backbone.encoder.layer3.3.bn3.running_var', 'backbone.encoder.layer3.4.conv1.weight', 'backbone.encoder.layer3.4.bn1.weight', 'backbone.encoder.layer3.4.bn1.bias', 'backbone.encoder.layer3.4.bn1.running_mean', 'backbone.encoder.layer3.4.bn1.running_var', 'backbone.encoder.layer3.4.conv2.weight', 'backbone.encoder.layer3.4.bn2.weight', 'backbone.encoder.layer3.4.bn2.bias', 'backbone.encoder.layer3.4.bn2.running_mean', 'backbone.encoder.layer3.4.bn2.running_var', 'backbone.encoder.layer3.4.conv3.weight', 'backbone.encoder.layer3.4.bn3.weight', 'backbone.encoder.layer3.4.bn3.bias', 'backbone.encoder.layer3.4.bn3.running_mean', 'backbone.encoder.layer3.4.bn3.running_var', 'backbone.encoder.layer3.5.conv1.weight', 'backbone.encoder.layer3.5.bn1.weight', 'backbone.encoder.layer3.5.bn1.bias', 'backbone.encoder.layer3.5.bn1.running_mean', 'backbone.encoder.layer3.5.bn1.running_var', 'backbone.encoder.layer3.5.conv2.weight', 'backbone.encoder.layer3.5.bn2.weight', 'backbone.encoder.layer3.5.bn2.bias', 'backbone.encoder.layer3.5.bn2.running_mean', 'backbone.encoder.layer3.5.bn2.running_var', 'backbone.encoder.layer3.5.conv3.weight', 'backbone.encoder.layer3.5.bn3.weight', 'backbone.encoder.layer3.5.bn3.bias', 'backbone.encoder.layer3.5.bn3.running_mean', 'backbone.encoder.layer3.5.bn3.running_var', 'backbone.encoder.layer4.0.conv1.weight', 'backbone.encoder.layer4.0.bn1.weight', 'backbone.encoder.layer4.0.bn1.bias', 'backbone.encoder.layer4.0.bn1.running_mean', 'backbone.encoder.layer4.0.bn1.running_var', 'backbone.encoder.layer4.0.conv2.weight', 'backbone.encoder.layer4.0.bn2.weight', 'backbone.encoder.layer4.0.bn2.bias', 'backbone.encoder.layer4.0.bn2.running_mean', 'backbone.encoder.layer4.0.bn2.running_var', 'backbone.encoder.layer4.0.conv3.weight', 'backbone.encoder.layer4.0.bn3.weight', 'backbone.encoder.layer4.0.bn3.bias', 'backbone.encoder.layer4.0.bn3.running_mean', 'backbone.encoder.layer4.0.bn3.running_var', 'backbone.encoder.layer4.0.downsample.0.weight', 'backbone.encoder.layer4.0.downsample.1.weight', 'backbone.encoder.layer4.0.downsample.1.bias', 'backbone.encoder.layer4.0.downsample.1.running_mean', 'backbone.encoder.layer4.0.downsample.1.running_var', 'backbone.encoder.layer4.1.conv1.weight', 'backbone.encoder.layer4.1.bn1.weight', 'backbone.encoder.layer4.1.bn1.bias', 'backbone.encoder.layer4.1.bn1.running_mean', 'backbone.encoder.layer4.1.bn1.running_var', 'backbone.encoder.layer4.1.conv2.weight', 'backbone.encoder.layer4.1.bn2.weight', 'backbone.encoder.layer4.1.bn2.bias', 'backbone.encoder.layer4.1.bn2.running_mean', 'backbone.encoder.layer4.1.bn2.running_var', 'backbone.encoder.layer4.1.conv3.weight', 'backbone.encoder.layer4.1.bn3.weight', 'backbone.encoder.layer4.1.bn3.bias', 'backbone.encoder.layer4.1.bn3.running_mean', 'backbone.encoder.layer4.1.bn3.running_var', 'backbone.encoder.layer4.2.conv1.weight', 'backbone.encoder.layer4.2.bn1.weight', 'backbone.encoder.layer4.2.bn1.bias', 'backbone.encoder.layer4.2.bn1.running_mean', 'backbone.encoder.layer4.2.bn1.running_var', 'backbone.encoder.layer4.2.conv2.weight', 'backbone.encoder.layer4.2.bn2.weight', 'backbone.encoder.layer4.2.bn2.bias', 'backbone.encoder.layer4.2.bn2.running_mean', 'backbone.encoder.layer4.2.bn2.running_var', 'backbone.encoder.layer4.2.conv3.weight', 'backbone.encoder.layer4.2.bn3.weight', 'backbone.encoder.layer4.2.bn3.bias', 'backbone.encoder.layer4.2.bn3.running_mean', 'backbone.encoder.layer4.2.bn3.running_var', 'transformer.enc_embedding.weight', 'transformer.encoder_layer.self_attn.in_proj_weight', 'transformer.encoder_layer.self_attn.in_proj_bias', 'transformer.encoder_layer.self_attn.out_proj.weight', 'transformer.encoder_layer.self_attn.out_proj.bias', 'transformer.encoder_layer.linear1.weight', 'transformer.encoder_layer.linear1.bias', 'transformer.encoder_layer.linear2.weight', 'transformer.encoder_layer.linear2.bias', 'transformer.encoder_layer.norm1.weight', 'transformer.encoder_layer.norm1.bias', 'transformer.encoder_layer.norm2.weight', 'transformer.encoder_layer.norm2.bias', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.dec_embedding.weight', 'transformer.decoder_layer.self_attn.in_proj_weight', 'transformer.decoder_layer.self_attn.in_proj_bias', 'transformer.decoder_layer.self_attn.out_proj.weight', 'transformer.decoder_layer.self_attn.out_proj.bias', 'transformer.decoder_layer.multihead_attn.in_proj_weight', 'transformer.decoder_layer.multihead_attn.in_proj_bias', 'transformer.decoder_layer.multihead_attn.out_proj.weight', 'transformer.decoder_layer.multihead_attn.out_proj.bias', 'transformer.decoder_layer.linear1.weight', 'transformer.decoder_layer.linear1.bias', 'transformer.decoder_layer.linear2.weight', 'transformer.decoder_layer.linear2.bias', 'transformer.decoder_layer.norm1.weight', 'transformer.decoder_layer.norm1.bias', 'transformer.decoder_layer.norm2.weight', 'transformer.decoder_layer.norm2.bias', 'transformer.decoder_layer.norm3.weight', 'transformer.decoder_layer.norm3.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.out_proj.weight', 'transformer.decoder.layers.1.self_attn.out_proj.bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'transformer.decoder.layers.1.linear1.weight', 'transformer.decoder.layers.1.linear1.bias', 'transformer.decoder.layers.1.linear2.weight', 'transformer.decoder.layers.1.linear2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.layers.2.self_attn.in_proj_weight', 'transformer.decoder.layers.2.self_attn.in_proj_bias', 'transformer.decoder.layers.2.self_attn.out_proj.weight', 'transformer.decoder.layers.2.self_attn.out_proj.bias', 'transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'transformer.decoder.layers.2.multihead_attn.out_proj.weight', 'transformer.decoder.layers.2.multihead_attn.out_proj.bias', 'transformer.decoder.layers.2.linear1.weight', 'transformer.decoder.layers.2.linear1.bias', 'transformer.decoder.layers.2.linear2.weight', 'transformer.decoder.layers.2.linear2.bias', 'transformer.decoder.layers.2.norm1.weight', 'transformer.decoder.layers.2.norm1.bias', 'transformer.decoder.layers.2.norm2.weight', 'transformer.decoder.layers.2.norm2.bias', 'transformer.decoder.layers.2.norm3.weight', 'transformer.decoder.layers.2.norm3.bias', 'transformer.decoder.layers.3.self_attn.in_proj_weight', 'transformer.decoder.layers.3.self_attn.in_proj_bias', 'transformer.decoder.layers.3.self_attn.out_proj.weight', 'transformer.decoder.layers.3.self_attn.out_proj.bias', 'transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'transformer.decoder.layers.3.multihead_attn.out_proj.weight', 'transformer.decoder.layers.3.multihead_attn.out_proj.bias', 'transformer.decoder.layers.3.linear1.weight', 'transformer.decoder.layers.3.linear1.bias', 'transformer.decoder.layers.3.linear2.weight', 'transformer.decoder.layers.3.linear2.bias', 'transformer.decoder.layers.3.norm1.weight', 'transformer.decoder.layers.3.norm1.bias', 'transformer.decoder.layers.3.norm2.weight', 'transformer.decoder.layers.3.norm2.bias', 'transformer.decoder.layers.3.norm3.weight', 'transformer.decoder.layers.3.norm3.bias', 'transformer.decoder.layers.4.self_attn.in_proj_weight', 'transformer.decoder.layers.4.self_attn.in_proj_bias', 'transformer.decoder.layers.4.self_attn.out_proj.weight', 'transformer.decoder.layers.4.self_attn.out_proj.bias', 'transformer.decoder.layers.4.multihead_attn.in_proj_weight', 'transformer.decoder.layers.4.multihead_attn.in_proj_bias', 'transformer.decoder.layers.4.multihead_attn.out_proj.weight', 'transformer.decoder.layers.4.multihead_attn.out_proj.bias', 'transformer.decoder.layers.4.linear1.weight', 'transformer.decoder.layers.4.linear1.bias', 'transformer.decoder.layers.4.linear2.weight', 'transformer.decoder.layers.4.linear2.bias', 'transformer.decoder.layers.4.norm1.weight', 'transformer.decoder.layers.4.norm1.bias', 'transformer.decoder.layers.4.norm2.weight', 'transformer.decoder.layers.4.norm2.bias', 'transformer.decoder.layers.4.norm3.weight', 'transformer.decoder.layers.4.norm3.bias', 'transformer.decoder.layers.5.self_attn.in_proj_weight', 'transformer.decoder.layers.5.self_attn.in_proj_bias', 'transformer.decoder.layers.5.self_attn.out_proj.weight', 'transformer.decoder.layers.5.self_attn.out_proj.bias', 'transformer.decoder.layers.5.multihead_attn.in_proj_weight', 'transformer.decoder.layers.5.multihead_attn.in_proj_bias', 'transformer.decoder.layers.5.multihead_attn.out_proj.weight', 'transformer.decoder.layers.5.multihead_attn.out_proj.bias', 'transformer.decoder.layers.5.linear1.weight', 'transformer.decoder.layers.5.linear1.bias', 'transformer.decoder.layers.5.linear2.weight', 'transformer.decoder.layers.5.linear2.bias', 'transformer.decoder.layers.5.norm1.weight', 'transformer.decoder.layers.5.norm1.bias', 'transformer.decoder.layers.5.norm2.weight', 'transformer.decoder.layers.5.norm2.bias', 'transformer.decoder.layers.5.norm3.weight', 'transformer.decoder.layers.5.norm3.bias', 'vqt_box_head.weight', 'vqt_box_head.bias', 'vqt_cls_head.weight', 'vqt_cls_head.bias', 'reduce_height_module.ghc_lst.0.layer.0.layers.0.weight', 'reduce_height_module.ghc_lst.0.layer.0.layers.0.bias', 'reduce_height_module.ghc_lst.0.layer.1.layers.0.weight', 'reduce_height_module.ghc_lst.0.layer.1.layers.0.bias', 'reduce_height_module.ghc_lst.0.layer.2.layers.0.weight', 'reduce_height_module.ghc_lst.0.layer.2.layers.0.bias', 'reduce_height_module.ghc_lst.0.layer.3.layers.0.weight', 'reduce_height_module.ghc_lst.0.layer.3.layers.0.bias', 'reduce_height_module.ghc_lst.1.layer.0.layers.0.weight', 'reduce_height_module.ghc_lst.1.layer.0.layers.0.bias', 'reduce_height_module.ghc_lst.1.layer.1.layers.0.weight', 'reduce_height_module.ghc_lst.1.layer.1.layers.0.bias', 'reduce_height_module.ghc_lst.1.layer.2.layers.0.weight', 'reduce_height_module.ghc_lst.1.layer.2.layers.0.bias', 'reduce_height_module.ghc_lst.1.layer.3.layers.0.weight', 'reduce_height_module.ghc_lst.1.layer.3.layers.0.bias', 'reduce_height_module.ghc_lst.2.layer.0.layers.0.weight', 'reduce_height_module.ghc_lst.2.layer.0.layers.0.bias', 'reduce_height_module.ghc_lst.2.layer.1.layers.0.weight', 'reduce_height_module.ghc_lst.2.layer.1.layers.0.bias', 'reduce_height_module.ghc_lst.2.layer.2.layers.0.weight', 'reduce_height_module.ghc_lst.2.layer.2.layers.0.bias', 'reduce_height_module.ghc_lst.2.layer.3.layers.0.weight', 'reduce_height_module.ghc_lst.2.layer.3.layers.0.bias', 'reduce_height_module.ghc_lst.3.layer.0.layers.0.weight', 'reduce_height_module.ghc_lst.3.layer.0.layers.0.bias', 'reduce_height_module.ghc_lst.3.layer.1.layers.0.weight', 'reduce_height_module.ghc_lst.3.layer.1.layers.0.bias', 'reduce_height_module.ghc_lst.3.layer.2.layers.0.weight', 'reduce_height_module.ghc_lst.3.layer.2.layers.0.bias', 'reduce_height_module.ghc_lst.3.layer.3.layers.0.weight', 'reduce_height_module.ghc_lst.3.layer.3.layers.0.bias', 'v_reproj.weight', 'v_reproj.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "horizon_path =r\"D:/OneDrive/OneDrive - NTHU/Layout/Horizon/0912_all_bk.pth\"\n",
    "#models_dict = torch.load_s\n",
    "checkpoint = torch.load(horizon_path ,  map_location=\"cpu\")\n",
    "print(checkpoint['state_dict'].keys())\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in m.state_dict()}\n",
    "m.load_state_dict(pretrained_dict , strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(200).view(2,100,1)\n",
    "#aa = a[:,:,0].unsqueeze(0)\n",
    "#print(aa.shape)\n",
    "b = F.interpolate(a.view(2,-1).unsqueeze(0), 10 )[0]\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1426, 0.5408, 0.1219, 0.1724, 0.7682],\n",
      "        [0.2389, 0.1592, 0.2596, 0.5540, 0.3866],\n",
      "        [0.5219, 0.1237, 0.5426, 0.8369, 0.1037],\n",
      "        [0.3582, 0.0399, 0.3789, 0.6733, 0.2673],\n",
      "        [0.6216, 0.2235, 0.6423, 0.9367, 0.0039],\n",
      "        [0.3501, 0.7483, 0.3295, 0.0351, 0.9757],\n",
      "        [0.2889, 0.6871, 0.2683, 0.0261, 0.9145],\n",
      "        [0.3629, 0.0352, 0.3836, 0.6780, 0.2626],\n",
      "        [0.4082, 0.0101, 0.4289, 0.7233, 0.2173],\n",
      "        [0.4492, 0.0511, 0.4699, 0.7643, 0.1763],\n",
      "        [0.5382, 0.1401, 0.5589, 0.8533, 0.0873],\n",
      "        [0.0368, 0.3614, 0.0575, 0.3518, 0.5888],\n",
      "        [0.3789, 0.0193, 0.3995, 0.6939, 0.2467],\n",
      "        [0.2068, 0.1913, 0.2275, 0.5219, 0.4187],\n",
      "        [0.2395, 0.1587, 0.2602, 0.5545, 0.3861],\n",
      "        [0.1033, 0.5014, 0.0826, 0.2118, 0.7288],\n",
      "        [0.3085, 0.0896, 0.3292, 0.6236, 0.3170],\n",
      "        [0.1729, 0.2252, 0.1936, 0.4880, 0.4526],\n",
      "        [0.5994, 0.2012, 0.6201, 0.9144, 0.0262],\n",
      "        [0.2217, 0.6199, 0.2010, 0.0933, 0.8473]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(20).view(-1,1)\n",
    "b = torch.rand(5).view(-1,1)\n",
    "c = torch.cdist(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0,   1],\n",
      "         [  2,   3],\n",
      "         [  4,   5],\n",
      "         [  6,   7],\n",
      "         [  8,   9],\n",
      "         [ 10,  11],\n",
      "         [ 12,  13],\n",
      "         [ 14,  15],\n",
      "         [ 16,  17],\n",
      "         [ 18,  19],\n",
      "         [ 20,  21],\n",
      "         [ 22,  23],\n",
      "         [ 24,  25],\n",
      "         [ 26,  27],\n",
      "         [ 28,  29],\n",
      "         [ 30,  31],\n",
      "         [ 32,  33],\n",
      "         [ 34,  35],\n",
      "         [ 36,  37],\n",
      "         [ 38,  39],\n",
      "         [ 40,  41],\n",
      "         [ 42,  43],\n",
      "         [ 44,  45],\n",
      "         [ 46,  47],\n",
      "         [ 48,  49],\n",
      "         [ 50,  51],\n",
      "         [ 52,  53],\n",
      "         [ 54,  55],\n",
      "         [ 56,  57],\n",
      "         [ 58,  59],\n",
      "         [ 60,  61],\n",
      "         [ 62,  63],\n",
      "         [ 64,  65],\n",
      "         [ 66,  67],\n",
      "         [ 68,  69],\n",
      "         [ 70,  71],\n",
      "         [ 72,  73],\n",
      "         [ 74,  75],\n",
      "         [ 76,  77],\n",
      "         [ 78,  79],\n",
      "         [ 80,  81],\n",
      "         [ 82,  83],\n",
      "         [ 84,  85],\n",
      "         [ 86,  87],\n",
      "         [ 88,  89],\n",
      "         [ 90,  91],\n",
      "         [ 92,  93],\n",
      "         [ 94,  95],\n",
      "         [ 96,  97],\n",
      "         [ 98,  99],\n",
      "         [100, 101],\n",
      "         [102, 103],\n",
      "         [104, 105],\n",
      "         [106, 107],\n",
      "         [108, 109],\n",
      "         [110, 111],\n",
      "         [112, 113],\n",
      "         [114, 115],\n",
      "         [116, 117],\n",
      "         [118, 119],\n",
      "         [120, 121],\n",
      "         [122, 123],\n",
      "         [124, 125],\n",
      "         [126, 127],\n",
      "         [128, 129],\n",
      "         [130, 131],\n",
      "         [132, 133],\n",
      "         [134, 135],\n",
      "         [136, 137],\n",
      "         [138, 139],\n",
      "         [140, 141],\n",
      "         [142, 143],\n",
      "         [144, 145],\n",
      "         [146, 147],\n",
      "         [148, 149],\n",
      "         [150, 151],\n",
      "         [152, 153],\n",
      "         [154, 155],\n",
      "         [156, 157],\n",
      "         [158, 159],\n",
      "         [160, 161],\n",
      "         [162, 163],\n",
      "         [164, 165],\n",
      "         [166, 167],\n",
      "         [168, 169],\n",
      "         [170, 171],\n",
      "         [172, 173],\n",
      "         [174, 175],\n",
      "         [176, 177],\n",
      "         [178, 179],\n",
      "         [180, 181],\n",
      "         [182, 183],\n",
      "         [184, 185],\n",
      "         [186, 187],\n",
      "         [188, 189],\n",
      "         [190, 191],\n",
      "         [192, 193],\n",
      "         [194, 195],\n",
      "         [196, 197],\n",
      "         [198, 199]],\n",
      "\n",
      "        [[200, 201],\n",
      "         [202, 203],\n",
      "         [204, 205],\n",
      "         [206, 207],\n",
      "         [208, 209],\n",
      "         [210, 211],\n",
      "         [212, 213],\n",
      "         [214, 215],\n",
      "         [216, 217],\n",
      "         [218, 219],\n",
      "         [220, 221],\n",
      "         [222, 223],\n",
      "         [224, 225],\n",
      "         [226, 227],\n",
      "         [228, 229],\n",
      "         [230, 231],\n",
      "         [232, 233],\n",
      "         [234, 235],\n",
      "         [236, 237],\n",
      "         [238, 239],\n",
      "         [240, 241],\n",
      "         [242, 243],\n",
      "         [244, 245],\n",
      "         [246, 247],\n",
      "         [248, 249],\n",
      "         [250, 251],\n",
      "         [252, 253],\n",
      "         [254, 255],\n",
      "         [256, 257],\n",
      "         [258, 259],\n",
      "         [260, 261],\n",
      "         [262, 263],\n",
      "         [264, 265],\n",
      "         [266, 267],\n",
      "         [268, 269],\n",
      "         [270, 271],\n",
      "         [272, 273],\n",
      "         [274, 275],\n",
      "         [276, 277],\n",
      "         [278, 279],\n",
      "         [280, 281],\n",
      "         [282, 283],\n",
      "         [284, 285],\n",
      "         [286, 287],\n",
      "         [288, 289],\n",
      "         [290, 291],\n",
      "         [292, 293],\n",
      "         [294, 295],\n",
      "         [296, 297],\n",
      "         [298, 299],\n",
      "         [300, 301],\n",
      "         [302, 303],\n",
      "         [304, 305],\n",
      "         [306, 307],\n",
      "         [308, 309],\n",
      "         [310, 311],\n",
      "         [312, 313],\n",
      "         [314, 315],\n",
      "         [316, 317],\n",
      "         [318, 319],\n",
      "         [320, 321],\n",
      "         [322, 323],\n",
      "         [324, 325],\n",
      "         [326, 327],\n",
      "         [328, 329],\n",
      "         [330, 331],\n",
      "         [332, 333],\n",
      "         [334, 335],\n",
      "         [336, 337],\n",
      "         [338, 339],\n",
      "         [340, 341],\n",
      "         [342, 343],\n",
      "         [344, 345],\n",
      "         [346, 347],\n",
      "         [348, 349],\n",
      "         [350, 351],\n",
      "         [352, 353],\n",
      "         [354, 355],\n",
      "         [356, 357],\n",
      "         [358, 359],\n",
      "         [360, 361],\n",
      "         [362, 363],\n",
      "         [364, 365],\n",
      "         [366, 367],\n",
      "         [368, 369],\n",
      "         [370, 371],\n",
      "         [372, 373],\n",
      "         [374, 375],\n",
      "         [376, 377],\n",
      "         [378, 379],\n",
      "         [380, 381],\n",
      "         [382, 383],\n",
      "         [384, 385],\n",
      "         [386, 387],\n",
      "         [388, 389],\n",
      "         [390, 391],\n",
      "         [392, 393],\n",
      "         [394, 395],\n",
      "         [396, 397],\n",
      "         [398, 399]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
      "tensor([[[ 0,  0],\n",
      "         [ 1,  1],\n",
      "         [ 2,  2],\n",
      "         [ 3,  3],\n",
      "         [ 4,  4],\n",
      "         [ 5,  5],\n",
      "         [ 6,  6],\n",
      "         [ 7,  7],\n",
      "         [ 8,  8],\n",
      "         [ 9,  9]],\n",
      "\n",
      "        [[10, 10],\n",
      "         [11, 11],\n",
      "         [12, 12],\n",
      "         [13, 13],\n",
      "         [14, 14],\n",
      "         [15, 15],\n",
      "         [16, 16],\n",
      "         [17, 17],\n",
      "         [18, 18],\n",
      "         [19, 19]]])\n",
      "torch.Size([2, 10, 2])\n",
      "tensor([[[  0,   1],\n",
      "         [  2,   3],\n",
      "         [  4,   5],\n",
      "         [  6,   7],\n",
      "         [  8,   9],\n",
      "         [ 10,  11],\n",
      "         [ 12,  13],\n",
      "         [ 14,  15],\n",
      "         [ 16,  17],\n",
      "         [ 18,  19]],\n",
      "\n",
      "        [[220, 221],\n",
      "         [222, 223],\n",
      "         [224, 225],\n",
      "         [226, 227],\n",
      "         [228, 229],\n",
      "         [230, 231],\n",
      "         [232, 233],\n",
      "         [234, 235],\n",
      "         [236, 237],\n",
      "         [238, 239]]])\n"
     ]
    }
   ],
   "source": [
    "#a = torch.rand(2,100,1)\n",
    "a = torch.arange(400).view(2,100,2)\n",
    "b = torch.arange(20).view(2,10)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "b= b.unsqueeze(-1).repeat(1,1,2)\n",
    "print(b)\n",
    "#print(\"b unsqueeze\",b.unsqueeze(-1))\n",
    "\n",
    "c = a.gather(1, b)\n",
    "print(c.shape)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.arange(5)\n",
    "b=torch.arange(5)\n",
    "c=torch.arange(5)\n",
    "\n",
    "d = torch.vstack([a,b,c]).permute(1,0)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "a = torch.tensor([ [0,1,2]  ,  [0,3,5] , [1,0,5] ]).to(torch.float32)\n",
    "b = torch.tensor([ [0,1,2] , [1,0,5] ]).to(torch.float32)\n",
    "\n",
    "cost = torch.cdist(b,a)\n",
    "print(cost)\n",
    "row , col = linear_sum_assignment(cost,)\n",
    "print(row)\n",
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.7605, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7730, 0.5752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7057, 0.5861, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8386, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8304, 0.7823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7034, 0.5994, 0.5691, 0.5652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.6996, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8305, 0.7819, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8238, 0.7839, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "# Get the indices of non-zero elements\n",
    "non_zero_indices = torch.nonzero(x)\n",
    "print(non_zero_indices)\n",
    "# Get the non-zero values\n",
    "non_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "unique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "print(\"unique\" , unique)\n",
    "# Print the result\n",
    "print(non_zero_values)\n",
    "non_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "print(\"split non_zero_values\" , non_zero_values)\n",
    "\n",
    "def unpad_data( x :[Tensor] ) :\n",
    "\tnon_zero_indices = torch.nonzero(x)\n",
    "\tprint(non_zero_indices)\n",
    "\t# Get the non-zero values\n",
    "\tnon_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "\tunique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "\tprint(\"unique\" , unique)\n",
    "\t# Print the result\n",
    "\tprint(non_zero_values)\n",
    "\tnon_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "\tprint(\"split non_zero_values\" , non_zero_values)\n",
    "\treturn non_zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.58 , 0.6] , [0.4] ] , )\n",
    "b = torch.tensor([0.1 , 0.2] , )\n",
    "\n",
    "c = a.repeat(2)\n",
    "print(a.repeat(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.rand(1)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Layout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
