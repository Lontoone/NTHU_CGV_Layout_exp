{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\layout\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Layout\\NTHU_CGV_Layout_exp\\ours\n",
      "d:\\Projects\\Layout\\NTHU_CGV_Layout_exp\\Horizon_and_SAM\\Horizon\n"
     ]
    }
   ],
   "source": [
    "# Data Loader\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import json\n",
    "from CustomDataset import * \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from config import *\n",
    "from file_helper import *\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "#=================================\n",
    "#             Augmentation\n",
    "#=================================\n",
    "\n",
    "def gauss_noise_tensor(img):\n",
    "    rand = torch.rand(1)[0]\n",
    "    if rand < 0.5 and Horizon_AUG:\n",
    "        sigma = rand *0.125\n",
    "        out = img + sigma * torch.randn_like(img)\n",
    "        return out\n",
    "    return img\n",
    "\n",
    "def blank(img):    \n",
    "    return img\n",
    "\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self , train_dir , test_dir , batch_size = 2, num_workers = 0 , img_size=[IMG_WIDTH, IMG_HEIGHT] , use_aug = True ,padding_count = 24 ,c =0.1 ):\n",
    "        super().__init__()\n",
    "        self.train_dir = train_dir\n",
    "        self.test_dir = test_dir\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_size = img_size      \n",
    "        self.use_aug = use_aug\n",
    "        self.padding_count  = padding_count\n",
    "        self.c = c\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download dataset\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # Create dataset...          \n",
    "                \n",
    "        self.entire_dataset = CustomDataset(self.train_dir  , use_aug= self.use_aug , padding_count= self.padding_count , c=self.c)\n",
    "        self.train_ds , self.val_ds = random_split(self.entire_dataset , [0.9, 0.1])        \n",
    "        self.test_ds = CustomDataset(self.test_dir  , use_aug= False)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # ToDo: Reture Dataloader...\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=False)\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "dm = CustomDataModule ( train_dir= f\"../anno/test_visiable_10_no_cross.json\" ,\n",
    "                       test_dir= f\"../anno/test_visiable_10_no_cross.json\" , padding_count=256\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "def unpad_data( x :[Tensor] ) :\n",
    "\tnon_zero_indices = torch.nonzero(x)\n",
    "\t#print(non_zero_indices)\n",
    "\t# Get the non-zero values\n",
    "\tnon_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "\tunique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "\t#print(\"unique\" , unique)\n",
    "\t# Print the result\n",
    "\t#print(non_zero_values)\n",
    "\tnon_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "\t#print(\"split non_zero_values\" , non_zero_values)\n",
    "\treturn non_zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\layout\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "d:\\conda\\envs\\layout\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position torch.Size([1024, 1])\n",
      "div_term torch.Size([256])\n",
      "pe torch.Size([1024, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type               | Params\n",
      "------------------------------------------------------------\n",
      "0 | backbone             | Resnet             | 23.5 M\n",
      "1 | fixed_pe             | PositionalEncoding | 0     \n",
      "2 | transformer          | TransformerModel   | 7.9 M \n",
      "3 | reduce_height_module | GlobalHeightStage  | 45.5 M\n",
      "------------------------------------------------------------\n",
      "76.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "76.8 M    Total params\n",
      "153.698   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:229: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val!!!!!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TransformerModel' object has no attribute 'decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25376\\854319292.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;31m#print(o)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gpu'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mmin_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m51\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mfast_dev_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         call._call_and_handle_interrupt(\n\u001b[1;32m--> 609\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m         )\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         )\n\u001b[1;32m--> 650\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m         \u001b[1;31m# enable train mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1274\u001b[0m             \u001b[1;31m# run eval step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1275\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1276\u001b[1;33m                 \u001b[0mval_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_sanity_check_end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dataloader_idx\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mdl_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;31m# store batch level output per dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m# lightning module methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluation_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \"\"\"\n\u001b[0;32m    233\u001b[0m         \u001b[0mhook_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"test_step\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"validation_step\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1493\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1494\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m         \u001b[1;31m# restore current_fx when nested context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValidationStep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25376\\854319292.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, input_b, batch_idx)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;31m#out_box , out_cls   = self.forward(img)  # [ batch , top_k , 5]   , [ batch , top_k , 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25376\\854319292.py\u001b[0m in \u001b[0;36minf\u001b[1;34m(self, imgs)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout_box\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mout_cls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m         \u001b[0mout_box\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mout_cls\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [ batch , top_k , 5]   , [ batch , top_k , 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;31m#sampled_u_idx = torch.argwhere( torch.sigmoid(out_cls.view(batch_size , -1)) > 0.01 )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25376\\854319292.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0membedded_feat\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpe_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_pe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_feats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mout_box\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mout_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_feats\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpe_pattern\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mpe_pattern\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (b , 1024 , 256 )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout_box\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mout_cls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25376\\854319292.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, query, pos)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mb_pe\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# torch.Size([1024, b, 256])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mbox_logits\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcls_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbox_logits\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcls_logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\layout\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1268\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1270\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TransformerModel' object has no attribute 'decoder'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Any\n",
    "import pytorch_lightning as pl\n",
    "from config import *\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import MLP\n",
    "import math\n",
    "from torch import Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from VerticalCompressionNet import * \n",
    "from CustomTransformer import *\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: 256, dropout: float = 0.0, max_len: int = 1024):\n",
    "        super().__init__()        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)        \n",
    "        print(\"position\", position.shape)\n",
    "        div_term = torch.exp(torch.arange(0, d_model) * (-math.log(10000.0) / d_model))\n",
    "        #div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        print(\"div_term\", div_term.shape)\n",
    "        #pe = torch.zeros(max_len, 1, d_model)  # []\n",
    "        pe = torch.zeros(max_len, d_model)  # [ 1024 , 256 ]\n",
    "        print(\"pe\", pe.shape)\n",
    "        #pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        #pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        pe[: ,:] = torch.sin(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:    \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            #x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "            x: Tensor, shape ``[batch_size , seq_len , embedding_dim]``\n",
    "        \"\"\"\n",
    "        #x = x + self.pe[:x.size(0)]\n",
    "        # [batch size , 1024 , 256 ]\n",
    "        x = x + self.pe\n",
    "        return self.dropout(x) , self.pe\n",
    "        #return self.dropout(x)\n",
    "        #return self.pe[:0]\n",
    "\n",
    "class CustomDecoder(nn.Module):\n",
    "    def __init__(self,  d_model = 256 ,dropout = 0.1 , out_length = 20 , cls_num = 1  , batch_size = 2):\n",
    "        super().__init__()        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #self.conv1 = nn.Conv1d(d_model, d_model//2 , kernel_size=3 , padding=1)\n",
    "        self.conv1 = nn.Linear(d_model, d_model//2)\n",
    "        #self.conv2 = nn.Conv1d(d_model//2, d_model//4 , kernel_size=3 , padding=1)\n",
    "\n",
    "        self.cls_head = nn.Linear(d_model//2, 1 )\n",
    "        self.box_head = nn.Linear(d_model//2, 5 )\n",
    "        #self.maxpool = nn.MaxPool2d((1024,out_length))\n",
    "\n",
    "        #self.norm1 = nn.LayerNorm( ( batch_size  , 1024,d_model//2))\n",
    "        #self.norm2 = nn.LayerNorm(d_model//4)\n",
    "        #self.dropout1 = nn.Dropout(dropout)\n",
    "        #self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self , memory ):\n",
    "        '''\n",
    "        memory : encoder output , shape [1024 , b , hidden_dim ]\n",
    "        '''\n",
    "        # permute to (batch , channels , sequence length)\n",
    "        #memory = memory.permute(1 , 2 , 0)\n",
    "\n",
    "        x = self.conv1(memory)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        '''\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm2(x)\n",
    "        '''\n",
    "        #x = x.permute(0,2,1) # [batch , 1024 , hidden]\n",
    "        #x = self.norm1(x)\n",
    "\n",
    "        x = x.permute(1,0,2) # [batch , 1024 , hidden]\n",
    "        #print(\"before max pool\" , x.shape)\n",
    "        #x = self.maxpool(x)\n",
    "        #print(\"max pool\" , x.shape)\n",
    "\n",
    "        box_logits , cls_logits = self.box_head(x) , self.cls_head(x)         \n",
    "        print(\"cls_logits\",cls_logits)\n",
    "        return box_logits , cls_logits\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int , d_hid: int, nlayers: int, dropout: float = 0.1 , activation=\"relu\" , normalize_before=False):\n",
    "        super().__init__()\n",
    "        #self.ntoken = ntoken\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, d_hid,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
    "        self.encoder = TransformerEncoder(encoder_layer, nlayers, encoder_norm)\n",
    "        #self.decoder = CustomDecoder(d_model=d_model , dropout= dropout )\n",
    "\n",
    "        '''\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, d_hid, dropout, activation, normalize_before)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, nlayers, decoder_norm,\n",
    "                                          return_intermediate=False)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_hid, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "        '''\n",
    "    \n",
    "        \n",
    "    def forward(self, src: Tensor, query , pos ) -> Tensor:\n",
    "        # permute to (Sequence_length , Batches , Hidden layer)\n",
    "        src         = src.permute(1 , 0 , 2)# torch.Size([1024, b, 256])        \n",
    "        batch_size  = src.shape[1]\n",
    "        b_pe        = pos.unsqueeze(1).repeat(1,batch_size , 1)  # torch.Size([1024, b, 256])\n",
    "        #b_query        = query.unsqueeze(1).repeat(1,batch_size , 1)  # torch.Size([1024, b, 256])\n",
    "        \n",
    "        memory = self.encoder(src , pos= b_pe)  # torch.Size([1024, b, 256])\n",
    "        box_logits , cls_logits = self.decoder(memory)\n",
    "\n",
    "        return box_logits , cls_logits\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class VerticalQueryTransformer(pl.LightningModule):    \n",
    "    def __init__(self  ,  max_predict_count = 24 , hidden_out = 128 , class_num = 1 , log_folder = \"__test\" , num_classes = 1 , backbone_trainable =False, load_weight =\"\" , top_k = 20):\n",
    "        #print(\" input_size\" ,  input_size)\n",
    "        super().__init__()\n",
    "        self.backbone = Resnet()\n",
    "        self.out_scale = 8\n",
    "        self.step_cols = 4        \n",
    "        self.hidden_size = hidden_out\n",
    "        self.max_predict_count = max_predict_count\n",
    "        self.num_classes  = num_classes \n",
    "        self.top_k_num = top_k        \n",
    "\n",
    "        self.fixed_pe = PositionalEncoding(hidden_out, 0.1 , 1024)\n",
    "\n",
    "        self.transformer = TransformerModel( d_model=hidden_out , nhead=8 , d_hid= 2048,nlayers=6 )\n",
    "        #self.transformer = nn.TransformerEncoderLayer(hidden_out ,  8 , 2048 , dropout= 0.1) \n",
    "\n",
    "        #self.box_head= nn.Linear( hidden_out , 6 )        \n",
    "        #self.vqt_box_head= nn.Linear( hidden_out , 5 )        \n",
    "        #self.vqt_cls_head= nn.Linear( hidden_out , class_num )        \n",
    "        self.confidence_threshold = 0.85\n",
    "\n",
    "        # loss\n",
    "        self.box_cost = 1\n",
    "        self.cls_cost = 5\n",
    "\n",
    "        self.log_folder = create_folder(os.path.join(os.getcwd() , \"output\" , log_folder))\n",
    "        #self.box_head.bias.data = torch.nn.Parameter(torch.tensor([0.3,0.2,0.2,0.3]))\n",
    "        #self.box_head.weight.data.fill_(0)\n",
    "        \n",
    "        # Inference channels number from each block of the encoder\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 320, 190)\n",
    "            c1, c2, c3, c4 = [b.shape[1] for b in self.backbone(dummy)] # resnet feature channel數\n",
    "            #print(\"c1, c2, c3, c4\" , c1, c2, c3, c4)\n",
    "            c_last = (c1*8 + c2*4 + c3*2 + c4*1) // self.out_scale            \n",
    "        #self.v_reproj = nn.Conv2d(1024 , self.max_predict_count,kernel_size=1)\n",
    "            \n",
    "        self.reduce_height_module = GlobalHeightStage(c1, c2, c3, c4 , out_scale=self.out_scale , pretrain_weight= load_weight , freeze_model= not backbone_trainable)\n",
    "\n",
    "    def post_process(self ,box_coord , index , max_length = 1024):\n",
    "        '''\n",
    "        box_coord : contains left top v ,left btm v , du , right top v , right btm v\n",
    "        '''\n",
    "        origin_shape = box_coord.shape\n",
    "        box_coord = box_coord.view(-1 , 5)\n",
    "        \n",
    "        us = (index/max_length).view((box_coord.shape[0] , 1))\n",
    "        uvv_uvv_b = torch.zeros((box_coord.shape[0] , 6) , device=box_coord.device)\n",
    "        uvv_uvv_b[:,0] = us[:,0]\n",
    "        uvv_uvv_b[:,1] = box_coord[:,0] # left top v\n",
    "        uvv_uvv_b[:,2] = box_coord[:,1] # left btm v\n",
    "        uvv_uvv_b[:,3] = us[:,0] + box_coord[:,2] # right u\n",
    "        uvv_uvv_b[:,4] = box_coord[:,3] # right top v\n",
    "        uvv_uvv_b[:,5] = box_coord[:,4] # right btm v\n",
    "\n",
    "        #return uvv_uvv_b.view(origin_shape[0] , origin_shape[1] , 6)\n",
    "        return uvv_uvv_b.view(origin_shape[0] , -1 , 6)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self ,x ):\n",
    "        features = self.backbone(x) # [4 , c , h, w]      \n",
    "        # vertical feature\n",
    "        reduced_feats  = self.reduce_height_module(features , x.shape[3]//self.step_cols ) # [b , 1024 ,  256] width = 1024 , 256d latent code each.\n",
    "\n",
    "        # Add fixed PE\n",
    "        embedded_feat , pe_pattern = self.fixed_pe(reduced_feats)\n",
    "\n",
    "        out_box , out_cls = self.transformer(reduced_feats , pe_pattern ,pe_pattern)  # (b , 1024 , 256 )       \n",
    "\n",
    "        return out_box , out_cls\n",
    "    def inf(self , imgs ):\n",
    "        out_box , out_cls   = self.forward(imgs)  # [ batch , top_k , 5]   , [ batch , top_k , 1]         \n",
    "        batch_size = out_box.shape[0]\n",
    "        #sampled_u_idx = torch.argwhere( torch.sigmoid(out_cls.view(batch_size , -1)) > 0.01 )\n",
    "        #print(\"sampled_u_idx\" , sampled_u_idx)\n",
    "\n",
    "        sampled_box_b = []\n",
    "        #each batch\n",
    "        for img , pbox , pcls in zip(imgs, out_box , out_cls.view(batch_size,-1)):  \n",
    "            u_id = torch.argwhere(torch.sigmoid(pcls) > self.confidence_threshold)\n",
    "            #u_id = torch.argwhere(torch.sigmoid(pcls) > 0.34)\n",
    "            #print(\"max sigmoid\" , torch.max(torch.sigmoid(pcls)))\n",
    "            print(\"u_id\" , u_id.shape)\n",
    "            #print(\"pbox\" , pbox.shape)\n",
    "            if(u_id.numel() ==0):\n",
    "                continue\n",
    "            u_id = u_id.view(-1)            \n",
    "            \n",
    "            pred = self.post_process(pbox[u_id,:] , u_id ).view(-1,6)\n",
    "            \n",
    "            save_folder = create_folder( os.path.join(self.log_folder ,\"val\"))\n",
    "            save_path = os.path.join(save_folder, f\"val_ep_{self.current_epoch}-{self.global_step}\" )\n",
    "            pred_us , pred_tops , pred_btms = self.pack_visualize(pred[:,0], pred[:,1],pred[:,2],pred[:,3] -pred[:,0] ,pred[:,4],pred[:,5] )                    \n",
    "            vis_imgs = visualize_2d_single(pred_us , pred_tops , pred_btms , u_grad = F.sigmoid(pcls).view(1 , -1 ) , imgs=  img , title=\"Pred\" , save_path= save_path  )\n",
    "            #plt.imshow(vis_imgs)\n",
    "            #plt.show()\n",
    "\n",
    "            # ToDo: calculate loss\n",
    "            \n",
    "            \n",
    "        \n",
    "        pass\n",
    "    #@torch.no_grad()\n",
    "    def find_match(self, gt , pred):\n",
    "        #print(\"gt\"  , gt)\n",
    "        gt_vec = torch.stack(gt).permute(1,0)\n",
    "        pred_vec = torch.stack(pred).permute(1,0)\n",
    "        loss_dist = torch.cdist(gt_vec , pred_vec)\n",
    "        \n",
    "        each_gt_pred_best_idx = torch.argmax(loss_dist, 0 )      \n",
    "        #print(\"each_gt_pred_best_idx\" , each_gt_pred_best_idx)          \n",
    "\n",
    "        return gt_vec[each_gt_pred_best_idx] , pred_vec\n",
    "        \n",
    "    def pack_visualize(self, gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , dv_btm_b ):\n",
    "        \n",
    "        if isinstance(gt_u_b, torch.Tensor):\n",
    "            sizes = [t.numel() for t in gt_u_b]               \n",
    "            us = gt_u_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            us[1::2]+=gt_du_b.flatten()\n",
    "            us = torch.split(us.view(-1,2) , sizes)\n",
    "\n",
    "            tops = gt_vtop_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            tops[1::2]=gt_dvtop_b.flatten()\n",
    "            tops = torch.split(tops.view(-1,2) , sizes)\n",
    "\n",
    "            btms = gt_vbtm_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            btms[1::2]=dv_btm_b.flatten()\n",
    "            btms = torch.split(btms.view(-1,2) , sizes)\n",
    "\n",
    "        elif isinstance(gt_u_b, tuple) and all(isinstance(t, torch.Tensor) for t in gt_u_b):        \n",
    "            sizes = [len(t) for t in gt_u_b]               \n",
    "            us = torch.cat(gt_u_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            us[1::2]+=torch.cat(gt_du_b).view(-1)\n",
    "            us = torch.split(us.view(-1,2) , sizes)\n",
    "\n",
    "            tops = torch.cat(gt_vtop_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            tops[1::2]=torch.cat(gt_dvtop_b).view(-1)\n",
    "            tops = torch.split(tops.view(-1,2) , sizes)\n",
    "\n",
    "            btms = torch.cat(gt_vbtm_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            btms[1::2]=torch.cat(dv_btm_b).view(-1)\n",
    "            btms = torch.split(btms.view(-1,2) , sizes)\n",
    "        else:\n",
    "            assert(\"Wrong Type.\")\n",
    "        \n",
    "        return us , tops ,btms\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def training_step(self , input_b ,batch_idx , optimizer_idx):\n",
    "        \n",
    "        img = input_b['image']\n",
    "        #h,w = img.shape[1:3]\n",
    "        out_box , out_cls   = self.forward(img)  # [ batch , top_k , 5]   , [ batch , top_k , 1] \n",
    "        print(\"max out_cls\" , torch.max(out_cls) , \"min \" , torch.min(out_cls))\n",
    "        batch_size = out_box.shape[0]\n",
    "        '''\n",
    "        if self.current_epoch % 5 == 0 and self.current_epoch > 0 :       \n",
    "            plt.imshow(out_cls[0].repeat(1,100).detach().cpu().numpy())\n",
    "            plt.title(\"encoder cls output\")\n",
    "            plt.show()      \n",
    "        '''\n",
    "\n",
    "        # Select top k        \n",
    "        top_k = torch.topk(out_cls ,self.top_k_num , dim= 1 )\n",
    "        top_k_idx = top_k[1].view(batch_size  , self.top_k_num)  # [b , top_k ]        \n",
    "        #print(\"top_k_idx\" , top_k_idx.shape , top_k_idx)\n",
    "\n",
    "        topk_box = out_box.gather(1 , top_k_idx.unsqueeze(-1).repeat(1,1,5))\n",
    "        topk_cls = out_cls.gather(1 , top_k_idx.unsqueeze(-1).repeat(1,1,1))\n",
    "        print(\"topk_cls\" , topk_cls.shape)\n",
    "        \n",
    "        pred_uvvboxes_b = self.post_process(topk_box , top_k_idx )\n",
    "        #print(\"pred_uvvboxes_b\" , pred_uvvboxes_b.shape ,pred_uvvboxes_b)\n",
    "\n",
    "        # remove padding , each batch have different length\n",
    "        gt_u_b = unpad_data( input_b['u'])          \n",
    "        gt_vtop_b =unpad_data(input_b['v_top'])\n",
    "        gt_vbtm_b = unpad_data (input_b['v_btm'])\n",
    "        gt_du_b = unpad_data(input_b['du'])\n",
    "        gt_dvtop_b = unpad_data(input_b['dv_top'])\n",
    "        gt_dv_btm_b = unpad_data(input_b['dv_btm'])\n",
    "\n",
    "        #selected_gt_u_grad =  input_b['u_grad'].view(batch_size , 1024 , 1).gather(1 , top_k_idx.unsqueeze(-1).repeat(1,1,1)).view(batch_size , self.top_k_num)        \n",
    "\n",
    "        total_loss = 0\n",
    "        b_cnt = 0\n",
    "        \n",
    "        for u,vtop,vbtm,du,dvtop, dvbtm , pred ,cls_b,gt_cls in zip(gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , gt_dv_btm_b , pred_uvvboxes_b , topk_cls , input_b['u_grad']):\n",
    "        #for u,vtop,vbtm,du,dvtop, dvbtm , pred ,cls_b,gt_cls , k_idx in zip(gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , gt_dv_btm_b , out_box , out_cls ,selected_gt_u_grad , top_k_idx):\n",
    "            \n",
    "            # match            \n",
    "            #gt_box =  torch.vstack([vtop,vbtm,du ,dvtop , dvbtm]).permute(1,0)\n",
    "            gt_box =  torch.vstack([ u, vtop,vbtm, u + du ,dvtop , dvbtm]).permute(1,0)\n",
    "\n",
    "            box_loss = torch.cdist( pred , gt_box , p=1)\n",
    "            cls_loss = - F.softmax( cls_b , -1)\n",
    "            #cls_loss =  -F.binary_cross_entropy_with_logits(cls_b.view(-1) , gt_cls.view(-1))\n",
    "\n",
    "            #cost_matrix = box_loss * self.box_cost + cls_loss             \n",
    "            ###\n",
    "            ### TODO: 檢查cls 跟GT是不是對的\n",
    "            ### TODO: 增加cls 訓練機會\n",
    "            ### TODO: 製作inf (testing)測試\n",
    "            ###\n",
    "            cost_matrix = box_loss * self.box_cost  + cls_loss \n",
    "            '''\n",
    "            cost_matrix =  torch.cdist( F.sigmoid(cls_b).view(-1,1) , gt_u_b.view(-1,1))            \n",
    "            '''\n",
    "            cost_matrix = cost_matrix.detach().cpu().numpy()            \n",
    "            row_idx  , col_idx = linear_sum_assignment(cost_matrix)                      \n",
    "\n",
    "            print(\"row_idx  \" , row_idx  )\n",
    "            print(\"col_idx  \" , col_idx  )\n",
    "            #print(\"matched pred\" , pred[row_idx] )\n",
    "            print(\"pred cls\" , cls_b)\n",
    "            #print(\"matched gt \" , gt_box[col_idx] )\n",
    "            print(\"gt cls\" , gt_cls.shape , gt_cls[row_idx].view(-1))\n",
    "            #print(\"gt cls\" , gt_binary_cls.view(-1))\n",
    "            #print(\"gt cls\" , gt_binary_cls)\n",
    "            \n",
    "            #row_idx = torch.round(u* self.max_predict_count ).to(torch.long).detach().cpu().numpy()           \n",
    "            #col_idx = torch.arange(u.shape[0]).detach().cpu().numpy()           \n",
    "\n",
    "            #matched_cls_loss = F.binary_cross_entropy_with_logits(cls_b[row_idx].view(-1) , gt_cls[col_idx])            \n",
    "            #total_loss += F.l1_loss(pred[row_idx] ,  gt_box[col_idx]) + F.binary_cross_entropy_with_logits(cls_b.view(-1), gt_cls  ) + matched_cls_loss\n",
    "            #total_loss += F.l1_loss(pred[row_idx] ,  gt_box[col_idx]) + F.binary_cross_entropy_with_logits(cls_b.view(-1), gt_cls  ) + matched_cls_loss\n",
    "\n",
    "            l1_loss = F.l1_loss(pred[row_idx] ,  gt_box[col_idx]) \n",
    "            cls_loss = F.binary_cross_entropy_with_logits(cls_b[row_idx].view(-1), gt_cls[row_idx].view(-1)) \n",
    "            print(\"L1 loss\"  , l1_loss)\n",
    "            print(\"cls_loss\"  , cls_loss)\n",
    "            \n",
    "            total_loss += l1_loss+ cls_loss\n",
    "            #total_loss += cls_loss\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                #if self.current_epoch % 5 == 0  :                \n",
    "                if self.current_epoch % 5 == 0 and self.current_epoch > 0 :                \n",
    "                    save_path =  os.path.join(self.log_folder , f\"gt_ep_{self.current_epoch}-{self.global_step}-{b_cnt}\" )\n",
    "                    gt_us , gt_tops , gt_btms = self.pack_visualize(u.view(1 , -1 ) , vtop , vbtm , du , dvtop , dvbtm )\n",
    "                    #print(\"gt_us , gt_tops , gt_btms\" , gt_us , gt_tops , gt_btms)\n",
    "                    vis_imgs = visualize_2d_single(gt_us , gt_tops , gt_btms , u_grad =  gt_cls.view(1 , -1 ), imgs= img[b_cnt] , title=\"GT\",save_path=save_path )                \n",
    "                    \n",
    "                    save_path =  os.path.join(self.log_folder , f\"pred_ep_{self.current_epoch}-{self.global_step}-{b_cnt}\" )\n",
    "                    \n",
    "                    print(\"pred[row_idx,0]\" , pred[row_idx,0])\n",
    "                    pred_us , pred_tops , pred_btms = self.pack_visualize(pred[row_idx,0], pred[row_idx,1],pred[row_idx,2],pred[row_idx,3] -pred[row_idx,0] ,pred[row_idx,4],pred[row_idx,5] )\n",
    "                    \n",
    "                    vis_imgs = visualize_2d_single(pred_us , pred_tops , pred_btms , u_grad = F.sigmoid(cls_b).view(1 , -1 ) , imgs=  img[b_cnt] , title=\"Pred\" , save_path= save_path  )\n",
    "                    \n",
    "           \n",
    "            b_cnt+=1\n",
    "            pass        \n",
    "        return total_loss / batch_size\n",
    "        pass    \n",
    "\n",
    "    def validation_step(self, input_b, batch_idx):\n",
    "        print(\"val!!!!!\")\n",
    "        img = input_b['image']\n",
    "        \n",
    "        #out_box , out_cls   = self.forward(img)  # [ batch , top_k , 5]   , [ batch , top_k , 1]         \n",
    "        self.inf(img)\n",
    "        return\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        backbone_opt = optim.Adam(self.backbone.parameters() , lr=0.00035)\n",
    "        transforms_opt = optim.Adam(self.transformer.parameters() , lr=0.000035)\n",
    "\n",
    "        return [backbone_opt , transforms_opt] , []\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "# Unit testing...\n",
    "\n",
    "# Test\n",
    "dm = CustomDataModule ( train_dir= f\"../anno/test_visiable_10_no_cross.json\" ,\n",
    "                        test_dir= f\"../anno/test_visiable_10_no_cross.json\" , padding_count=100 , use_aug=False , c= 0.95\n",
    "                       )\n",
    "m = VerticalQueryTransformer(max_predict_count = 256 , hidden_out=256 , load_weight=\"D:/OneDrive/OneDrive - NTHU/Layout/Horizon/0912_all_bk.pth\"  , backbone_trainable=True, top_k=100)\n",
    "#img = torch.randn((3,3,1024,512))\n",
    "#o = m(img)\n",
    "\n",
    "#print(o)\n",
    "trainer = pl.Trainer(accelerator='gpu' , devices=1 ,min_epochs=1, max_epochs=51 , precision=16 , fast_dev_run=False )\n",
    "trainer.fit(m , dm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "torch.Size([24, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,5,5)\n",
    "b = torch.argwhere(a>0.5)\n",
    "bl = [ c.shape[0] for c in b]\n",
    "print(bl)\n",
    "\n",
    "print(b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "horizon_path =r\"D:/OneDrive/OneDrive - NTHU/Layout/Horizon/0912_all_bk.pth\"\n",
    "#models_dict = torch.load_s\n",
    "checkpoint = torch.load(horizon_path ,  map_location=\"cpu\")\n",
    "print(checkpoint['state_dict'].keys())\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in m.state_dict()}\n",
    "m.load_state_dict(pretrained_dict , strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.rand(2,256 ,1024)\n",
    "b = nn.Conv1d(256 , 64 , kernel_size=3 ,padding=1)\n",
    "c = b(a)\n",
    "print(c.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,5,2)\n",
    "print(a)\n",
    "b=  nn.MaxPool2d((5,1))\n",
    "c = b(a)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(200).view(2,100,1)\n",
    "#aa = a[:,:,0].unsqueeze(0)\n",
    "#print(aa.shape)\n",
    "b = F.interpolate(a.view(2,-1).unsqueeze(0), 10 )[0]\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,10,1)\n",
    "b = torch.cat([a,a] , dim=1)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = torch.rand(2,100,1)\n",
    "a = torch.arange(400).view(2,100,2)\n",
    "b = torch.arange(20).view(2,10)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "b= b.unsqueeze(-1).repeat(1,1,2)\n",
    "print(b)\n",
    "#print(\"b unsqueeze\",b.unsqueeze(-1))\n",
    "\n",
    "c = a.gather(1, b)\n",
    "print(c.shape)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.arange(5)\n",
    "b=torch.arange(5)\n",
    "c=torch.arange(5)\n",
    "\n",
    "d = torch.vstack([a,b,c]).permute(1,0)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "a = torch.tensor([ [0,1,2]  ,  [0,3,5] , [1,0,5] ]).to(torch.float32)\n",
    "b = torch.tensor([ [0,1,2] , [1,0,5] ]).to(torch.float32)\n",
    "\n",
    "cost = torch.cdist(b,a)\n",
    "print(cost)\n",
    "row , col = linear_sum_assignment(cost,)\n",
    "print(row)\n",
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.7605, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7730, 0.5752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7057, 0.5861, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8386, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8304, 0.7823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7034, 0.5994, 0.5691, 0.5652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.6996, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8305, 0.7819, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8238, 0.7839, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "# Get the indices of non-zero elements\n",
    "non_zero_indices = torch.nonzero(x)\n",
    "print(non_zero_indices)\n",
    "# Get the non-zero values\n",
    "non_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "unique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "print(\"unique\" , unique)\n",
    "# Print the result\n",
    "print(non_zero_values)\n",
    "non_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "print(\"split non_zero_values\" , non_zero_values)\n",
    "\n",
    "def unpad_data( x :[Tensor] ) :\n",
    "\tnon_zero_indices = torch.nonzero(x)\n",
    "\tprint(non_zero_indices)\n",
    "\t# Get the non-zero values\n",
    "\tnon_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "\tunique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "\tprint(\"unique\" , unique)\n",
    "\t# Print the result\n",
    "\tprint(non_zero_values)\n",
    "\tnon_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "\tprint(\"split non_zero_values\" , non_zero_values)\n",
    "\treturn non_zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.58 , 0.6] , [0.4] ] , )\n",
    "b = torch.tensor([0.1 , 0.2] , )\n",
    "\n",
    "c = a.repeat(2)\n",
    "print(a.repeat(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.rand(1)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Layout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
