{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import json\n",
    "from CustomDataset import * \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from config import *\n",
    "from file_helper import *\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "#=================================\n",
    "#             Augmentation\n",
    "#=================================\n",
    "\n",
    "def gauss_noise_tensor(img):\n",
    "    rand = torch.rand(1)[0]\n",
    "    if rand < 0.5 and Horizon_AUG:\n",
    "        sigma = rand *0.125\n",
    "        out = img + sigma * torch.randn_like(img)\n",
    "        return out\n",
    "    return img\n",
    "\n",
    "def blank(img):    \n",
    "    return img\n",
    "\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self ,\n",
    "                 train_dir ,\n",
    "                 test_dir , batch_size = 2, num_workers = 0 , img_size=[IMG_WIDTH, IMG_HEIGHT] , use_aug = True ,padding_count = 24 ,c =0.1\n",
    "                   ):\n",
    "        super().__init__()\n",
    "        self.train_dir = train_dir\n",
    "        self.test_dir = test_dir\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_size = img_size      \n",
    "        self.use_aug = use_aug\n",
    "        self.padding_count  = padding_count\n",
    "        self.c = c\n",
    "        \n",
    "\n",
    "        pass\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download dataset\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # Create dataset...          \n",
    "                \n",
    "        self.entire_dataset = CustomDataset(self.train_dir  , use_aug= self.use_aug , padding_count= self.padding_count , c=self.c , img_size=self.img_size)\n",
    "        self.train_ds , self.val_ds = random_split(self.entire_dataset , [0.9, 0.1])        \n",
    "        self.test_ds = CustomDataset(self.test_dir  , use_aug= False , img_size=self.img_size)\n",
    "        \n",
    "        print(\"image size \",self.img_size)\n",
    "        pass\n",
    "\n",
    "    # ToDo: Reture Dataloader...\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_ds , batch_size= self.batch_size , num_workers= self.num_workers , shuffle=False)\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "dm = CustomDataModule ( train_dir= f\"../anno/test_visiable_10_no_cross.json\" ,\n",
    "                       test_dir= f\"../anno/test_visiable_10_no_cross.json\" , padding_count=256, img_size=[512,256]\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "def unpad_data( x :[Tensor] ) :\n",
    "\tnon_zero_indices = torch.nonzero(x)\n",
    "\t#print(non_zero_indices)\n",
    "\t# Get the non-zero values\n",
    "\tnon_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "\tunique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\t\n",
    "\tnon_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "\t\n",
    "\treturn non_zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Any\n",
    "import pytorch_lightning as pl\n",
    "from config import *\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import MLP\n",
    "import math\n",
    "from torch import Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from VerticalCompressionNet import * \n",
    "from CustomTransformer import *\n",
    "\n",
    "def encode_target(box_b , base_u):\n",
    "    box_b[:, 1] = (0.5 -box_b[:, 1])  # v top\n",
    "    box_b[:, 2] = (box_b[:, 2] -0.5) # v btm\n",
    "    #box_b[:, 3] = ( box_b[:, 3] + base_u) # du\n",
    "    box_b[:, 3] = box_b[:, 3] # du\n",
    "\n",
    "    box_b[:, 4] = (0.5 -box_b[:, 4])  # v top\n",
    "    box_b[:, 5] = (box_b[:, 5] -0.5) # v btm\n",
    "    box_b[:, 0] = (base_u - box_b[:, 0])  # u\n",
    "    '''\n",
    "    box_b[:, 1] = torch.exp(torch.abs(0.5 -box_b[:, 1]))  # v top\n",
    "    box_b[:, 2] = torch.exp(torch.abs(box_b[:, 2] -0.5)) # v btm\n",
    "    box_b[:, 3] = torch.exp(torch.abs(box_b[:, 3])) # du\n",
    "\n",
    "    box_b[:, 4] = torch.exp(torch.abs(0.5 -box_b[:, 4]))  # v top\n",
    "    box_b[:, 5] = torch.exp(torch.abs(box_b[:, 4] -0.5)) # v btm\n",
    "    box_b[:, 0] = torch.exp( box_b[:, 0] )  # u\n",
    "    '''\n",
    "\n",
    "    return box_b\n",
    "def decode_target(box_b , base_u):    \n",
    "    box_b[:, 0] = base_u - box_b[:, 0]  # u\n",
    "    box_b[:, 1] = 0.5 - box_b[:, 1]  # v top\n",
    "    box_b[:, 2] = box_b[:, 2] +0.5 # v btm\n",
    "    box_b[:, 3] = base_u + box_b[:, 3]  # du\n",
    "\n",
    "    box_b[:, 4] = 0.5 -box_b[:, 4]  # v top    \n",
    "    box_b[:, 5] = box_b[:, 5] +0.5 # v btm\n",
    "    '''\n",
    "    box_b[:, 0] = torch.log( box_b[:, 0] )  # u\n",
    "    box_b[:, 1] = 0.5 - torch.log(box_b[:, 1])  # v top\n",
    "    box_b[:, 2] = torch.log(box_b[:, 2]) +0.5 # v btm\n",
    "    box_b[:, 3] = torch.log(box_b[:, 3]) + box_b[:, 0] # du\n",
    "\n",
    "    box_b[:, 4] = 0.5 - torch.log(box_b[:, 4])  # v top    \n",
    "    box_b[:, 5] = torch.log(box_b[:, 4]) +0.5 # v btm\n",
    "    '''\n",
    "    return box_b\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: 256, dropout: float = 0.1, max_len: int = 1024):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model: 256, nhead: int , d_hid: int, nlayers: int, dropout: float = 0.1 , activation=\"relu\" ,\n",
    "                  normalize_before=False , out_dim=20 ,channel = 1024):\n",
    "        super().__init__()\n",
    "        #self.ntoken = ntoken\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.out_dim =out_dim\n",
    "        \n",
    "        '''\n",
    "        #self.query_embed = nn.Embedding(out_dim, d_model)\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(1024, nhead, d_hid,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        self.pe = PositionalEncoding(1024 ,dropout , max_len= d_model)\n",
    "        encoder_norm = nn.LayerNorm(1024) if normalize_before else None\n",
    "        self.encoder = TransformerEncoder(encoder_layer, nlayers, encoder_norm)\n",
    "       \n",
    "        self.decoder = nn.Linear(out_dim, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=256, nhead=8  )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "\n",
    "        self.decoder1 = nn.Conv1d(d_model , d_model//2 , kernel_size=3 , padding=1)\n",
    "        self.decoder2 = nn.Conv1d(d_model//2 , d_model//4 , kernel_size=1)\n",
    "        self.decoder3 = nn.Conv1d(d_model//4, self.out_dim , kernel_size=1)\n",
    "        #self.mlp = torchvision.ops.mlp(,)\n",
    "        '''\n",
    "        self.enc_proj = nn.Linear(channel , self.out_dim * 4)\n",
    "        \n",
    "        self.cls_head = nn.Linear(channel, 1 )\n",
    "        self.u_head = nn.Linear(channel, 2 )\n",
    "        self.v_head = nn.Linear(channel, 4 )\n",
    "\n",
    "        self.u_head.bias.data.fill_(self.out_dim /100*0.5)\n",
    "        self.v_head.bias.data.fill_(0.15)\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self, src: Tensor ) -> Tensor:\n",
    "        # permute to (Sequence_length , Batches , Hidden layer)\n",
    "        '''\n",
    "        plt.imshow(src[0].detach().cpu().numpy())\n",
    "        plt.title(\"src\")\n",
    "        plt.show()\n",
    "        '''\n",
    "        src         = src.permute(2,0,1)   #  [w, b, c*h]  , example: [256 , 5 , 1024]        \n",
    "        #src         = src.permute(1 , 0 , 2)# torch.Size([1024, b, 256])        \n",
    "        batch_size  = src.shape[1]   \n",
    "        '''\n",
    "        src_pe         = self.pe(src)   # [ 256 , b , hidden_dim]\n",
    "        #print(\"src_pe\",src_pe.shape)\n",
    "        src_pe         = self.encoder(src_pe) # [ 256 , b , hidden_dim]\n",
    "        '''     \n",
    "        src_pe         = self.enc_proj(src) # [ 256 , b , max count * 4\n",
    "        src_pe         = src_pe.view(src_pe.shape[0], src_pe.shape[1], self.out_dim , 4) # [ 256 , b , max count * 4\n",
    "        #src_pe         = src_pe.permute(1,0,2)  # [b , width , 1024]\n",
    "        out = src_pe.permute(1 , 2 , 0, 3)  # [b, max_count , seq_len, step_cols]\n",
    "        out = out.contiguous().view(out.shape[0] , self.out_dim , -1) \n",
    "\n",
    "        #print(src_pe.shape)\n",
    "        #plt.imshow(src_pe[0].detach().cpu().numpy())\n",
    "        #plt.title(\"encoder output\")\n",
    "        #plt.show()\n",
    "        #print(\"self.query_embed.weight \" , self.query_embed.weight .shape)\n",
    "        #out = self.decoder( self.query_embed.weight , src_pe)\n",
    "        '''\n",
    "        out = self.decoder1(src_pe)        \n",
    "        out = torch.relu(out)\n",
    "        out = self.decoder2(out)        \n",
    "        out = torch.relu(out)\n",
    "        out = self.decoder3(out)        \n",
    "        out = torch.relu(out)\n",
    "        '''\n",
    "        #print(\"out\",src_pe.shape)        \n",
    "       \n",
    "        box_u_logits = self.u_head(out)\n",
    "        box_v_logits = self.v_head(out)\n",
    "        cls_logits = self.cls_head(out)\n",
    "        \n",
    "        #print(\"box_v_logits\" , box_v_logits.shape)\n",
    "        box_logits = torch.cat([ box_u_logits[:,:,0].unsqueeze(2) ,\n",
    "                                 box_v_logits[:,:,0].unsqueeze(2) ,\n",
    "                                 box_v_logits[:,:,1].unsqueeze(2) , \n",
    "                                 box_u_logits[:,:,1].unsqueeze(2) ,\n",
    "                                 box_v_logits[:,:,2].unsqueeze(2) ,\n",
    "                                 box_v_logits[:,:,3].unsqueeze(2)] , dim=-1 )\n",
    "        #print(\"box_logits\" , box_logits.shape)\n",
    "        return box_logits ,cls_logits\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class VerticalQueryTransformer(pl.LightningModule):    \n",
    "    def __init__(self  ,  max_predict_count = 24 ,\n",
    "                    hidden_out = 128 , class_num = 1 ,\n",
    "                    log_folder = \"__test\" , num_classes = 1 , backbone_trainable =False, load_weight =\"\"  , dropout = 0.01 , normalize_before=False\n",
    "                    ,stride = 3,\n",
    "                    img_size = [1024,512]):\n",
    "        #print(\" input_size\" ,  input_size)\n",
    "        super().__init__()\n",
    "        self.confidence_threshold = 0.75\n",
    "        self.log_folder = create_folder(os.path.join(os.getcwd() , \"output\" , log_folder))\n",
    "        self.automatic_optimization = False\n",
    "        self.hidden_size = hidden_out\n",
    "        self.max_predict_count = max_predict_count\n",
    "        self.num_classes  = num_classes \n",
    "\n",
    "        self.input_width = img_size[0]\n",
    "        self.input_height = img_size[1]\n",
    "        self.stride=stride\n",
    "        #self.patch_out =  self.input_width//self.stride\n",
    "        self.patch_out =  8  # temp\n",
    "        self.bk = Resnet()\n",
    "        \n",
    "\n",
    "        #self.pixel_value_proj = nn.Linear( 3*self.input_height , self.hidden_size )\n",
    "\n",
    "        '''\n",
    "        self.pe = PositionalEncoding(self.hidden_size ,dropout , max_len=  self.input_width)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=8, dropout=dropout, activation='relu', batch_first=True, norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "        #decoder_layer = nn.TransformerDecoderLayer(d_model=self.hidden_size, nhead=8  )\n",
    "        #self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "\n",
    "\n",
    "        #self.backbone = Resnet()\n",
    "\n",
    "        #self.pos_emb = nn.Embedding(self.max_predict_count , self.hidden_size)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(size=(1,  self.patch_out, hidden_out)), requires_grad=True)\n",
    "        #nn.init.constant_(self.pos_emb.weight , 0)\n",
    "        #self.pixel_query = nn.Linear(self.input_width , self.hidden_size , kernel_size=3 , padding=1)\n",
    "        #self.pixel_query = nn.Linear(self.hidden_size , self.hidden_size )\n",
    "        self.pixel_value_proj =nn.Conv2d(\n",
    "                in_channels= 3 ,\n",
    "                out_channels=hidden_out,\n",
    "                kernel_size= (self.input_height , stride),\n",
    "                stride= (self.input_height , stride),\n",
    "            )\n",
    "        '''\n",
    "        self.mlp = MLP(hidden_out , [hidden_out//2 , hidden_out//4 ,hidden_out//8 , hidden_out//16 ])\n",
    "\n",
    "        self.cls_head = nn.Linear(self.hidden_size//16 , 1 )\n",
    "        self.u_head = nn.Linear(self.hidden_size//16, 2 )\n",
    "        self.v_head = nn.Linear(self.hidden_size//16, 4 )\n",
    "\n",
    "        self.u_head.bias.data.fill_(1 /self.patch_out*0.5)\n",
    "        self.v_head.bias.data.fill_(0.25)\n",
    "\n",
    "    def forward(self ,x ):\n",
    "        #x = self.backbone(x)[-1] \n",
    "        #x = x.permute(0,3,1,2)  # [ batch , width , channel , height]\n",
    "        #x = x.view(x.shape[0] , x.shape[1] , -1)  # [ batch , width , channel * height]        \n",
    "        '''\n",
    "        pixel_feat = self.pixel_value_proj(x)  # [ batch , hidden , height(=1) , patches ]\n",
    "        pixel_feat = pixel_feat.flatten(2).permute(0,2,1)   # [ batch , patches ,hidden ]\n",
    "        \n",
    "        src_pe =  pixel_feat + self.pos_emb  # [ batch , patches ,hidden ]\n",
    "        src_pe =  self.pe(src_pe)\n",
    "        \n",
    "        \n",
    "        enc_out = self.encoder(src_pe)        \n",
    "        '''\n",
    "        enc_out = self.bk(x)[-1]  # [b , 2048 , h , w]\n",
    "        enc_out = enc_out.permute( 0,3,1,2)\n",
    "        enc_out = enc_out.view(enc_out.shape[0] , enc_out.shape[1] , -1) # [b , w , h *2048]\n",
    "        \n",
    "        \n",
    "        \n",
    "        enc_out = self.mlp(enc_out)\n",
    "        box_u_logits = self.u_head(enc_out)\n",
    "        box_v_logits = self.v_head(enc_out)\n",
    "        cls_logits = self.cls_head(enc_out)\n",
    "        \n",
    "        box_logits = torch.cat([ box_u_logits[:,:,0].unsqueeze(2) ,\n",
    "                                 box_v_logits[:,:,0].unsqueeze(2) ,\n",
    "                                 box_v_logits[:,:,1].unsqueeze(2) , \n",
    "                                 box_u_logits[:,:,1].unsqueeze(2) ,\n",
    "                                 box_v_logits[:,:,2].unsqueeze(2) ,\n",
    "                                 box_v_logits[:,:,3].unsqueeze(2)] , dim=-1 )\n",
    "        \n",
    "        return box_logits ,cls_logits            \n",
    "\n",
    "        return out_box , out_cls\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inf(self , imgs ):\n",
    "        \n",
    "        out_box , out_cls   = self.forward(imgs)  # [ batch , top_k , 5]   , [ batch , top_k , 1]         \n",
    "        #print(\"val \" , out_cls)\n",
    "        #print(\"val sigmoid\" , torch.sigmoid(out_cls))\n",
    "        \n",
    "        batch_size = out_box.shape[0]\n",
    "        #sampled_u_idx = torch.argwhere( torch.sigmoid(out_cls.view(batch_size , -1)) > 0.01 )\n",
    "        #print(\"sampled_u_idx\" , sampled_u_idx)\n",
    "\n",
    "        sampled_box_b = []\n",
    "        #each batch\n",
    "        for img , pred , pcls in zip(imgs, out_box , out_cls.view(batch_size,-1)):  \n",
    "            u_id = torch.argwhere(torch.sigmoid(pcls) > self.confidence_threshold)\n",
    "            print(\"u_id\"  , u_id)\n",
    "            if(u_id.numel() ==0):\n",
    "                continue\n",
    "            u_id = u_id.view(-1)            \n",
    "            \n",
    "            #pred = self.post_process(pbox[u_id,:] , u_id ).view(-1,6)\n",
    "            pred = pred[u_id]\n",
    "            \n",
    "            save_folder = create_folder( os.path.join(self.log_folder ,\"val\"))\n",
    "            save_path = os.path.join(save_folder, f\"val_ep_{self.current_epoch}-{self.global_step}\" )\n",
    "\n",
    "            decode_pred = decode_target(pred.clone())\n",
    "            #pred_us , pred_tops , pred_btms = self.pack_visualize(pred[:,0], pred[:,1],pred[:,2],pred[:,3] -pred[:,0] ,pred[:,4],pred[:,5] )                    \n",
    "            pred_us , pred_tops , pred_btms = self.pack_visualize(decode_pred[:,0], decode_pred[:,1],decode_pred[:,2],decode_pred[:,3] ,decode_pred[:,4],decode_pred[:,5] )                    \n",
    "            vis_imgs = visualize_2d_single(pred_us , pred_tops , pred_btms , u_grad = F.sigmoid(pcls).view(1 , -1 ) , imgs=  img , title=\"Pred\" , save_path= save_path  )\n",
    "            #plt.imshow(vis_imgs)\n",
    "            #plt.show()\n",
    "\n",
    "            # ToDo: calculate loss          \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pack_visualize(self, gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , dv_btm_b ):\n",
    "        \n",
    "        if isinstance(gt_u_b, torch.Tensor):\n",
    "            sizes = [t.numel() for t in gt_u_b]               \n",
    "            us = gt_u_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            us[1::2]+=gt_du_b.flatten()\n",
    "            us = torch.split(us.view(-1,2) , sizes)\n",
    "\n",
    "            tops = gt_vtop_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            tops[1::2]=gt_dvtop_b.flatten()\n",
    "            tops = torch.split(tops.view(-1,2) , sizes)\n",
    "\n",
    "            btms = gt_vbtm_b.flatten().unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            btms[1::2]=dv_btm_b.flatten()\n",
    "            btms = torch.split(btms.view(-1,2) , sizes)\n",
    "\n",
    "        elif isinstance(gt_u_b, tuple) and all(isinstance(t, torch.Tensor) for t in gt_u_b):        \n",
    "            sizes = [len(t) for t in gt_u_b]               \n",
    "            us = torch.cat(gt_u_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            us[1::2]+=torch.cat(gt_du_b).view(-1)\n",
    "            us = torch.split(us.view(-1,2) , sizes)\n",
    "\n",
    "            tops = torch.cat(gt_vtop_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            tops[1::2]=torch.cat(gt_dvtop_b).view(-1)\n",
    "            tops = torch.split(tops.view(-1,2) , sizes)\n",
    "\n",
    "            btms = torch.cat(gt_vbtm_b).view(-1).unsqueeze(0).repeat(2, 1).permute(1,0).reshape(-1)\n",
    "            btms[1::2]=torch.cat(dv_btm_b).view(-1)\n",
    "            btms = torch.split(btms.view(-1,2) , sizes)\n",
    "        else:\n",
    "            assert(\"Wrong Type.\")\n",
    "        \n",
    "        return us , tops ,btms\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def training_step(self , input_b ,batch_idx ):\n",
    "        \n",
    "        img = input_b['image']        \n",
    "        out_box , out_cls   = self.forward(img)  # [ batch , top_k , 5]   , [ batch , top_k , 1] \n",
    "        #print(\"max out_cls\" , torch.max(out_cls) , \"min \" , torch.min(out_cls))\n",
    "        batch_size = out_box.shape[0]\n",
    "        \n",
    "        # remove padding , each batch have different length\n",
    "        gt_u_b = unpad_data( input_b['u'])          \n",
    "        gt_vtop_b =unpad_data(input_b['v_top'])\n",
    "        gt_vbtm_b = unpad_data (input_b['v_btm'])\n",
    "        gt_du_b = unpad_data(input_b['du'])\n",
    "        gt_dvtop_b = unpad_data(input_b['dv_top'])\n",
    "        gt_dv_btm_b = unpad_data(input_b['dv_btm'])\n",
    "        \n",
    "        total_loss = 0\n",
    "        b_cnt = 0        \n",
    "\n",
    "        \n",
    "        for u,vtop,vbtm,du,dvtop, dvbtm , pred ,cls_b , gt_grad_cls  in zip(gt_u_b , gt_vtop_b , gt_vbtm_b , gt_du_b , gt_dvtop_b , gt_dv_btm_b , out_box , out_cls ,input_b['u_grad'] ):\n",
    "            \n",
    "            # match                        \n",
    "            gt_box =  torch.vstack([ u, vtop,vbtm,  du ,dvtop , dvbtm]).permute(1,0)   # [n , 6]\n",
    "            \n",
    "            u_grad = (torch.arange(self.patch_out,device=u.device).unsqueeze(-1) /self.patch_out ).view(-1)\n",
    "            \n",
    "            decode_pred_b = decode_target(pred.clone() , u_grad)          \n",
    "            \n",
    "\n",
    "            u_cost = torch.cdist( (torch.arange(self.patch_out,device=u.device).unsqueeze(-1) /self.patch_out ) , u.unsqueeze(-1) )                        \n",
    "            #box_loss = torch.cdist( pred , gt_box , p=1)\n",
    "            #box_loss = torch.cdist( pred , encode_gt_b , p=1)\n",
    "            #box_loss = torch.cdist( decode_pred_b , gt_box , p=1)\n",
    "            cls_cost = -torch.sigmoid(cls_b)\n",
    "            \n",
    "            #cost_matrix = box_loss  + cls_cost  + u_cost\n",
    "            #cost_matrix =  cls_cost  + u_cost*3\n",
    "            cost_matrix =  cls_cost + u_cost * 2\n",
    "            #cost_matrix = u_cost\n",
    "            \n",
    "            \n",
    "            cost_matrix = cost_matrix.detach().cpu().numpy()            \n",
    "            row_idx  , col_idx = linear_sum_assignment(cost_matrix)    \n",
    "            matched_u = torch.tensor( np.float32(row_idx)/self.patch_out,device=u.device)\n",
    "\n",
    "            encode_gt_b = encode_target(gt_box.clone()  , matched_u)\n",
    "            '''\n",
    "            print(\"matched u\" , row_idx / self.patch_out )\n",
    "            print(\"pred\" , pred[row_idx])\n",
    "            print(\"decode_pred_b\",decode_pred_b[row_idx])\n",
    "            print(\"encode_gt_b\",encode_gt_b[col_idx])\n",
    "            print(\"gt\" , gt_box[col_idx])\n",
    "            '''\n",
    "            \n",
    "            gt_cls = torch.zeros(self.patch_out,device= cls_b.device )            \n",
    "            gt_cls[row_idx] = 1            \n",
    "            \n",
    "            #gt_l1_target = encode_target(gt_box.clone() , matched_u)           \n",
    "            \n",
    "            #decode_pred = decode_target(pred.clone()[row_idx].view(-1,6) , matched_u)\n",
    "            #decode_pred = pred.clone()[row_idx].view(-1,6)   # debug\n",
    "            #l1_loss = F.l1_loss(decode_pred ,  gt_l1_target[col_idx]) \n",
    "            l1_loss = F.l1_loss(pred[row_idx] ,  encode_gt_b[col_idx]) \n",
    "            #l1_loss = F.l1_loss(decode_pred_b[row_idx] ,  gt_box[col_idx]) \n",
    "            #cls_loss = F.binary_cross_entropy_with_logits(cls_b.view(-1), gt_cls.view(-1)) \n",
    "            cls_loss = F.binary_cross_entropy_with_logits(cls_b.view(-1), gt_grad_cls) \n",
    "            #cls_loss = F.binary_cross_entropy_with_logits(cls_b.view(-1)[cls_sample_idx], gt_cls.view(-1)[cls_sample_idx]) \n",
    "            #print(\"cls_b\" , cls_b.view(-1))\n",
    "            #print(\"gt_cls\" , gt_cls.view(-1))\n",
    "            #print(\"pred box\" , decode_pred)\n",
    "            #print(\"gt box\" , gt_box[col_idx])\n",
    "\n",
    "            #print(\"L1 loss\"  , l1_loss)\n",
    "            #print(\"cls_loss\"  , cls_loss)\n",
    "            \n",
    "            total_loss += l1_loss + cls_loss*5\n",
    "            #total_loss += cls_loss\n",
    "                        \n",
    "            with torch.no_grad():\n",
    "                #if self.current_epoch % 5 == 0  :                \n",
    "                #if self.current_epoch % 5 == 0 and self.current_epoch > 0 and batch_idx<2 :                \n",
    "                if self.current_epoch > 0 and batch_idx <5 :                \n",
    "                    save_path =  os.path.join(self.log_folder , f\"gt_ep_{self.current_epoch}-{self.global_step}-{batch_idx}\" )\n",
    "                    \n",
    "                    gt_us , gt_tops , gt_btms = self.pack_visualize(u.view(1 , -1 ) , vtop , vbtm , du , dvtop , dvbtm )\n",
    "                    \n",
    "\n",
    "                    #vis_imgs = visualize_2d_single(gt_us , gt_tops , gt_btms , u_grad =  gt_cls.view(1 , -1 ), imgs= img[b_cnt] , title=\"GT\",save_path=save_path )                \n",
    "                    vis_imgs = visualize_2d_single(gt_us , gt_tops , gt_btms , u_grad =  gt_grad_cls.view(1 , -1 ), imgs= img[b_cnt] , title=\"GT\",save_path=save_path )                \n",
    "                    \n",
    "                    decode_pred = decode_pred_b[row_idx]\n",
    "                    \n",
    "                    save_path =  os.path.join(self.log_folder , f\"pred_ep_{self.current_epoch}-{self.global_step}-{batch_idx}\" )\n",
    "                    #decode_pred = decode_target(pred.clone()[row_idx].view(-1,6) , matched_u)\n",
    "                    #pred_us , pred_tops , pred_btms = self.pack_visualize(pred[row_idx,0], pred[row_idx,1],pred[row_idx,2],pred[row_idx,3] ,pred[row_idx,4],pred[row_idx,5] )                    \n",
    "                    pred_us , pred_tops , pred_btms = self.pack_visualize(decode_pred[:,0], decode_pred[:,1],decode_pred[:,2],\n",
    "                                                                          decode_pred[:,3]-decode_pred[:,0] ,\n",
    "                                                                          decode_pred[:,4],decode_pred[:,5] )                    \n",
    "                    vis_imgs = visualize_2d_single(pred_us , pred_tops , pred_btms , u_grad = F.sigmoid(cls_b).view(1 , -1 ) , imgs=  img[b_cnt] ,\n",
    "                                                    title=f\"Pred_row{row_idx}-\\n u:{pred_us}\" , save_path= save_path  )\n",
    "                    \n",
    "                    #plt.imshow(vis_imgs)\n",
    "                    #plt.show()\n",
    "           \n",
    "            b_cnt+=1\n",
    "            pass        \n",
    "        \n",
    "        op1  = self.optimizers()\n",
    "        op1.zero_grad()        \n",
    "        self.manual_backward(total_loss / batch_size)\n",
    "        op1.step()\n",
    "\n",
    "        return total_loss / batch_size\n",
    "        pass    \n",
    "\n",
    "    def __validation_step(self, input_b, batch_idx):\n",
    "        print(\"val!!!!!\")\n",
    "        img = input_b['image']\n",
    "        \n",
    "        #out_box , out_cls   = self.forward(img)  # [ batch , top_k , 5]   , [ batch , top_k , 1]         \n",
    "        if(batch_idx %2==0 and self.current_epoch>0 ):\n",
    "            self.inf(img)\n",
    "        return\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        backbone_opt = optim.Adam(self.backbone.parameters() , lr=0.00035)\n",
    "        comp_opt = optim.Adam(self.reduce_height_module.parameters() , lr=0.00035)\n",
    "        transforms_opt = optim.Adam(self.transformer.parameters() , lr=0.00035)\n",
    "        '''\n",
    "        opt = optim.Adam(self.parameters() , lr=0.00035)\n",
    "\n",
    "        return [opt] , []\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "# Unit testing...\n",
    "save_path = create_folder( os.path.join(os.getcwd() , \"output\" , \"checkpoints\"))\n",
    "save_file = os.path.join(save_path , \"detr_v1_d20_e50.pth\")\n",
    "\n",
    "img_size=[256, 128] \n",
    "# Test\n",
    "dm = CustomDataModule ( train_dir= f\"../anno/train_visiable_20_no_cross.json\" ,\n",
    "                        test_dir= f\"../anno/test_visiable_10_no_cross.json\" ,\n",
    "                          padding_count=8 , use_aug=False , c= 0.65,batch_size=5 ,\n",
    "                          img_size=img_size\n",
    "                       )\n",
    "\n",
    "#m = VerticalQueryTransformer(max_predict_count = 20 , hidden_out=256 , load_weight=\"D:/OneDrive/OneDrive - NTHU/Layout/Horizon/0912_all_bk.pth\"  , backbone_trainable=True)\n",
    "m = VerticalQueryTransformer(max_predict_count = 100 , hidden_out=2048 *4 ,img_size=img_size  , backbone_trainable=True ,dropout=0)\n",
    "\n",
    "'''\n",
    "save_path = create_folder( os.path.join(os.getcwd() , \"output\" , \"checkpoints\"))\n",
    "save_file = os.path.join(save_path , \"detr_v1_d20_e50.pth\")\n",
    "m = torch.load(save_file)\n",
    "          \n",
    "\n",
    "\n",
    "save_path = create_folder( os.path.join(os.getcwd() , \"output\" , \"checkpoints\"))\n",
    "save_file = os.path.join(save_path , \"detr_v9_d20_e200.pth\")\n",
    "m.load_state_dict(torch.load(save_file))\n",
    "'''\n",
    "\n",
    "trainer = pl.Trainer(accelerator='gpu' , devices=1 ,\n",
    "                     min_epochs=1, max_epochs=201 , precision=16 , fast_dev_run=False )\n",
    "trainer.fit(m , dm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,3,512,1024)\n",
    "c = nn.Conv2d(in_channels= 3, out_channels= 256 , kernel_size=(512,1) , stride=(512,1))\n",
    "\n",
    "b = c(a)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,5,8)\n",
    "self_attn = nn.MultiheadAttention(8, 8, dropout=0.1)\n",
    "atten , src = self_attn(a  , a, a)\n",
    "\n",
    "print(atten.shape) \t# ([20, 5, 256])\n",
    "print(src.shape) \t# ([5, 20, 20])\n",
    "plt.imshow(src[0].detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(atten.permute(1,0,2)[0].detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "print(atten)\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = create_folder( os.path.join(os.getcwd() , \"output\" , \"checkpoints\"))\n",
    "save_file = os.path.join(save_path , \"detr_v1_d20_e50.pth\")\n",
    "torch.save(m , save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = torch.rand(1,5,8)\n",
    "a = torch.arange(80.0).view(2,5,8)\n",
    "n = nn.Linear(8,2)\n",
    "n.bias.data.fill_(1)\n",
    "n.weight.data.fill_(1)\n",
    "\n",
    "print(n.weight.shape)\n",
    "print(n.bias)\n",
    "b =n(a)\n",
    "print(a)\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.rand(20,2)\n",
    "b = torch.rand(20)*2\n",
    "b = b.to(torch.long).view(-1,1)\n",
    "#c = a[b]\n",
    "c= torch.gather(a,1,b)\n",
    "print(c.shape)\n",
    "print(a)\n",
    "print(b)\n",
    "print()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "horizon_path =r\"D:/OneDrive/OneDrive - NTHU/Layout/Horizon/0912_all_bk.pth\"\n",
    "#models_dict = torch.load_s\n",
    "checkpoint = torch.load(horizon_path ,  map_location=\"cpu\")\n",
    "print(checkpoint['state_dict'].keys())\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in m.state_dict()}\n",
    "m.load_state_dict(pretrained_dict , strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.rand(2,256 ,1024)\n",
    "b = nn.Conv1d(256 , 64 , kernel_size=3 ,padding=1)\n",
    "c = b(a)\n",
    "print(c.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,5,2)\n",
    "print(a)\n",
    "b=  nn.MaxPool2d((5,1))\n",
    "c = b(a)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(200).view(2,100,1)\n",
    "#aa = a[:,:,0].unsqueeze(0)\n",
    "#print(aa.shape)\n",
    "b = F.interpolate(a.view(2,-1).unsqueeze(0), 10 )[0]\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,10,1)\n",
    "b = torch.cat([a,a] , dim=1)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = torch.rand(2,100,1)\n",
    "a = torch.arange(400).view(2,100,2)\n",
    "b = torch.arange(20).view(2,10)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "b= b.unsqueeze(-1).repeat(1,1,2)\n",
    "print(b)\n",
    "#print(\"b unsqueeze\",b.unsqueeze(-1))\n",
    "\n",
    "c = a.gather(1, b)\n",
    "print(c.shape)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.arange(5)\n",
    "b=torch.arange(5)\n",
    "c=torch.arange(5)\n",
    "\n",
    "d = torch.vstack([a,b,c]).permute(1,0)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "a = torch.tensor([ [0,1,2]  ,  [0,3,5] , [1,0,5] ]).to(torch.float32)\n",
    "b = torch.tensor([ [0,1,2] , [1,0,5] ]).to(torch.float32)\n",
    "\n",
    "cost = torch.cdist(b,a)\n",
    "print(cost)\n",
    "row , col = linear_sum_assignment(cost,)\n",
    "print(row)\n",
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.7605, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7730, 0.5752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7057, 0.5861, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8386, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8304, 0.7823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7034, 0.5994, 0.5691, 0.5652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.6996, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8305, 0.7819, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.8238, 0.7839, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "# Get the indices of non-zero elements\n",
    "non_zero_indices = torch.nonzero(x)\n",
    "print(non_zero_indices)\n",
    "# Get the non-zero values\n",
    "non_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "unique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "print(\"unique\" , unique)\n",
    "# Print the result\n",
    "print(non_zero_values)\n",
    "non_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "print(\"split non_zero_values\" , non_zero_values)\n",
    "\n",
    "def unpad_data( x :[Tensor] ) :\n",
    "\tnon_zero_indices = torch.nonzero(x)\n",
    "\tprint(non_zero_indices)\n",
    "\t# Get the non-zero values\n",
    "\tnon_zero_values = x[non_zero_indices[:,0], non_zero_indices[:,1]]\n",
    "\n",
    "\tunique = torch.unique(non_zero_indices[:,0] ,return_counts=True)\n",
    "\tprint(\"unique\" , unique)\n",
    "\t# Print the result\n",
    "\tprint(non_zero_values)\n",
    "\tnon_zero_values = torch.split(non_zero_values , tuple(unique[1]))\n",
    "\tprint(\"split non_zero_values\" , non_zero_values)\n",
    "\treturn non_zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.58 , 0.6] , [0.4] ] , )\n",
    "b = torch.tensor([0.1 , 0.2] , )\n",
    "\n",
    "c = a.repeat(2)\n",
    "print(a.repeat(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.rand(1)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Layout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
