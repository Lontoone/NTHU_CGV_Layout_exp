[01/08 22:44:19] detectron2 INFO: Rank of current process: 0. World size: 1
[01/08 22:44:19] detectron2 INFO: Environment info:
----------------------  -----------------------------------------------------------------------------------------------
sys.platform            win32
Python                  3.7.7 (default, Mar 23 2020, 23:19:08) [MSC v.1916 64 bit (AMD64)]
numpy                   1.21.6
detectron2              0.6 @e:\projects\layout\detrex\detectron2\detectron2
Compiler                MSVC 193632537
CUDA compiler           CUDA 11.7
detectron2 arch flags   e:\projects\layout\detrex\detectron2\detectron2\_C.cp37-win_amd64.pyd; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.1 @C:\Users\User\anaconda3\envs\Layout\lib\site-packages\torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version          537.13
CUDA_HOME               C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.7
Pillow                  9.3.0
torchvision             0.14.1 @C:\Users\User\anaconda3\envs\Layout\lib\site-packages\torchvision
torchvision arch flags  C:\Users\User\anaconda3\envs\Layout\lib\site-packages\torchvision\_C.pyd; cannot find cuobjdump
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.0
----------------------  -----------------------------------------------------------------------------------------------
PyTorch built with:
  - C++ Version: 199711
  - MSVC 192829337
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 2019
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:/cb/pytorch_1000000000000/work/mkl/include -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/08 22:44:19] detectron2 INFO: Command line arguments: Namespace(config_file='projects/detr/configs/detr_r50_300ep.py', dist_url='tcp://127.0.0.1:49153', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[01/08 22:44:19] detectron2 INFO: Contents of args.config_file=projects/detr/configs/detr_r50_300ep.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mdetrex[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mget_config[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mdetr_r50[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m

[38;5;242m#dataloader = get_config("common/data/coco_detr.py").dataloader[39m
[38;5;15mdataloader[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/data/custom_zillow.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;197m.[39m[38;5;15mdataloader[39m
[38;5;15mlr_multiplier[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/coco_schedule.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;197m.[39m[38;5;15mlr_multiplier_50ep[39m
[38;5;15moptimizer[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/optim.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;197m.[39m[38;5;15mAdamW[39m
[38;5;15mtrain[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/train.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;197m.[39m[38;5;15mtrain[39m

[38;5;242m# modify training config[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/torchvision/R-50.pkl[39m[38;5;186m"[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./output/detr_r50_300ep[39m[38;5;186m"[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmax_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m554400[39m

[38;5;242m# modify lr_multiplier[39m
[38;5;15mlr_multiplier[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m369600[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m554400[39m[38;5;15m][39m

[38;5;242m# modify optimizer config[39m
[38;5;15moptimizer[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;197m.[39m[38;5;15mparams[39m[38;5;197m.[39m[38;5;15mlr_factor_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mlambda[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m[38;5;15m [39m[38;5;81mif[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbackbone[39m[38;5;186m"[39m[38;5;15m [39m[38;5;197min[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m [39m[38;5;81melse[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# modify dataloader config[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mnum_workers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtotal_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m10[39m

[01/08 22:44:19] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/detr_r50_300ep\config.yaml is human-readable but cannot be loaded.
[01/08 22:44:19] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/detr_r50_300ep\config.yaml.pkl.
[01/08 22:44:19] detectron2 INFO: Full config saved to ./output/detr_r50_300ep\config.yaml
[01/08 22:44:19] d2.utils.env INFO: Using a generated random seed 19806302
[01/08 22:44:20] detectron2 INFO: Model:
DETR(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (position_embedding): PositionEmbeddingSine()
  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
  (transformer): DetrTransformer(
    (encoder): DetrTransformerEncoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (query_embed): Embedding(100, 256)
  (class_embed): Linear(in_features=256, out_features=81, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 1
          cost_bbox: 5.0
          cost_giou: 2.0
          cost_class_type: ce_cost
          focal cost alpha: 0.25
          focal cost gamma: 2.0
      losses: ['class', 'boxes']
      loss_class_type: ce_loss
      weight_dict: {'loss_class': 1, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_class_0': 1, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_class_1': 1, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_class_2': 1, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_class_3': 1, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_class_4': 1, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0}
      num_classes: 80
      eos_coef: 0.1
      focal loss alpha: 0.25
      focal loss gamma: 2.0
)
[01/08 22:44:20] d2.data.datasets.coco INFO: Loaded 200 images in COCO format from ../anno/instances_train2017.json
[01/08 22:44:20] d2.data.build INFO: Removed 0 images with no usable annotations. 200 images left.
[01/08 22:44:20] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    door    | 428          |
|            |              |[0m
[01/08 22:44:20] d2.data.common INFO: Serializing 200 elements to byte tensors and concatenating them all ...
[01/08 22:44:20] d2.data.common INFO: Serialized dataset takes 0.08 MiB
[01/08 22:44:20] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[01/08 22:44:20] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[01/08 22:44:20] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint                                                               | Shapes                                          |
|:------------------|:----------------------------------------------------------------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.*      | stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |
[01/08 22:44:20] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbbox_embed.layers.0.{bias, weight}[0m
[34mbbox_embed.layers.1.{bias, weight}[0m
[34mbbox_embed.layers.2.{bias, weight}[0m
[34mclass_embed.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34minput_proj.{bias, weight}[0m
[34mquery_embed.weight[0m
[34mtransformer.decoder.layers.0.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.0.attentions.1.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.0.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.0.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.0.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.0.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.0.norms.2.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.1.attentions.1.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.1.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.1.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.1.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.1.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.1.norms.2.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.2.attentions.1.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.2.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.2.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.2.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.2.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.2.norms.2.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.3.attentions.1.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.3.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.3.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.3.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.3.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.3.norms.2.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.4.attentions.1.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.4.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.4.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.4.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.4.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.4.norms.2.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.5.attentions.1.attn.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.decoder.layers.5.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.5.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.5.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.5.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.5.norms.2.{bias, weight}[0m
[34mtransformer.decoder.post_norm_layer.{bias, weight}[0m
[34mtransformer.encoder.layers.0.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.0.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.0.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.0.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.0.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.0.norms.1.{bias, weight}[0m
[34mtransformer.encoder.layers.1.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.1.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.1.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.1.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.1.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.1.norms.1.{bias, weight}[0m
[34mtransformer.encoder.layers.2.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.2.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.2.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.2.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.2.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.2.norms.1.{bias, weight}[0m
[34mtransformer.encoder.layers.3.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.3.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.3.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.3.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.3.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.3.norms.1.{bias, weight}[0m
[34mtransformer.encoder.layers.4.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.4.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.4.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.4.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.4.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.4.norms.1.{bias, weight}[0m
[34mtransformer.encoder.layers.5.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.5.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.5.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.5.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.5.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.5.norms.1.{bias, weight}[0m
[01/08 22:44:20] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[01/08 22:44:20] d2.engine.train_loop INFO: Starting training from iteration 0
[01/08 22:47:08] d2.utils.events INFO:  eta: 46 days, 0:45:09  iter: 19  total_loss: 50.6  loss_class: 0.6882  loss_bbox: 5.431  loss_giou: 2.289  loss_class_0: 0.6594  loss_bbox_0: 5.837  loss_giou_0: 2.27  loss_class_1: 0.7024  loss_bbox_1: 5.634  loss_giou_1: 2.277  loss_class_2: 0.7307  loss_bbox_2: 5.349  loss_giou_2: 2.31  loss_class_3: 0.6972  loss_bbox_3: 5.312  loss_giou_3: 2.306  loss_class_4: 0.7026  loss_bbox_4: 5.386  loss_giou_4: 2.318  time: 8.1172  data_time: 0.4431  lr: 0.0001  max_mem: 23550M
[01/08 22:49:51] d2.utils.events INFO:  eta: 45 days, 19:34:59  iter: 39  total_loss: 39.57  loss_class: 0.5651  loss_bbox: 3.111  loss_giou: 2.758  loss_class_0: 0.5384  loss_bbox_0: 3.294  loss_giou_0: 2.717  loss_class_1: 0.5602  loss_bbox_1: 3.116  loss_giou_1: 2.755  loss_class_2: 0.5594  loss_bbox_2: 3.17  loss_giou_2: 2.724  loss_class_3: 0.5494  loss_bbox_3: 3.148  loss_giou_3: 2.738  loss_class_4: 0.5474  loss_bbox_4: 3.13  loss_giou_4: 2.758  time: 8.1299  data_time: 0.3854  lr: 0.0001  max_mem: 23550M
[01/08 22:52:30] d2.utils.events INFO:  eta: 45 days, 21:40:32  iter: 59  total_loss: 36.29  loss_class: 0.4914  loss_bbox: 2.291  loss_giou: 3.292  loss_class_0: 0.4962  loss_bbox_0: 2.277  loss_giou_0: 3.268  loss_class_1: 0.4987  loss_bbox_1: 2.251  loss_giou_1: 3.285  loss_class_2: 0.4916  loss_bbox_2: 2.296  loss_giou_2: 3.302  loss_class_3: 0.4931  loss_bbox_3: 2.296  loss_giou_3: 3.297  loss_class_4: 0.49  loss_bbox_4: 2.268  loss_giou_4: 3.301  time: 8.0640  data_time: 0.3923  lr: 0.0001  max_mem: 24333M
[01/08 22:55:06] d2.utils.events INFO:  eta: 45 days, 22:25:37  iter: 79  total_loss: 35.7  loss_class: 0.4452  loss_bbox: 2.014  loss_giou: 3.419  loss_class_0: 0.4303  loss_bbox_0: 2.021  loss_giou_0: 3.439  loss_class_1: 0.4329  loss_bbox_1: 2.01  loss_giou_1: 3.416  loss_class_2: 0.4321  loss_bbox_2: 2.033  loss_giou_2: 3.438  loss_class_3: 0.4378  loss_bbox_3: 2.041  loss_giou_3: 3.414  loss_class_4: 0.4299  loss_bbox_4: 2.027  loss_giou_4: 3.475  time: 7.9873  data_time: 0.3767  lr: 0.0001  max_mem: 24333M
[01/08 22:57:39] d2.utils.events INFO:  eta: 46 days, 8:27:34  iter: 99  total_loss: 33.49  loss_class: 0.4523  loss_bbox: 1.738  loss_giou: 3.335  loss_class_0: 0.4228  loss_bbox_0: 1.755  loss_giou_0: 3.47  loss_class_1: 0.4353  loss_bbox_1: 1.728  loss_giou_1: 3.405  loss_class_2: 0.4238  loss_bbox_2: 1.779  loss_giou_2: 3.434  loss_class_3: 0.4338  loss_bbox_3: 1.741  loss_giou_3: 3.402  loss_class_4: 0.4251  loss_bbox_4: 1.767  loss_giou_4: 3.476  time: 7.9189  data_time: 0.3773  lr: 0.0001  max_mem: 24333M
[01/08 23:00:16] d2.utils.events INFO:  eta: 46 days, 0:52:38  iter: 119  total_loss: 31.96  loss_class: 0.5052  loss_bbox: 1.575  loss_giou: 3.117  loss_class_0: 0.4556  loss_bbox_0: 1.576  loss_giou_0: 3.335  loss_class_1: 0.47  loss_bbox_1: 1.574  loss_giou_1: 3.235  loss_class_2: 0.4756  loss_bbox_2: 1.632  loss_giou_2: 3.274  loss_class_3: 0.4646  loss_bbox_3: 1.575  loss_giou_3: 3.26  loss_class_4: 0.4676  loss_bbox_4: 1.635  loss_giou_4: 3.313  time: 7.9053  data_time: 0.3820  lr: 0.0001  max_mem: 24333M
[01/08 23:03:06] d2.utils.events INFO:  eta: 46 days, 7:38:04  iter: 139  total_loss: 29.01  loss_class: 0.4769  loss_bbox: 1.427  loss_giou: 2.941  loss_class_0: 0.4283  loss_bbox_0: 1.352  loss_giou_0: 3.144  loss_class_1: 0.4747  loss_bbox_1: 1.345  loss_giou_1: 2.957  loss_class_2: 0.468  loss_bbox_2: 1.338  loss_giou_2: 3.034  loss_class_3: 0.4416  loss_bbox_3: 1.346  loss_giou_3: 2.987  loss_class_4: 0.4432  loss_bbox_4: 1.346  loss_giou_4: 3.057  time: 7.9945  data_time: 0.3872  lr: 0.0001  max_mem: 24333M
[01/08 23:05:52] d2.utils.events INFO:  eta: 46 days, 9:35:37  iter: 159  total_loss: 26.48  loss_class: 0.4744  loss_bbox: 1.145  loss_giou: 2.683  loss_class_0: 0.4356  loss_bbox_0: 1.27  loss_giou_0: 2.989  loss_class_1: 0.4741  loss_bbox_1: 1.256  loss_giou_1: 2.748  loss_class_2: 0.4785  loss_bbox_2: 1.192  loss_giou_2: 2.709  loss_class_3: 0.4508  loss_bbox_3: 1.233  loss_giou_3: 2.724  loss_class_4: 0.4553  loss_bbox_4: 1.135  loss_giou_4: 2.766  time: 8.0327  data_time: 0.3732  lr: 0.0001  max_mem: 24333M
[01/08 23:08:31] d2.utils.events INFO:  eta: 46 days, 10:23:29  iter: 179  total_loss: 26.38  loss_class: 0.4697  loss_bbox: 1.15  loss_giou: 2.78  loss_class_0: 0.4549  loss_bbox_0: 1.233  loss_giou_0: 2.951  loss_class_1: 0.4898  loss_bbox_1: 1.233  loss_giou_1: 2.695  loss_class_2: 0.4836  loss_bbox_2: 1.177  loss_giou_2: 2.623  loss_class_3: 0.4419  loss_bbox_3: 1.126  loss_giou_3: 2.639  loss_class_4: 0.4508  loss_bbox_4: 1.089  loss_giou_4: 2.859  time: 8.0215  data_time: 0.3843  lr: 0.0001  max_mem: 24333M
[01/08 23:11:24] d2.utils.events INFO:  eta: 46 days, 13:21:05  iter: 199  total_loss: 24.44  loss_class: 0.4575  loss_bbox: 1.073  loss_giou: 2.503  loss_class_0: 0.4496  loss_bbox_0: 1.128  loss_giou_0: 2.618  loss_class_1: 0.4655  loss_bbox_1: 1.036  loss_giou_1: 2.551  loss_class_2: 0.4641  loss_bbox_2: 1.039  loss_giou_2: 2.355  loss_class_3: 0.4373  loss_bbox_3: 0.9887  loss_giou_3: 2.496  loss_class_4: 0.4382  loss_bbox_4: 1.012  loss_giou_4: 2.369  time: 8.0867  data_time: 0.3852  lr: 0.0001  max_mem: 24333M
[01/08 23:14:10] d2.utils.events INFO:  eta: 46 days, 13:23:30  iter: 219  total_loss: 23.9  loss_class: 0.4408  loss_bbox: 0.9487  loss_giou: 2.539  loss_class_0: 0.4618  loss_bbox_0: 1.074  loss_giou_0: 2.604  loss_class_1: 0.4592  loss_bbox_1: 1.064  loss_giou_1: 2.459  loss_class_2: 0.4726  loss_bbox_2: 1.027  loss_giou_2: 2.391  loss_class_3: 0.4386  loss_bbox_3: 1.199  loss_giou_3: 2.406  loss_class_4: 0.4714  loss_bbox_4: 0.9629  loss_giou_4: 2.353  time: 8.1026  data_time: 0.3814  lr: 0.0001  max_mem: 24333M
[01/08 23:16:47] d2.utils.events INFO:  eta: 46 days, 14:10:37  iter: 239  total_loss: 24.51  loss_class: 0.4199  loss_bbox: 0.9862  loss_giou: 2.757  loss_class_0: 0.4251  loss_bbox_0: 1.08  loss_giou_0: 2.522  loss_class_1: 0.4542  loss_bbox_1: 1.05  loss_giou_1: 2.442  loss_class_2: 0.4793  loss_bbox_2: 0.994  loss_giou_2: 2.577  loss_class_3: 0.4165  loss_bbox_3: 0.8614  loss_giou_3: 2.543  loss_class_4: 0.4693  loss_bbox_4: 0.8991  loss_giou_4: 2.743  time: 8.0831  data_time: 0.3893  lr: 0.0001  max_mem: 24333M
[01/08 23:19:35] d2.utils.events INFO:  eta: 46 days, 17:56:03  iter: 259  total_loss: 23.32  loss_class: 0.425  loss_bbox: 0.9412  loss_giou: 2.494  loss_class_0: 0.444  loss_bbox_0: 1.026  loss_giou_0: 2.515  loss_class_1: 0.4595  loss_bbox_1: 0.9859  loss_giou_1: 2.438  loss_class_2: 0.454  loss_bbox_2: 0.9501  loss_giou_2: 2.357  loss_class_3: 0.4217  loss_bbox_3: 0.983  loss_giou_3: 2.515  loss_class_4: 0.4488  loss_bbox_4: 0.9051  loss_giou_4: 2.462  time: 8.1065  data_time: 0.3814  lr: 0.0001  max_mem: 24333M
[01/08 23:22:20] d2.utils.events INFO:  eta: 46 days, 19:17:33  iter: 279  total_loss: 23.05  loss_class: 0.4206  loss_bbox: 1.014  loss_giou: 2.424  loss_class_0: 0.4151  loss_bbox_0: 0.9834  loss_giou_0: 2.466  loss_class_1: 0.4577  loss_bbox_1: 1.01  loss_giou_1: 2.484  loss_class_2: 0.4406  loss_bbox_2: 0.9159  loss_giou_2: 2.392  loss_class_3: 0.4074  loss_bbox_3: 0.9827  loss_giou_3: 2.403  loss_class_4: 0.4341  loss_bbox_4: 0.9607  loss_giou_4: 2.405  time: 8.1156  data_time: 0.3831  lr: 0.0001  max_mem: 24333M
[01/08 23:25:09] d2.utils.events INFO:  eta: 46 days, 18:55:13  iter: 299  total_loss: 22.73  loss_class: 0.4302  loss_bbox: 0.8819  loss_giou: 2.407  loss_class_0: 0.4126  loss_bbox_0: 0.9524  loss_giou_0: 2.381  loss_class_1: 0.4358  loss_bbox_1: 0.9645  loss_giou_1: 2.445  loss_class_2: 0.4155  loss_bbox_2: 0.8825  loss_giou_2: 2.47  loss_class_3: 0.3916  loss_bbox_3: 0.9341  loss_giou_3: 2.528  loss_class_4: 0.4237  loss_bbox_4: 0.8825  loss_giou_4: 2.464  time: 8.1389  data_time: 0.3929  lr: 0.0001  max_mem: 26224M
[01/08 23:28:07] d2.utils.events INFO:  eta: 47 days, 6:31:26  iter: 319  total_loss: 23  loss_class: 0.4086  loss_bbox: 0.8926  loss_giou: 2.458  loss_class_0: 0.4132  loss_bbox_0: 0.9702  loss_giou_0: 2.485  loss_class_1: 0.4366  loss_bbox_1: 0.9242  loss_giou_1: 2.372  loss_class_2: 0.4287  loss_bbox_2: 0.8853  loss_giou_2: 2.35  loss_class_3: 0.4246  loss_bbox_3: 0.9351  loss_giou_3: 2.478  loss_class_4: 0.4074  loss_bbox_4: 0.9104  loss_giou_4: 2.692  time: 8.1858  data_time: 0.3950  lr: 0.0001  max_mem: 26224M
[01/08 23:31:12] d2.utils.events INFO:  eta: 47 days, 9:28:53  iter: 339  total_loss: 22.92  loss_class: 0.4126  loss_bbox: 0.9195  loss_giou: 2.303  loss_class_0: 0.3941  loss_bbox_0: 1.015  loss_giou_0: 2.551  loss_class_1: 0.427  loss_bbox_1: 0.9822  loss_giou_1: 2.43  loss_class_2: 0.4312  loss_bbox_2: 0.8551  loss_giou_2: 2.354  loss_class_3: 0.4123  loss_bbox_3: 0.9175  loss_giou_3: 2.431  loss_class_4: 0.4093  loss_bbox_4: 1.007  loss_giou_4: 2.383  time: 8.2486  data_time: 0.3931  lr: 0.0001  max_mem: 26224M
