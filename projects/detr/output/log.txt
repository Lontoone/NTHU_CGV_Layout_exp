[08/11 16:38:25] detectron2 INFO: Rank of current process: 0. World size: 1
[08/11 16:38:34] detectron2 INFO: Environment info:
----------------------  ------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/comp_robot/rentianhe/code/detectron2/detectron2
Compiler                GCC 9.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   8.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3,4,5,6,7     A100-SXM4-40GB (arch=8.0)
Driver version          450.80.02
CUDA_HOME               /comp_robot/liushilong/software/cuda-11.1/
Pillow                  9.2.0
torchvision             0.13.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     Not found
----------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[08/11 16:38:34] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr_training.py', dist_url='tcp://127.0.0.1:54980', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[08/11 16:38:34] detectron2 INFO: Contents of args.config_file=configs/detr_training.py:
from ideadet.config import get_config

from .models.detr_r50 import model
from .common.coco_loader import dataloader

lr_multiplier = get_config("common/coco_schedule.py").lr_multiplier_1x
optimizer = get_config("common/optim.py").SGD
train = get_config("common/train.py").train

train.init_checkpoint = "/comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth"
train.output_dir = "./output"

[08/11 16:38:34] detectron2 INFO: Full config saved to ./output/config.yaml
[08/11 16:38:34] d2.utils.env INFO: Using a generated random seed 37908151
[08/11 16:38:40] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth ...
[08/11 16:38:41] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                   | Names in Checkpoint                                                                                        | Shapes                                          |
|:-------------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:------------------------------------------------|
| backbone.0.backbone.res2.0.conv1.*               | backbone.0.backbone.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| backbone.0.backbone.res2.0.conv2.*               | backbone.0.backbone.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.0.conv3.*               | backbone.0.backbone.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.0.shortcut.*            | backbone.0.backbone.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.1.conv1.*               | backbone.0.backbone.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.1.conv2.*               | backbone.0.backbone.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.1.conv3.*               | backbone.0.backbone.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.2.conv1.*               | backbone.0.backbone.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.2.conv2.*               | backbone.0.backbone.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.2.conv3.*               | backbone.0.backbone.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res3.0.conv1.*               | backbone.0.backbone.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| backbone.0.backbone.res3.0.conv2.*               | backbone.0.backbone.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.0.conv3.*               | backbone.0.backbone.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.0.shortcut.*            | backbone.0.backbone.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| backbone.0.backbone.res3.1.conv1.*               | backbone.0.backbone.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.1.conv2.*               | backbone.0.backbone.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.1.conv3.*               | backbone.0.backbone.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.2.conv1.*               | backbone.0.backbone.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.2.conv2.*               | backbone.0.backbone.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.2.conv3.*               | backbone.0.backbone.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.3.conv1.*               | backbone.0.backbone.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.3.conv2.*               | backbone.0.backbone.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.3.conv3.*               | backbone.0.backbone.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res4.0.conv1.*               | backbone.0.backbone.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| backbone.0.backbone.res4.0.conv2.*               | backbone.0.backbone.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.0.conv3.*               | backbone.0.backbone.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.0.shortcut.*            | backbone.0.backbone.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| backbone.0.backbone.res4.1.conv1.*               | backbone.0.backbone.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.1.conv2.*               | backbone.0.backbone.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.1.conv3.*               | backbone.0.backbone.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.2.conv1.*               | backbone.0.backbone.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.2.conv2.*               | backbone.0.backbone.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.2.conv3.*               | backbone.0.backbone.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.3.conv1.*               | backbone.0.backbone.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.3.conv2.*               | backbone.0.backbone.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.3.conv3.*               | backbone.0.backbone.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.4.conv1.*               | backbone.0.backbone.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.4.conv2.*               | backbone.0.backbone.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.4.conv3.*               | backbone.0.backbone.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.5.conv1.*               | backbone.0.backbone.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.5.conv2.*               | backbone.0.backbone.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.5.conv3.*               | backbone.0.backbone.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res5.0.conv1.*               | backbone.0.backbone.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| backbone.0.backbone.res5.0.conv2.*               | backbone.0.backbone.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.0.conv3.*               | backbone.0.backbone.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.0.shortcut.*            | backbone.0.backbone.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| backbone.0.backbone.res5.1.conv1.*               | backbone.0.backbone.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.1.conv2.*               | backbone.0.backbone.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.1.conv3.*               | backbone.0.backbone.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.2.conv1.*               | backbone.0.backbone.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.2.conv2.*               | backbone.0.backbone.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.2.conv3.*               | backbone.0.backbone.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.stem.conv1.*                 | backbone.0.backbone.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}           | (64,) (64,) (64,) (64,) (64,3,7,7)              |
| bbox_embed.layers.0.*                            | bbox_embed.layers.0.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.1.*                            | bbox_embed.layers.1.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.2.*                            | bbox_embed.layers.2.{bias,weight}                                                                          | (4,) (4,256)                                    |
| class_embed.*                                    | class_embed.{bias,weight}                                                                                  | (81,) (81,256)                                  |
| input_proj.*                                     | input_proj.{bias,weight}                                                                                   | (256,) (256,2048,1,1)                           |
| query_embed.weight                               | query_embed.weight                                                                                         | (100, 256)                                      |
| transformer.decoder.layers.0.attentions.0.attn.* | transformer.decoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.attentions.1.attn.* | transformer.decoder.layers.0.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.ffns.0.layers.0.0.* | transformer.decoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.0.ffns.0.layers.1.*   | transformer.decoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.0.norms.0.*           | transformer.decoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.1.*           | transformer.decoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.2.*           | transformer.decoder.layers.0.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.attentions.0.attn.* | transformer.decoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.attentions.1.attn.* | transformer.decoder.layers.1.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.ffns.0.layers.0.0.* | transformer.decoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.1.ffns.0.layers.1.*   | transformer.decoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.1.norms.0.*           | transformer.decoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.1.*           | transformer.decoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.2.*           | transformer.decoder.layers.1.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.attentions.0.attn.* | transformer.decoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.attentions.1.attn.* | transformer.decoder.layers.2.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.ffns.0.layers.0.0.* | transformer.decoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.2.ffns.0.layers.1.*   | transformer.decoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.2.norms.0.*           | transformer.decoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.1.*           | transformer.decoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.2.*           | transformer.decoder.layers.2.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.attentions.0.attn.* | transformer.decoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.attentions.1.attn.* | transformer.decoder.layers.3.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.ffns.0.layers.0.0.* | transformer.decoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.3.ffns.0.layers.1.*   | transformer.decoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.3.norms.0.*           | transformer.decoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.1.*           | transformer.decoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.2.*           | transformer.decoder.layers.3.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.attentions.0.attn.* | transformer.decoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.attentions.1.attn.* | transformer.decoder.layers.4.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.ffns.0.layers.0.0.* | transformer.decoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.4.ffns.0.layers.1.*   | transformer.decoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.4.norms.0.*           | transformer.decoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.1.*           | transformer.decoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.2.*           | transformer.decoder.layers.4.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.attentions.0.attn.* | transformer.decoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.attentions.1.attn.* | transformer.decoder.layers.5.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.ffns.0.layers.0.0.* | transformer.decoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.5.ffns.0.layers.1.*   | transformer.decoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.5.norms.0.*           | transformer.decoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.1.*           | transformer.decoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.2.*           | transformer.decoder.layers.5.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.post_norm_layer.*            | transformer.decoder.post_norm_layer.{bias,weight}                                                          | (256,) (256,)                                   |
| transformer.encoder.layers.0.attentions.0.attn.* | transformer.encoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.0.ffns.0.layers.0.0.* | transformer.encoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.0.ffns.0.layers.1.*   | transformer.encoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.0.norms.0.*           | transformer.encoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.0.norms.1.*           | transformer.encoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.attentions.0.attn.* | transformer.encoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.1.ffns.0.layers.0.0.* | transformer.encoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.1.ffns.0.layers.1.*   | transformer.encoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.1.norms.0.*           | transformer.encoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.norms.1.*           | transformer.encoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.attentions.0.attn.* | transformer.encoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.2.ffns.0.layers.0.0.* | transformer.encoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.2.ffns.0.layers.1.*   | transformer.encoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.2.norms.0.*           | transformer.encoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.norms.1.*           | transformer.encoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.attentions.0.attn.* | transformer.encoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.3.ffns.0.layers.0.0.* | transformer.encoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.3.ffns.0.layers.1.*   | transformer.encoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.3.norms.0.*           | transformer.encoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.norms.1.*           | transformer.encoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.attentions.0.attn.* | transformer.encoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.4.ffns.0.layers.0.0.* | transformer.encoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.4.ffns.0.layers.1.*   | transformer.encoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.4.norms.0.*           | transformer.encoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.norms.1.*           | transformer.encoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.attentions.0.attn.* | transformer.encoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.5.ffns.0.layers.0.0.* | transformer.encoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.5.ffns.0.layers.1.*   | transformer.encoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.5.norms.0.*           | transformer.encoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.norms.1.*           | transformer.encoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
[08/11 16:38:42] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[08/11 16:38:42] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_val2017.json
[08/11 16:38:43] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[08/11 16:38:43] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[08/11 16:38:43] d2.data.common INFO: Serialized dataset takes 19.26 MiB
[08/11 16:38:43] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[08/11 16:38:52] d2.evaluation.evaluator INFO: Inference done 1/5000. Dataloading: 0.3237 s/iter. Inference: 8.3803 s/iter. Eval: 0.0004 s/iter. Total: 8.7064 s/iter. ETA=12:05:23
[08/11 16:38:57] d2.evaluation.evaluator INFO: Inference done 69/5000. Dataloading: 0.0014 s/iter. Inference: 0.0720 s/iter. Eval: 0.0005 s/iter. Total: 0.0739 s/iter. ETA=0:06:04
[08/11 16:39:02] d2.evaluation.evaluator INFO: Inference done 140/5000. Dataloading: 0.0015 s/iter. Inference: 0.0703 s/iter. Eval: 0.0004 s/iter. Total: 0.0724 s/iter. ETA=0:05:51
[08/11 16:39:07] d2.evaluation.evaluator INFO: Inference done 214/5000. Dataloading: 0.0020 s/iter. Inference: 0.0687 s/iter. Eval: 0.0005 s/iter. Total: 0.0712 s/iter. ETA=0:05:40
[08/11 16:39:12] d2.evaluation.evaluator INFO: Inference done 286/5000. Dataloading: 0.0020 s/iter. Inference: 0.0663 s/iter. Eval: 0.0025 s/iter. Total: 0.0709 s/iter. ETA=0:05:34
[08/11 16:39:17] d2.evaluation.evaluator INFO: Inference done 358/5000. Dataloading: 0.0023 s/iter. Inference: 0.0662 s/iter. Eval: 0.0022 s/iter. Total: 0.0707 s/iter. ETA=0:05:28
[08/11 16:39:22] d2.evaluation.evaluator INFO: Inference done 429/5000. Dataloading: 0.0027 s/iter. Inference: 0.0659 s/iter. Eval: 0.0020 s/iter. Total: 0.0707 s/iter. ETA=0:05:23
[08/11 16:39:27] d2.evaluation.evaluator INFO: Inference done 497/5000. Dataloading: 0.0031 s/iter. Inference: 0.0662 s/iter. Eval: 0.0019 s/iter. Total: 0.0713 s/iter. ETA=0:05:20
[08/11 16:39:33] d2.evaluation.evaluator INFO: Inference done 571/5000. Dataloading: 0.0032 s/iter. Inference: 0.0658 s/iter. Eval: 0.0018 s/iter. Total: 0.0710 s/iter. ETA=0:05:14
[08/11 16:39:38] d2.evaluation.evaluator INFO: Inference done 656/5000. Dataloading: 0.0032 s/iter. Inference: 0.0645 s/iter. Eval: 0.0016 s/iter. Total: 0.0694 s/iter. ETA=0:05:01
[08/11 16:39:43] d2.evaluation.evaluator INFO: Inference done 746/5000. Dataloading: 0.0030 s/iter. Inference: 0.0632 s/iter. Eval: 0.0014 s/iter. Total: 0.0678 s/iter. ETA=0:04:48
[08/11 16:39:48] d2.evaluation.evaluator INFO: Inference done 834/5000. Dataloading: 0.0029 s/iter. Inference: 0.0621 s/iter. Eval: 0.0015 s/iter. Total: 0.0666 s/iter. ETA=0:04:37
[08/11 16:39:53] d2.evaluation.evaluator INFO: Inference done 921/5000. Dataloading: 0.0028 s/iter. Inference: 0.0614 s/iter. Eval: 0.0014 s/iter. Total: 0.0658 s/iter. ETA=0:04:28
[08/11 16:39:58] d2.evaluation.evaluator INFO: Inference done 1010/5000. Dataloading: 0.0027 s/iter. Inference: 0.0608 s/iter. Eval: 0.0013 s/iter. Total: 0.0649 s/iter. ETA=0:04:19
[08/11 16:40:03] d2.evaluation.evaluator INFO: Inference done 1099/5000. Dataloading: 0.0027 s/iter. Inference: 0.0602 s/iter. Eval: 0.0012 s/iter. Total: 0.0642 s/iter. ETA=0:04:10
[08/11 16:40:08] d2.evaluation.evaluator INFO: Inference done 1192/5000. Dataloading: 0.0026 s/iter. Inference: 0.0596 s/iter. Eval: 0.0012 s/iter. Total: 0.0634 s/iter. ETA=0:04:01
[08/11 16:40:13] d2.evaluation.evaluator INFO: Inference done 1284/5000. Dataloading: 0.0026 s/iter. Inference: 0.0590 s/iter. Eval: 0.0011 s/iter. Total: 0.0628 s/iter. ETA=0:03:53
[08/11 16:40:18] d2.evaluation.evaluator INFO: Inference done 1365/5000. Dataloading: 0.0026 s/iter. Inference: 0.0590 s/iter. Eval: 0.0011 s/iter. Total: 0.0628 s/iter. ETA=0:03:48
[08/11 16:40:23] d2.evaluation.evaluator INFO: Inference done 1454/5000. Dataloading: 0.0025 s/iter. Inference: 0.0587 s/iter. Eval: 0.0010 s/iter. Total: 0.0624 s/iter. ETA=0:03:41
[08/11 16:40:28] d2.evaluation.evaluator INFO: Inference done 1544/5000. Dataloading: 0.0026 s/iter. Inference: 0.0583 s/iter. Eval: 0.0010 s/iter. Total: 0.0620 s/iter. ETA=0:03:34
[08/11 16:40:33] d2.evaluation.evaluator INFO: Inference done 1633/5000. Dataloading: 0.0026 s/iter. Inference: 0.0581 s/iter. Eval: 0.0009 s/iter. Total: 0.0617 s/iter. ETA=0:03:27
[08/11 16:40:38] d2.evaluation.evaluator INFO: Inference done 1727/5000. Dataloading: 0.0025 s/iter. Inference: 0.0577 s/iter. Eval: 0.0009 s/iter. Total: 0.0613 s/iter. ETA=0:03:20
[08/11 16:40:43] d2.evaluation.evaluator INFO: Inference done 1813/5000. Dataloading: 0.0025 s/iter. Inference: 0.0576 s/iter. Eval: 0.0009 s/iter. Total: 0.0611 s/iter. ETA=0:03:14
[08/11 16:40:48] d2.evaluation.evaluator INFO: Inference done 1901/5000. Dataloading: 0.0025 s/iter. Inference: 0.0575 s/iter. Eval: 0.0009 s/iter. Total: 0.0609 s/iter. ETA=0:03:08
[08/11 16:40:53] d2.evaluation.evaluator INFO: Inference done 1983/5000. Dataloading: 0.0025 s/iter. Inference: 0.0575 s/iter. Eval: 0.0008 s/iter. Total: 0.0610 s/iter. ETA=0:03:03
[08/11 16:40:58] d2.evaluation.evaluator INFO: Inference done 2069/5000. Dataloading: 0.0025 s/iter. Inference: 0.0574 s/iter. Eval: 0.0008 s/iter. Total: 0.0608 s/iter. ETA=0:02:58
[08/11 16:41:03] d2.evaluation.evaluator INFO: Inference done 2151/5000. Dataloading: 0.0025 s/iter. Inference: 0.0575 s/iter. Eval: 0.0008 s/iter. Total: 0.0609 s/iter. ETA=0:02:53
[08/11 16:41:08] d2.evaluation.evaluator INFO: Inference done 2239/5000. Dataloading: 0.0025 s/iter. Inference: 0.0573 s/iter. Eval: 0.0008 s/iter. Total: 0.0607 s/iter. ETA=0:02:47
[08/11 16:41:13] d2.evaluation.evaluator INFO: Inference done 2320/5000. Dataloading: 0.0025 s/iter. Inference: 0.0573 s/iter. Eval: 0.0009 s/iter. Total: 0.0608 s/iter. ETA=0:02:42
[08/11 16:41:18] d2.evaluation.evaluator INFO: Inference done 2413/5000. Dataloading: 0.0024 s/iter. Inference: 0.0571 s/iter. Eval: 0.0009 s/iter. Total: 0.0605 s/iter. ETA=0:02:36
[08/11 16:41:23] d2.evaluation.evaluator INFO: Inference done 2503/5000. Dataloading: 0.0024 s/iter. Inference: 0.0570 s/iter. Eval: 0.0009 s/iter. Total: 0.0603 s/iter. ETA=0:02:30
[08/11 16:41:28] d2.evaluation.evaluator INFO: Inference done 2595/5000. Dataloading: 0.0024 s/iter. Inference: 0.0568 s/iter. Eval: 0.0008 s/iter. Total: 0.0601 s/iter. ETA=0:02:24
[08/11 16:41:33] d2.evaluation.evaluator INFO: Inference done 2675/5000. Dataloading: 0.0024 s/iter. Inference: 0.0569 s/iter. Eval: 0.0008 s/iter. Total: 0.0602 s/iter. ETA=0:02:20
[08/11 16:41:38] d2.evaluation.evaluator INFO: Inference done 2761/5000. Dataloading: 0.0024 s/iter. Inference: 0.0569 s/iter. Eval: 0.0008 s/iter. Total: 0.0602 s/iter. ETA=0:02:14
[08/11 16:41:43] d2.evaluation.evaluator INFO: Inference done 2853/5000. Dataloading: 0.0023 s/iter. Inference: 0.0567 s/iter. Eval: 0.0008 s/iter. Total: 0.0600 s/iter. ETA=0:02:08
[08/11 16:41:48] d2.evaluation.evaluator INFO: Inference done 2945/5000. Dataloading: 0.0023 s/iter. Inference: 0.0566 s/iter. Eval: 0.0008 s/iter. Total: 0.0598 s/iter. ETA=0:02:02
[08/11 16:41:53] d2.evaluation.evaluator INFO: Inference done 3030/5000. Dataloading: 0.0023 s/iter. Inference: 0.0566 s/iter. Eval: 0.0008 s/iter. Total: 0.0598 s/iter. ETA=0:01:57
[08/11 16:41:58] d2.evaluation.evaluator INFO: Inference done 3120/5000. Dataloading: 0.0023 s/iter. Inference: 0.0565 s/iter. Eval: 0.0008 s/iter. Total: 0.0597 s/iter. ETA=0:01:52
[08/11 16:42:03] d2.evaluation.evaluator INFO: Inference done 3207/5000. Dataloading: 0.0023 s/iter. Inference: 0.0565 s/iter. Eval: 0.0008 s/iter. Total: 0.0596 s/iter. ETA=0:01:46
[08/11 16:42:08] d2.evaluation.evaluator INFO: Inference done 3294/5000. Dataloading: 0.0023 s/iter. Inference: 0.0564 s/iter. Eval: 0.0007 s/iter. Total: 0.0596 s/iter. ETA=0:01:41
[08/11 16:42:13] d2.evaluation.evaluator INFO: Inference done 3382/5000. Dataloading: 0.0023 s/iter. Inference: 0.0563 s/iter. Eval: 0.0008 s/iter. Total: 0.0595 s/iter. ETA=0:01:36
[08/11 16:42:18] d2.evaluation.evaluator INFO: Inference done 3459/5000. Dataloading: 0.0023 s/iter. Inference: 0.0565 s/iter. Eval: 0.0008 s/iter. Total: 0.0596 s/iter. ETA=0:01:31
[08/11 16:42:23] d2.evaluation.evaluator INFO: Inference done 3542/5000. Dataloading: 0.0023 s/iter. Inference: 0.0565 s/iter. Eval: 0.0008 s/iter. Total: 0.0597 s/iter. ETA=0:01:26
[08/11 16:42:28] d2.evaluation.evaluator INFO: Inference done 3629/5000. Dataloading: 0.0023 s/iter. Inference: 0.0565 s/iter. Eval: 0.0008 s/iter. Total: 0.0596 s/iter. ETA=0:01:21
[08/11 16:42:34] d2.evaluation.evaluator INFO: Inference done 3714/5000. Dataloading: 0.0023 s/iter. Inference: 0.0565 s/iter. Eval: 0.0008 s/iter. Total: 0.0596 s/iter. ETA=0:01:16
[08/11 16:42:39] d2.evaluation.evaluator INFO: Inference done 3797/5000. Dataloading: 0.0023 s/iter. Inference: 0.0565 s/iter. Eval: 0.0008 s/iter. Total: 0.0596 s/iter. ETA=0:01:11
[08/11 16:42:44] d2.evaluation.evaluator INFO: Inference done 3881/5000. Dataloading: 0.0023 s/iter. Inference: 0.0565 s/iter. Eval: 0.0008 s/iter. Total: 0.0596 s/iter. ETA=0:01:06
[08/11 16:42:49] d2.evaluation.evaluator INFO: Inference done 3970/5000. Dataloading: 0.0023 s/iter. Inference: 0.0565 s/iter. Eval: 0.0007 s/iter. Total: 0.0596 s/iter. ETA=0:01:01
[08/11 16:42:54] d2.evaluation.evaluator INFO: Inference done 4061/5000. Dataloading: 0.0023 s/iter. Inference: 0.0564 s/iter. Eval: 0.0007 s/iter. Total: 0.0595 s/iter. ETA=0:00:55
[08/11 16:42:59] d2.evaluation.evaluator INFO: Inference done 4142/5000. Dataloading: 0.0023 s/iter. Inference: 0.0564 s/iter. Eval: 0.0007 s/iter. Total: 0.0595 s/iter. ETA=0:00:51
[08/11 16:43:04] d2.evaluation.evaluator INFO: Inference done 4231/5000. Dataloading: 0.0023 s/iter. Inference: 0.0564 s/iter. Eval: 0.0007 s/iter. Total: 0.0595 s/iter. ETA=0:00:45
[08/11 16:43:09] d2.evaluation.evaluator INFO: Inference done 4317/5000. Dataloading: 0.0023 s/iter. Inference: 0.0564 s/iter. Eval: 0.0007 s/iter. Total: 0.0594 s/iter. ETA=0:00:40
[08/11 16:43:14] d2.evaluation.evaluator INFO: Inference done 4400/5000. Dataloading: 0.0022 s/iter. Inference: 0.0564 s/iter. Eval: 0.0007 s/iter. Total: 0.0595 s/iter. ETA=0:00:35
[08/11 16:43:19] d2.evaluation.evaluator INFO: Inference done 4485/5000. Dataloading: 0.0022 s/iter. Inference: 0.0564 s/iter. Eval: 0.0007 s/iter. Total: 0.0595 s/iter. ETA=0:00:30
[08/11 16:43:24] d2.evaluation.evaluator INFO: Inference done 4568/5000. Dataloading: 0.0022 s/iter. Inference: 0.0564 s/iter. Eval: 0.0007 s/iter. Total: 0.0595 s/iter. ETA=0:00:25
[08/11 16:43:29] d2.evaluation.evaluator INFO: Inference done 4650/5000. Dataloading: 0.0022 s/iter. Inference: 0.0564 s/iter. Eval: 0.0008 s/iter. Total: 0.0595 s/iter. ETA=0:00:20
[08/11 16:43:34] d2.evaluation.evaluator INFO: Inference done 4742/5000. Dataloading: 0.0022 s/iter. Inference: 0.0563 s/iter. Eval: 0.0008 s/iter. Total: 0.0594 s/iter. ETA=0:00:15
[08/11 16:43:39] d2.evaluation.evaluator INFO: Inference done 4828/5000. Dataloading: 0.0022 s/iter. Inference: 0.0563 s/iter. Eval: 0.0008 s/iter. Total: 0.0594 s/iter. ETA=0:00:10
[08/11 16:43:44] d2.evaluation.evaluator INFO: Inference done 4915/5000. Dataloading: 0.0022 s/iter. Inference: 0.0563 s/iter. Eval: 0.0007 s/iter. Total: 0.0594 s/iter. ETA=0:00:05
[08/11 16:43:49] d2.evaluation.evaluator INFO: Inference done 5000/5000. Dataloading: 0.0022 s/iter. Inference: 0.0563 s/iter. Eval: 0.0007 s/iter. Total: 0.0594 s/iter. ETA=0:00:00
[08/11 16:43:49] d2.evaluation.evaluator INFO: Total inference time: 0:04:56.620905 (0.059384 s / iter per device, on 1 devices)
[08/11 16:43:49] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:04:41 (0.056290 s / iter per device, on 1 devices)
[08/11 16:43:49] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[08/11 16:43:49] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[08/11 16:43:51] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[08/11 16:44:00] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 9.54 seconds.
[08/11 16:44:01] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[08/11 16:44:02] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 1.09 seconds.
[08/11 16:44:02] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 42.002 | 62.321 | 44.295 | 20.696 | 45.885 | 61.063 |
[08/11 16:44:02] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 52.941 | bicycle      | 31.574 | car            | 39.982 |
| motorcycle    | 46.314 | airplane     | 67.391 | bus            | 65.680 |
| train         | 66.184 | truck        | 36.818 | boat           | 23.971 |
| traffic light | 21.146 | fire hydrant | 66.380 | stop sign      | 61.750 |
| parking meter | 41.774 | bench        | 26.374 | bird           | 34.684 |
| cat           | 75.427 | dog          | 66.905 | horse          | 62.336 |
| sheep         | 54.376 | cow          | 56.933 | elephant       | 67.860 |
| bear          | 74.521 | zebra        | 71.895 | giraffe        | 71.769 |
| backpack      | 15.796 | umbrella     | 40.636 | handbag        | 14.816 |
| tie           | 30.818 | suitcase     | 40.969 | frisbee        | 58.635 |
| skis          | 26.965 | snowboard    | 39.070 | sports ball    | 38.658 |
| kite          | 38.203 | baseball bat | 32.995 | baseball glove | 32.866 |
| skateboard    | 56.150 | surfboard    | 37.980 | tennis racket  | 47.950 |
| bottle        | 31.310 | wine glass   | 31.983 | cup            | 39.111 |
| fork          | 37.064 | knife        | 16.022 | spoon          | 16.953 |
| bowl          | 40.731 | banana       | 22.808 | apple          | 21.307 |
| sandwich      | 38.657 | orange       | 28.690 | broccoli       | 23.051 |
| carrot        | 20.383 | hot dog      | 37.146 | pizza          | 53.216 |
| donut         | 42.891 | cake         | 39.236 | chair          | 27.551 |
| couch         | 48.083 | potted plant | 28.560 | bed            | 51.687 |
| dining table  | 32.195 | toilet       | 65.673 | tv             | 58.757 |
| laptop        | 64.872 | mouse        | 54.675 | remote         | 25.533 |
| keyboard      | 52.720 | cell phone   | 30.235 | microwave      | 58.383 |
| oven          | 39.769 | toaster      | 32.528 | sink           | 36.379 |
| refrigerator  | 61.886 | book         | 11.408 | clock          | 44.211 |
| vase          | 32.678 | scissors     | 33.255 | teddy bear     | 50.312 |
| hair drier    | 21.578 | toothbrush   | 19.167 |                |        |
[08/11 16:44:03] d2.evaluation.testing INFO: copypaste: Task: bbox
[08/11 16:44:03] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[08/11 16:44:03] d2.evaluation.testing INFO: copypaste: 42.0018,62.3211,44.2946,20.6955,45.8853,61.0626
[08/11 17:06:02] detectron2 INFO: Rank of current process: 0. World size: 1
[08/11 17:06:09] detectron2 INFO: Environment info:
----------------------  ------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/comp_robot/rentianhe/code/detectron2/detectron2
Compiler                GCC 9.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   8.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3,4,5,6,7     A100-SXM4-40GB (arch=8.0)
Driver version          450.80.02
CUDA_HOME               /comp_robot/liushilong/software/cuda-11.1/
Pillow                  9.2.0
torchvision             0.13.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     Not found
----------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[08/11 17:06:09] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr_training.py', dist_url='tcp://127.0.0.1:54980', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[08/11 17:06:09] detectron2 INFO: Contents of args.config_file=configs/detr_training.py:
from ideadet.config import get_config

from .models.detr_r50 import model
from .common.coco_loader import dataloader

lr_multiplier = get_config("common/coco_schedule.py").lr_multiplier_1x
optimizer = get_config("common/optim.py").SGD
train = get_config("common/train.py").train

train.init_checkpoint = "/comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth"
train.output_dir = "./output"

[08/11 17:06:09] detectron2 INFO: Full config saved to ./output/config.yaml
[08/11 17:06:09] d2.utils.env INFO: Using a generated random seed 13585874
[08/11 17:06:13] detectron2 INFO: Model:
DETR(
  (transformer): DetrTransformer(
    (encoder): DetrTransformerEncoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (class_embed): Linear(in_features=256, out_features=81, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (query_embed): Embedding(100, 256)
  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
  (backbone): Joiner(
    (0): MaskedBackbone(
      (backbone): ResNet(
        (stem): BasicStem(
          (conv1): Conv2d(
            3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (res2): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv1): Conv2d(
              64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
        )
        (res3): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv1): Conv2d(
              256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
        )
        (res4): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
            (conv1): Conv2d(
              512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (4): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (5): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
        )
        (res5): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
            (conv1): Conv2d(
              1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[08/11 17:06:28] d2.data.datasets.coco INFO: Loading /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json takes 14.19 seconds.
[08/11 17:06:29] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json
[08/11 17:06:41] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[08/11 17:06:41] d2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[08/11 17:06:44] d2.data.common INFO: Serialized dataset takes 455.28 MiB
[08/11 17:06:49] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth ...
[08/11 17:06:49] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                   | Names in Checkpoint                                                                                        | Shapes                                          |
|:-------------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:------------------------------------------------|
| backbone.0.backbone.res2.0.conv1.*               | backbone.0.backbone.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| backbone.0.backbone.res2.0.conv2.*               | backbone.0.backbone.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.0.conv3.*               | backbone.0.backbone.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.0.shortcut.*            | backbone.0.backbone.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.1.conv1.*               | backbone.0.backbone.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.1.conv2.*               | backbone.0.backbone.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.1.conv3.*               | backbone.0.backbone.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.2.conv1.*               | backbone.0.backbone.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.2.conv2.*               | backbone.0.backbone.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.2.conv3.*               | backbone.0.backbone.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res3.0.conv1.*               | backbone.0.backbone.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| backbone.0.backbone.res3.0.conv2.*               | backbone.0.backbone.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.0.conv3.*               | backbone.0.backbone.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.0.shortcut.*            | backbone.0.backbone.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| backbone.0.backbone.res3.1.conv1.*               | backbone.0.backbone.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.1.conv2.*               | backbone.0.backbone.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.1.conv3.*               | backbone.0.backbone.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.2.conv1.*               | backbone.0.backbone.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.2.conv2.*               | backbone.0.backbone.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.2.conv3.*               | backbone.0.backbone.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.3.conv1.*               | backbone.0.backbone.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.3.conv2.*               | backbone.0.backbone.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.3.conv3.*               | backbone.0.backbone.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res4.0.conv1.*               | backbone.0.backbone.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| backbone.0.backbone.res4.0.conv2.*               | backbone.0.backbone.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.0.conv3.*               | backbone.0.backbone.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.0.shortcut.*            | backbone.0.backbone.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| backbone.0.backbone.res4.1.conv1.*               | backbone.0.backbone.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.1.conv2.*               | backbone.0.backbone.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.1.conv3.*               | backbone.0.backbone.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.2.conv1.*               | backbone.0.backbone.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.2.conv2.*               | backbone.0.backbone.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.2.conv3.*               | backbone.0.backbone.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.3.conv1.*               | backbone.0.backbone.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.3.conv2.*               | backbone.0.backbone.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.3.conv3.*               | backbone.0.backbone.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.4.conv1.*               | backbone.0.backbone.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.4.conv2.*               | backbone.0.backbone.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.4.conv3.*               | backbone.0.backbone.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.5.conv1.*               | backbone.0.backbone.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.5.conv2.*               | backbone.0.backbone.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.5.conv3.*               | backbone.0.backbone.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res5.0.conv1.*               | backbone.0.backbone.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| backbone.0.backbone.res5.0.conv2.*               | backbone.0.backbone.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.0.conv3.*               | backbone.0.backbone.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.0.shortcut.*            | backbone.0.backbone.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| backbone.0.backbone.res5.1.conv1.*               | backbone.0.backbone.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.1.conv2.*               | backbone.0.backbone.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.1.conv3.*               | backbone.0.backbone.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.2.conv1.*               | backbone.0.backbone.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.2.conv2.*               | backbone.0.backbone.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.2.conv3.*               | backbone.0.backbone.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.stem.conv1.*                 | backbone.0.backbone.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}           | (64,) (64,) (64,) (64,) (64,3,7,7)              |
| bbox_embed.layers.0.*                            | bbox_embed.layers.0.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.1.*                            | bbox_embed.layers.1.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.2.*                            | bbox_embed.layers.2.{bias,weight}                                                                          | (4,) (4,256)                                    |
| class_embed.*                                    | class_embed.{bias,weight}                                                                                  | (81,) (81,256)                                  |
| input_proj.*                                     | input_proj.{bias,weight}                                                                                   | (256,) (256,2048,1,1)                           |
| query_embed.weight                               | query_embed.weight                                                                                         | (100, 256)                                      |
| transformer.decoder.layers.0.attentions.0.attn.* | transformer.decoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.attentions.1.attn.* | transformer.decoder.layers.0.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.ffns.0.layers.0.0.* | transformer.decoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.0.ffns.0.layers.1.*   | transformer.decoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.0.norms.0.*           | transformer.decoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.1.*           | transformer.decoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.2.*           | transformer.decoder.layers.0.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.attentions.0.attn.* | transformer.decoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.attentions.1.attn.* | transformer.decoder.layers.1.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.ffns.0.layers.0.0.* | transformer.decoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.1.ffns.0.layers.1.*   | transformer.decoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.1.norms.0.*           | transformer.decoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.1.*           | transformer.decoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.2.*           | transformer.decoder.layers.1.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.attentions.0.attn.* | transformer.decoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.attentions.1.attn.* | transformer.decoder.layers.2.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.ffns.0.layers.0.0.* | transformer.decoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.2.ffns.0.layers.1.*   | transformer.decoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.2.norms.0.*           | transformer.decoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.1.*           | transformer.decoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.2.*           | transformer.decoder.layers.2.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.attentions.0.attn.* | transformer.decoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.attentions.1.attn.* | transformer.decoder.layers.3.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.ffns.0.layers.0.0.* | transformer.decoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.3.ffns.0.layers.1.*   | transformer.decoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.3.norms.0.*           | transformer.decoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.1.*           | transformer.decoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.2.*           | transformer.decoder.layers.3.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.attentions.0.attn.* | transformer.decoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.attentions.1.attn.* | transformer.decoder.layers.4.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.ffns.0.layers.0.0.* | transformer.decoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.4.ffns.0.layers.1.*   | transformer.decoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.4.norms.0.*           | transformer.decoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.1.*           | transformer.decoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.2.*           | transformer.decoder.layers.4.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.attentions.0.attn.* | transformer.decoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.attentions.1.attn.* | transformer.decoder.layers.5.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.ffns.0.layers.0.0.* | transformer.decoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.5.ffns.0.layers.1.*   | transformer.decoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.5.norms.0.*           | transformer.decoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.1.*           | transformer.decoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.2.*           | transformer.decoder.layers.5.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.post_norm_layer.*            | transformer.decoder.post_norm_layer.{bias,weight}                                                          | (256,) (256,)                                   |
| transformer.encoder.layers.0.attentions.0.attn.* | transformer.encoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.0.ffns.0.layers.0.0.* | transformer.encoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.0.ffns.0.layers.1.*   | transformer.encoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.0.norms.0.*           | transformer.encoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.0.norms.1.*           | transformer.encoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.attentions.0.attn.* | transformer.encoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.1.ffns.0.layers.0.0.* | transformer.encoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.1.ffns.0.layers.1.*   | transformer.encoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.1.norms.0.*           | transformer.encoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.norms.1.*           | transformer.encoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.attentions.0.attn.* | transformer.encoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.2.ffns.0.layers.0.0.* | transformer.encoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.2.ffns.0.layers.1.*   | transformer.encoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.2.norms.0.*           | transformer.encoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.norms.1.*           | transformer.encoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.attentions.0.attn.* | transformer.encoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.3.ffns.0.layers.0.0.* | transformer.encoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.3.ffns.0.layers.1.*   | transformer.encoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.3.norms.0.*           | transformer.encoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.norms.1.*           | transformer.encoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.attentions.0.attn.* | transformer.encoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.4.ffns.0.layers.0.0.* | transformer.encoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.4.ffns.0.layers.1.*   | transformer.encoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.4.norms.0.*           | transformer.encoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.norms.1.*           | transformer.encoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.attentions.0.attn.* | transformer.encoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.5.ffns.0.layers.0.0.* | transformer.encoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.5.ffns.0.layers.1.*   | transformer.encoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.5.norms.0.*           | transformer.encoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.norms.1.*           | transformer.encoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
[08/11 17:06:51] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[08/11 17:06:51] d2.engine.train_loop INFO: Starting training from iteration 0
[08/11 17:07:14] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/comp_robot/rentianhe/code/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/comp_robot/rentianhe/code/detectron2/detectron2/engine/train_loop.py", line 274, in run_step
    loss_dict = self.model(data)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/projects/DETR/modeling/detr.py", line 99, in forward
    hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/projects/DETR/modeling/transformer.py", line 172, in forward
    query_key_padding_mask=mask,
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/projects/DETR/modeling/transformer.py", line 63, in forward
    **kwargs,
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/layers/transformer.py", line 116, in forward
    **kwargs,
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/layers/attention.py", line 83, in forward
    key_padding_mask=key_padding_mask,
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/activation.py", line 1160, in forward
    attn_mask=attn_mask, average_attn_weights=average_attn_weights)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/functional.py", line 5179, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/functional.py", line 4858, in _scaled_dot_product_attention
    attn = dropout(attn, p=dropout_p)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 478.00 MiB (GPU 0; 39.59 GiB total capacity; 16.97 GiB already allocated; 279.19 MiB free; 17.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[08/11 17:07:14] d2.engine.hooks INFO: Total training time: 0:00:01 (0:00:00 on hooks)
[08/11 17:07:14] d2.utils.events INFO:  iter: 2  total_loss: 59.26  loss_ce: 0.3314  loss_bbox: 0.2333  loss_giou: 0.5461  loss_ce_0: 0.3962  loss_bbox_0: 0.2462  loss_giou_0: 0.5969  loss_ce_1: 0.3546  loss_bbox_1: 0.2436  loss_giou_1: 0.5763  loss_ce_2: 0.3333  loss_bbox_2: 0.2382  loss_giou_2: 0.5699  loss_ce_3: 0.3263  loss_bbox_3: 0.2418  loss_giou_3: 0.5498  loss_ce_4: 0.3285  loss_bbox_4: 0.2366  loss_giou_4: 0.5455  data_time: 0.3891  lr: 3.998e-05  max_mem: 17731M
[08/11 17:09:27] detectron2 INFO: Rank of current process: 0. World size: 1
[08/11 17:09:37] detectron2 INFO: Environment info:
----------------------  ------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/comp_robot/rentianhe/code/detectron2/detectron2
Compiler                GCC 9.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   8.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   A100-SXM4-40GB (arch=8.0)
Driver version          450.80.02
CUDA_HOME               /comp_robot/liushilong/software/cuda-11.1/
Pillow                  9.2.0
torchvision             0.13.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     Not found
----------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[08/11 17:09:37] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr_training.py', dist_url='tcp://127.0.0.1:54980', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[08/11 17:09:37] detectron2 INFO: Contents of args.config_file=configs/detr_training.py:
from ideadet.config import get_config

from .models.detr_r50 import model
from .common.coco_loader import dataloader

lr_multiplier = get_config("common/coco_schedule.py").lr_multiplier_1x
optimizer = get_config("common/optim.py").SGD
train = get_config("common/train.py").train

train.init_checkpoint = "/comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth"
train.output_dir = "./output"

[08/11 17:09:37] detectron2 INFO: Full config saved to ./output/config.yaml
[08/11 17:09:37] d2.utils.env INFO: Using a generated random seed 41588521
[08/11 17:09:42] detectron2 INFO: Model:
DETR(
  (transformer): DetrTransformer(
    (encoder): DetrTransformerEncoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (class_embed): Linear(in_features=256, out_features=81, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (query_embed): Embedding(100, 256)
  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
  (backbone): Joiner(
    (0): MaskedBackbone(
      (backbone): ResNet(
        (stem): BasicStem(
          (conv1): Conv2d(
            3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (res2): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv1): Conv2d(
              64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
        )
        (res3): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv1): Conv2d(
              256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
        )
        (res4): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
            (conv1): Conv2d(
              512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (4): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (5): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
        )
        (res5): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
            (conv1): Conv2d(
              1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[08/11 17:09:58] d2.data.datasets.coco INFO: Loading /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json takes 15.74 seconds.
[08/11 17:09:58] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json
[08/11 17:10:11] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[08/11 17:10:11] d2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[08/11 17:10:14] d2.data.common INFO: Serialized dataset takes 455.28 MiB
[08/11 17:10:18] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth ...
[08/11 17:10:18] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                   | Names in Checkpoint                                                                                        | Shapes                                          |
|:-------------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:------------------------------------------------|
| backbone.0.backbone.res2.0.conv1.*               | backbone.0.backbone.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| backbone.0.backbone.res2.0.conv2.*               | backbone.0.backbone.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.0.conv3.*               | backbone.0.backbone.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.0.shortcut.*            | backbone.0.backbone.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.1.conv1.*               | backbone.0.backbone.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.1.conv2.*               | backbone.0.backbone.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.1.conv3.*               | backbone.0.backbone.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.2.conv1.*               | backbone.0.backbone.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.2.conv2.*               | backbone.0.backbone.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.2.conv3.*               | backbone.0.backbone.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res3.0.conv1.*               | backbone.0.backbone.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| backbone.0.backbone.res3.0.conv2.*               | backbone.0.backbone.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.0.conv3.*               | backbone.0.backbone.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.0.shortcut.*            | backbone.0.backbone.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| backbone.0.backbone.res3.1.conv1.*               | backbone.0.backbone.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.1.conv2.*               | backbone.0.backbone.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.1.conv3.*               | backbone.0.backbone.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.2.conv1.*               | backbone.0.backbone.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.2.conv2.*               | backbone.0.backbone.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.2.conv3.*               | backbone.0.backbone.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.3.conv1.*               | backbone.0.backbone.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.3.conv2.*               | backbone.0.backbone.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.3.conv3.*               | backbone.0.backbone.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res4.0.conv1.*               | backbone.0.backbone.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| backbone.0.backbone.res4.0.conv2.*               | backbone.0.backbone.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.0.conv3.*               | backbone.0.backbone.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.0.shortcut.*            | backbone.0.backbone.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| backbone.0.backbone.res4.1.conv1.*               | backbone.0.backbone.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.1.conv2.*               | backbone.0.backbone.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.1.conv3.*               | backbone.0.backbone.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.2.conv1.*               | backbone.0.backbone.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.2.conv2.*               | backbone.0.backbone.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.2.conv3.*               | backbone.0.backbone.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.3.conv1.*               | backbone.0.backbone.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.3.conv2.*               | backbone.0.backbone.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.3.conv3.*               | backbone.0.backbone.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.4.conv1.*               | backbone.0.backbone.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.4.conv2.*               | backbone.0.backbone.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.4.conv3.*               | backbone.0.backbone.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.5.conv1.*               | backbone.0.backbone.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.5.conv2.*               | backbone.0.backbone.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.5.conv3.*               | backbone.0.backbone.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res5.0.conv1.*               | backbone.0.backbone.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| backbone.0.backbone.res5.0.conv2.*               | backbone.0.backbone.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.0.conv3.*               | backbone.0.backbone.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.0.shortcut.*            | backbone.0.backbone.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| backbone.0.backbone.res5.1.conv1.*               | backbone.0.backbone.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.1.conv2.*               | backbone.0.backbone.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.1.conv3.*               | backbone.0.backbone.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.2.conv1.*               | backbone.0.backbone.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.2.conv2.*               | backbone.0.backbone.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.2.conv3.*               | backbone.0.backbone.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.stem.conv1.*                 | backbone.0.backbone.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}           | (64,) (64,) (64,) (64,) (64,3,7,7)              |
| bbox_embed.layers.0.*                            | bbox_embed.layers.0.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.1.*                            | bbox_embed.layers.1.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.2.*                            | bbox_embed.layers.2.{bias,weight}                                                                          | (4,) (4,256)                                    |
| class_embed.*                                    | class_embed.{bias,weight}                                                                                  | (81,) (81,256)                                  |
| input_proj.*                                     | input_proj.{bias,weight}                                                                                   | (256,) (256,2048,1,1)                           |
| query_embed.weight                               | query_embed.weight                                                                                         | (100, 256)                                      |
| transformer.decoder.layers.0.attentions.0.attn.* | transformer.decoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.attentions.1.attn.* | transformer.decoder.layers.0.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.ffns.0.layers.0.0.* | transformer.decoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.0.ffns.0.layers.1.*   | transformer.decoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.0.norms.0.*           | transformer.decoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.1.*           | transformer.decoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.2.*           | transformer.decoder.layers.0.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.attentions.0.attn.* | transformer.decoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.attentions.1.attn.* | transformer.decoder.layers.1.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.ffns.0.layers.0.0.* | transformer.decoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.1.ffns.0.layers.1.*   | transformer.decoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.1.norms.0.*           | transformer.decoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.1.*           | transformer.decoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.2.*           | transformer.decoder.layers.1.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.attentions.0.attn.* | transformer.decoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.attentions.1.attn.* | transformer.decoder.layers.2.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.ffns.0.layers.0.0.* | transformer.decoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.2.ffns.0.layers.1.*   | transformer.decoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.2.norms.0.*           | transformer.decoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.1.*           | transformer.decoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.2.*           | transformer.decoder.layers.2.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.attentions.0.attn.* | transformer.decoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.attentions.1.attn.* | transformer.decoder.layers.3.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.ffns.0.layers.0.0.* | transformer.decoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.3.ffns.0.layers.1.*   | transformer.decoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.3.norms.0.*           | transformer.decoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.1.*           | transformer.decoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.2.*           | transformer.decoder.layers.3.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.attentions.0.attn.* | transformer.decoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.attentions.1.attn.* | transformer.decoder.layers.4.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.ffns.0.layers.0.0.* | transformer.decoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.4.ffns.0.layers.1.*   | transformer.decoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.4.norms.0.*           | transformer.decoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.1.*           | transformer.decoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.2.*           | transformer.decoder.layers.4.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.attentions.0.attn.* | transformer.decoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.attentions.1.attn.* | transformer.decoder.layers.5.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.ffns.0.layers.0.0.* | transformer.decoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.5.ffns.0.layers.1.*   | transformer.decoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.5.norms.0.*           | transformer.decoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.1.*           | transformer.decoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.2.*           | transformer.decoder.layers.5.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.post_norm_layer.*            | transformer.decoder.post_norm_layer.{bias,weight}                                                          | (256,) (256,)                                   |
| transformer.encoder.layers.0.attentions.0.attn.* | transformer.encoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.0.ffns.0.layers.0.0.* | transformer.encoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.0.ffns.0.layers.1.*   | transformer.encoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.0.norms.0.*           | transformer.encoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.0.norms.1.*           | transformer.encoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.attentions.0.attn.* | transformer.encoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.1.ffns.0.layers.0.0.* | transformer.encoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.1.ffns.0.layers.1.*   | transformer.encoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.1.norms.0.*           | transformer.encoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.norms.1.*           | transformer.encoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.attentions.0.attn.* | transformer.encoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.2.ffns.0.layers.0.0.* | transformer.encoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.2.ffns.0.layers.1.*   | transformer.encoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.2.norms.0.*           | transformer.encoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.norms.1.*           | transformer.encoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.attentions.0.attn.* | transformer.encoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.3.ffns.0.layers.0.0.* | transformer.encoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.3.ffns.0.layers.1.*   | transformer.encoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.3.norms.0.*           | transformer.encoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.norms.1.*           | transformer.encoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.attentions.0.attn.* | transformer.encoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.4.ffns.0.layers.0.0.* | transformer.encoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.4.ffns.0.layers.1.*   | transformer.encoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.4.norms.0.*           | transformer.encoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.norms.1.*           | transformer.encoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.attentions.0.attn.* | transformer.encoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.5.ffns.0.layers.0.0.* | transformer.encoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.5.ffns.0.layers.1.*   | transformer.encoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.5.norms.0.*           | transformer.encoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.norms.1.*           | transformer.encoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
[08/11 17:10:18] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[08/11 17:10:18] d2.engine.train_loop INFO: Starting training from iteration 0
[08/11 17:10:37] d2.utils.events INFO:  eta: 14:47:53  iter: 19  total_loss: 82.22  loss_ce: 0.4338  loss_bbox: 0.4967  loss_giou: 1.381  loss_ce_0: 0.5165  loss_bbox_0: 0.4883  loss_giou_0: 1.384  loss_ce_1: 0.5075  loss_bbox_1: 0.4887  loss_giou_1: 1.415  loss_ce_2: 0.4573  loss_bbox_2: 0.4983  loss_giou_2: 1.409  loss_ce_3: 0.4467  loss_bbox_3: 0.5019  loss_giou_3: 1.393  loss_ce_4: 0.4217  loss_bbox_4: 0.5016  loss_giou_4: 1.389  time: 0.5864  data_time: 0.1159  lr: 0.00039962  max_mem: 22331M
[08/11 17:10:51] d2.utils.events INFO:  eta: 15:01:16  iter: 39  total_loss: 181.8  loss_ce: 2.678  loss_bbox: 1.888  loss_giou: 1.881  loss_ce_0: 2.763  loss_bbox_0: 1.838  loss_giou_0: 1.901  loss_ce_1: 2.751  loss_bbox_1: 1.886  loss_giou_1: 1.861  loss_ce_2: 2.67  loss_bbox_2: 1.973  loss_giou_2: 1.877  loss_ce_3: 2.751  loss_bbox_3: 1.886  loss_giou_3: 1.871  loss_ce_4: 2.645  loss_bbox_4: 1.908  loss_giou_4: 1.866  time: 0.6382  data_time: 0.0942  lr: 0.00079922  max_mem: 28220M
[08/11 17:10:57] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/comp_robot/rentianhe/code/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/comp_robot/rentianhe/code/detectron2/detectron2/engine/train_loop.py", line 274, in run_step
    loss_dict = self.model(data)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/projects/DETR/modeling/detr.py", line 110, in forward
    loss_dict = self.criterion(output, targets)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/modeling/criterion/criterion.py", line 186, in forward
    indices = self.matcher(outputs_without_aux, targets)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/modeling/matcher/matcher.py", line 91, in forward
    cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/layers/box_ops.py", line 50, in generalized_box_iou
    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()
AssertionError
[08/11 17:10:57] d2.engine.hooks INFO: Overall training speed: 47 iterations in 0:00:30 (0.6533 s / it)
[08/11 17:10:57] d2.engine.hooks INFO: Total training time: 0:00:30 (0:00:00 on hooks)
[08/11 17:10:57] d2.utils.events INFO:  eta: 15:01:12  iter: 49  total_loss: 173.1  loss_ce: 2.388  loss_bbox: 1.663  loss_giou: 1.849  loss_ce_0: 2.487  loss_bbox_0: 1.717  loss_giou_0: 1.846  loss_ce_1: 2.453  loss_bbox_1: 1.714  loss_giou_1: 1.803  loss_ce_2: 2.421  loss_bbox_2: 1.706  loss_giou_2: 1.818  loss_ce_3: 2.355  loss_bbox_3: 1.639  loss_giou_3: 1.815  loss_ce_4: 2.366  loss_bbox_4: 1.616  loss_giou_4: 1.813  time: 0.6486  data_time: 0.0931  lr: 0.00097904  max_mem: 30188M
[08/11 17:19:13] detectron2 INFO: Rank of current process: 0. World size: 1
[08/11 17:19:18] detectron2 INFO: Environment info:
----------------------  ------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/comp_robot/rentianhe/code/detectron2/detectron2
Compiler                GCC 9.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   8.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   A100-SXM4-40GB (arch=8.0)
Driver version          450.80.02
CUDA_HOME               /comp_robot/liushilong/software/cuda-11.1/
Pillow                  9.2.0
torchvision             0.13.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     Not found
----------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[08/11 17:19:18] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr_training.py', dist_url='tcp://127.0.0.1:54980', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[08/11 17:19:18] detectron2 INFO: Contents of args.config_file=configs/detr_training.py:
from ideadet.config import get_config

from .models.detr_r50 import model
from .common.coco_loader import dataloader

lr_multiplier = get_config("common/coco_schedule.py").lr_multiplier_1x
optimizer = get_config("common/optim.py").SGD
train = get_config("common/train.py").train

train.init_checkpoint = "/comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth"
train.output_dir = "./output"

[08/11 17:19:18] detectron2 INFO: Full config saved to ./output/config.yaml
[08/11 17:19:18] d2.utils.env INFO: Using a generated random seed 22561749
[08/11 17:19:22] detectron2 INFO: Model:
DETR(
  (transformer): DetrTransformer(
    (encoder): DetrTransformerEncoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (class_embed): Linear(in_features=256, out_features=81, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (query_embed): Embedding(100, 256)
  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
  (backbone): Joiner(
    (0): MaskedBackbone(
      (backbone): ResNet(
        (stem): BasicStem(
          (conv1): Conv2d(
            3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (res2): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv1): Conv2d(
              64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
        )
        (res3): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv1): Conv2d(
              256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
        )
        (res4): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
            (conv1): Conv2d(
              512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (4): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (5): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
        )
        (res5): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
            (conv1): Conv2d(
              1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[08/11 17:19:38] d2.data.datasets.coco INFO: Loading /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json takes 15.26 seconds.
[08/11 17:19:39] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json
[08/11 17:19:47] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[08/11 17:19:51] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[08/11 17:19:51] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[08/11 17:19:54] d2.data.common INFO: Serialized dataset takes 455.13 MiB
[08/11 17:19:57] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth ...
[08/11 17:19:58] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                   | Names in Checkpoint                                                                                        | Shapes                                          |
|:-------------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:------------------------------------------------|
| backbone.0.backbone.res2.0.conv1.*               | backbone.0.backbone.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| backbone.0.backbone.res2.0.conv2.*               | backbone.0.backbone.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.0.conv3.*               | backbone.0.backbone.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.0.shortcut.*            | backbone.0.backbone.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.1.conv1.*               | backbone.0.backbone.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.1.conv2.*               | backbone.0.backbone.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.1.conv3.*               | backbone.0.backbone.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.2.conv1.*               | backbone.0.backbone.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.2.conv2.*               | backbone.0.backbone.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.2.conv3.*               | backbone.0.backbone.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res3.0.conv1.*               | backbone.0.backbone.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| backbone.0.backbone.res3.0.conv2.*               | backbone.0.backbone.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.0.conv3.*               | backbone.0.backbone.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.0.shortcut.*            | backbone.0.backbone.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| backbone.0.backbone.res3.1.conv1.*               | backbone.0.backbone.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.1.conv2.*               | backbone.0.backbone.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.1.conv3.*               | backbone.0.backbone.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.2.conv1.*               | backbone.0.backbone.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.2.conv2.*               | backbone.0.backbone.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.2.conv3.*               | backbone.0.backbone.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.3.conv1.*               | backbone.0.backbone.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.3.conv2.*               | backbone.0.backbone.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.3.conv3.*               | backbone.0.backbone.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res4.0.conv1.*               | backbone.0.backbone.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| backbone.0.backbone.res4.0.conv2.*               | backbone.0.backbone.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.0.conv3.*               | backbone.0.backbone.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.0.shortcut.*            | backbone.0.backbone.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| backbone.0.backbone.res4.1.conv1.*               | backbone.0.backbone.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.1.conv2.*               | backbone.0.backbone.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.1.conv3.*               | backbone.0.backbone.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.2.conv1.*               | backbone.0.backbone.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.2.conv2.*               | backbone.0.backbone.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.2.conv3.*               | backbone.0.backbone.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.3.conv1.*               | backbone.0.backbone.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.3.conv2.*               | backbone.0.backbone.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.3.conv3.*               | backbone.0.backbone.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.4.conv1.*               | backbone.0.backbone.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.4.conv2.*               | backbone.0.backbone.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.4.conv3.*               | backbone.0.backbone.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.5.conv1.*               | backbone.0.backbone.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.5.conv2.*               | backbone.0.backbone.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.5.conv3.*               | backbone.0.backbone.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res5.0.conv1.*               | backbone.0.backbone.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| backbone.0.backbone.res5.0.conv2.*               | backbone.0.backbone.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.0.conv3.*               | backbone.0.backbone.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.0.shortcut.*            | backbone.0.backbone.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| backbone.0.backbone.res5.1.conv1.*               | backbone.0.backbone.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.1.conv2.*               | backbone.0.backbone.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.1.conv3.*               | backbone.0.backbone.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.2.conv1.*               | backbone.0.backbone.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.2.conv2.*               | backbone.0.backbone.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.2.conv3.*               | backbone.0.backbone.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.stem.conv1.*                 | backbone.0.backbone.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}           | (64,) (64,) (64,) (64,) (64,3,7,7)              |
| bbox_embed.layers.0.*                            | bbox_embed.layers.0.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.1.*                            | bbox_embed.layers.1.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.2.*                            | bbox_embed.layers.2.{bias,weight}                                                                          | (4,) (4,256)                                    |
| class_embed.*                                    | class_embed.{bias,weight}                                                                                  | (81,) (81,256)                                  |
| input_proj.*                                     | input_proj.{bias,weight}                                                                                   | (256,) (256,2048,1,1)                           |
| query_embed.weight                               | query_embed.weight                                                                                         | (100, 256)                                      |
| transformer.decoder.layers.0.attentions.0.attn.* | transformer.decoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.attentions.1.attn.* | transformer.decoder.layers.0.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.ffns.0.layers.0.0.* | transformer.decoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.0.ffns.0.layers.1.*   | transformer.decoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.0.norms.0.*           | transformer.decoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.1.*           | transformer.decoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.2.*           | transformer.decoder.layers.0.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.attentions.0.attn.* | transformer.decoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.attentions.1.attn.* | transformer.decoder.layers.1.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.ffns.0.layers.0.0.* | transformer.decoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.1.ffns.0.layers.1.*   | transformer.decoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.1.norms.0.*           | transformer.decoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.1.*           | transformer.decoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.2.*           | transformer.decoder.layers.1.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.attentions.0.attn.* | transformer.decoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.attentions.1.attn.* | transformer.decoder.layers.2.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.ffns.0.layers.0.0.* | transformer.decoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.2.ffns.0.layers.1.*   | transformer.decoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.2.norms.0.*           | transformer.decoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.1.*           | transformer.decoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.2.*           | transformer.decoder.layers.2.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.attentions.0.attn.* | transformer.decoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.attentions.1.attn.* | transformer.decoder.layers.3.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.ffns.0.layers.0.0.* | transformer.decoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.3.ffns.0.layers.1.*   | transformer.decoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.3.norms.0.*           | transformer.decoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.1.*           | transformer.decoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.2.*           | transformer.decoder.layers.3.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.attentions.0.attn.* | transformer.decoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.attentions.1.attn.* | transformer.decoder.layers.4.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.ffns.0.layers.0.0.* | transformer.decoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.4.ffns.0.layers.1.*   | transformer.decoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.4.norms.0.*           | transformer.decoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.1.*           | transformer.decoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.2.*           | transformer.decoder.layers.4.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.attentions.0.attn.* | transformer.decoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.attentions.1.attn.* | transformer.decoder.layers.5.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.ffns.0.layers.0.0.* | transformer.decoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.5.ffns.0.layers.1.*   | transformer.decoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.5.norms.0.*           | transformer.decoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.1.*           | transformer.decoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.2.*           | transformer.decoder.layers.5.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.post_norm_layer.*            | transformer.decoder.post_norm_layer.{bias,weight}                                                          | (256,) (256,)                                   |
| transformer.encoder.layers.0.attentions.0.attn.* | transformer.encoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.0.ffns.0.layers.0.0.* | transformer.encoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.0.ffns.0.layers.1.*   | transformer.encoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.0.norms.0.*           | transformer.encoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.0.norms.1.*           | transformer.encoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.attentions.0.attn.* | transformer.encoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.1.ffns.0.layers.0.0.* | transformer.encoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.1.ffns.0.layers.1.*   | transformer.encoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.1.norms.0.*           | transformer.encoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.norms.1.*           | transformer.encoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.attentions.0.attn.* | transformer.encoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.2.ffns.0.layers.0.0.* | transformer.encoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.2.ffns.0.layers.1.*   | transformer.encoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.2.norms.0.*           | transformer.encoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.norms.1.*           | transformer.encoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.attentions.0.attn.* | transformer.encoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.3.ffns.0.layers.0.0.* | transformer.encoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.3.ffns.0.layers.1.*   | transformer.encoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.3.norms.0.*           | transformer.encoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.norms.1.*           | transformer.encoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.attentions.0.attn.* | transformer.encoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.4.ffns.0.layers.0.0.* | transformer.encoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.4.ffns.0.layers.1.*   | transformer.encoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.4.norms.0.*           | transformer.encoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.norms.1.*           | transformer.encoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.attentions.0.attn.* | transformer.encoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.5.ffns.0.layers.0.0.* | transformer.encoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.5.ffns.0.layers.1.*   | transformer.encoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.5.norms.0.*           | transformer.encoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.norms.1.*           | transformer.encoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
[08/11 17:19:58] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[08/11 17:19:58] d2.engine.train_loop INFO: Starting training from iteration 0
[08/11 17:20:20] d2.utils.events INFO:  eta: 16:34:52  iter: 19  total_loss: 75.51  loss_ce: 0.3904  loss_bbox: 0.4271  loss_giou: 1.118  loss_ce_0: 0.4498  loss_bbox_0: 0.4518  loss_giou_0: 1.128  loss_ce_1: 0.4073  loss_bbox_1: 0.4359  loss_giou_1: 1.136  loss_ce_2: 0.3626  loss_bbox_2: 0.4356  loss_giou_2: 1.133  loss_ce_3: 0.3439  loss_bbox_3: 0.431  loss_giou_3: 1.131  loss_ce_4: 0.3632  loss_bbox_4: 0.4262  loss_giou_4: 1.118  time: 0.7923  data_time: 0.1282  lr: 0.00039962  max_mem: 27957M
[08/11 17:20:32] d2.utils.events INFO:  eta: 16:05:25  iter: 39  total_loss: 190.3  loss_ce: 2.77  loss_bbox: 2.754  loss_giou: 2.656  loss_ce_0: 2.586  loss_bbox_0: 3.06  loss_giou_0: 2.646  loss_ce_1: 2.573  loss_bbox_1: 3.185  loss_giou_1: 2.491  loss_ce_2: 2.592  loss_bbox_2: 2.93  loss_giou_2: 2.551  loss_ce_3: 2.605  loss_bbox_3: 2.929  loss_giou_3: 2.677  loss_ce_4: 2.711  loss_bbox_4: 2.775  loss_giou_4: 2.68  time: 0.7103  data_time: 0.0999  lr: 0.00079922  max_mem: 28976M
[08/11 17:20:36] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/comp_robot/rentianhe/code/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/comp_robot/rentianhe/code/detectron2/detectron2/engine/train_loop.py", line 274, in run_step
    loss_dict = self.model(data)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/projects/DETR/modeling/detr.py", line 110, in forward
    loss_dict = self.criterion(output, targets)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/modeling/criterion/criterion.py", line 186, in forward
    indices = self.matcher(outputs_without_aux, targets)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/modeling/matcher/matcher.py", line 91, in forward
    cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/layers/box_ops.py", line 50, in generalized_box_iou
    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()
AssertionError
[08/11 17:20:36] d2.engine.hooks INFO: Overall training speed: 44 iterations in 0:00:30 (0.7002 s / it)
[08/11 17:20:36] d2.engine.hooks INFO: Total training time: 0:00:30 (0:00:00 on hooks)
[08/11 17:20:36] d2.utils.events INFO:  eta: 15:36:47  iter: 46  total_loss: 190.3  loss_ce: 2.585  loss_bbox: 3.588  loss_giou: 2.747  loss_ce_0: 2.458  loss_bbox_0: 3.708  loss_giou_0: 2.798  loss_ce_1: 2.511  loss_bbox_1: 3.627  loss_giou_1: 2.761  loss_ce_2: 2.455  loss_bbox_2: 3.621  loss_giou_2: 2.818  loss_ce_3: 2.496  loss_bbox_3: 3.571  loss_giou_3: 2.73  loss_ce_4: 2.484  loss_bbox_4: 3.559  loss_giou_4: 2.713  time: 0.6931  data_time: 0.0978  lr: 0.0009191  max_mem: 28976M
[08/11 17:25:27] detectron2 INFO: Rank of current process: 0. World size: 1
[08/11 17:25:36] detectron2 INFO: Environment info:
----------------------  ------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/comp_robot/rentianhe/code/detectron2/detectron2
Compiler                GCC 9.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   8.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   A100-SXM4-40GB (arch=8.0)
Driver version          450.80.02
CUDA_HOME               /comp_robot/liushilong/software/cuda-11.1/
Pillow                  9.2.0
torchvision             0.13.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     Not found
----------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[08/11 17:25:36] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr_training.py', dist_url='tcp://127.0.0.1:54980', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[08/11 17:25:36] detectron2 INFO: Contents of args.config_file=configs/detr_training.py:
from ideadet.config import get_config

from .models.detr_r50 import model
from .common.coco_loader import dataloader

lr_multiplier = get_config("common/coco_schedule.py").lr_multiplier_1x
optimizer = get_config("common/optim.py").SGD
train = get_config("common/train.py").train

train.init_checkpoint = "/comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth"
train.output_dir = "./output"

[08/11 17:25:36] detectron2 INFO: Full config saved to ./output/config.yaml
[08/11 17:25:36] d2.utils.env INFO: Using a generated random seed 41062936
[08/11 17:25:39] detectron2 INFO: Model:
DETR(
  (transformer): DetrTransformer(
    (encoder): DetrTransformerEncoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (class_embed): Linear(in_features=256, out_features=81, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (query_embed): Embedding(100, 256)
  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
  (backbone): Joiner(
    (0): MaskedBackbone(
      (backbone): ResNet(
        (stem): BasicStem(
          (conv1): Conv2d(
            3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (res2): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv1): Conv2d(
              64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
        )
        (res3): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv1): Conv2d(
              256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
        )
        (res4): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
            (conv1): Conv2d(
              512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (4): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (5): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
        )
        (res5): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
            (conv1): Conv2d(
              1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[08/11 17:25:55] d2.data.datasets.coco INFO: Loading /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json takes 15.48 seconds.
[08/11 17:25:56] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json
[08/11 17:26:05] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[08/11 17:26:09] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[08/11 17:26:09] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[08/11 17:26:13] d2.data.common INFO: Serialized dataset takes 455.13 MiB
[08/11 17:26:16] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth ...
[08/11 17:26:16] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                   | Names in Checkpoint                                                                                        | Shapes                                          |
|:-------------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:------------------------------------------------|
| backbone.0.backbone.res2.0.conv1.*               | backbone.0.backbone.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| backbone.0.backbone.res2.0.conv2.*               | backbone.0.backbone.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.0.conv3.*               | backbone.0.backbone.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.0.shortcut.*            | backbone.0.backbone.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.1.conv1.*               | backbone.0.backbone.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.1.conv2.*               | backbone.0.backbone.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.1.conv3.*               | backbone.0.backbone.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.2.conv1.*               | backbone.0.backbone.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.2.conv2.*               | backbone.0.backbone.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.2.conv3.*               | backbone.0.backbone.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res3.0.conv1.*               | backbone.0.backbone.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| backbone.0.backbone.res3.0.conv2.*               | backbone.0.backbone.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.0.conv3.*               | backbone.0.backbone.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.0.shortcut.*            | backbone.0.backbone.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| backbone.0.backbone.res3.1.conv1.*               | backbone.0.backbone.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.1.conv2.*               | backbone.0.backbone.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.1.conv3.*               | backbone.0.backbone.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.2.conv1.*               | backbone.0.backbone.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.2.conv2.*               | backbone.0.backbone.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.2.conv3.*               | backbone.0.backbone.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.3.conv1.*               | backbone.0.backbone.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.3.conv2.*               | backbone.0.backbone.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.3.conv3.*               | backbone.0.backbone.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res4.0.conv1.*               | backbone.0.backbone.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| backbone.0.backbone.res4.0.conv2.*               | backbone.0.backbone.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.0.conv3.*               | backbone.0.backbone.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.0.shortcut.*            | backbone.0.backbone.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| backbone.0.backbone.res4.1.conv1.*               | backbone.0.backbone.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.1.conv2.*               | backbone.0.backbone.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.1.conv3.*               | backbone.0.backbone.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.2.conv1.*               | backbone.0.backbone.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.2.conv2.*               | backbone.0.backbone.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.2.conv3.*               | backbone.0.backbone.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.3.conv1.*               | backbone.0.backbone.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.3.conv2.*               | backbone.0.backbone.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.3.conv3.*               | backbone.0.backbone.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.4.conv1.*               | backbone.0.backbone.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.4.conv2.*               | backbone.0.backbone.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.4.conv3.*               | backbone.0.backbone.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.5.conv1.*               | backbone.0.backbone.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.5.conv2.*               | backbone.0.backbone.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.5.conv3.*               | backbone.0.backbone.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res5.0.conv1.*               | backbone.0.backbone.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| backbone.0.backbone.res5.0.conv2.*               | backbone.0.backbone.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.0.conv3.*               | backbone.0.backbone.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.0.shortcut.*            | backbone.0.backbone.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| backbone.0.backbone.res5.1.conv1.*               | backbone.0.backbone.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.1.conv2.*               | backbone.0.backbone.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.1.conv3.*               | backbone.0.backbone.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.2.conv1.*               | backbone.0.backbone.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.2.conv2.*               | backbone.0.backbone.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.2.conv3.*               | backbone.0.backbone.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.stem.conv1.*                 | backbone.0.backbone.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}           | (64,) (64,) (64,) (64,) (64,3,7,7)              |
| bbox_embed.layers.0.*                            | bbox_embed.layers.0.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.1.*                            | bbox_embed.layers.1.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.2.*                            | bbox_embed.layers.2.{bias,weight}                                                                          | (4,) (4,256)                                    |
| class_embed.*                                    | class_embed.{bias,weight}                                                                                  | (81,) (81,256)                                  |
| input_proj.*                                     | input_proj.{bias,weight}                                                                                   | (256,) (256,2048,1,1)                           |
| query_embed.weight                               | query_embed.weight                                                                                         | (100, 256)                                      |
| transformer.decoder.layers.0.attentions.0.attn.* | transformer.decoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.attentions.1.attn.* | transformer.decoder.layers.0.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.ffns.0.layers.0.0.* | transformer.decoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.0.ffns.0.layers.1.*   | transformer.decoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.0.norms.0.*           | transformer.decoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.1.*           | transformer.decoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.2.*           | transformer.decoder.layers.0.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.attentions.0.attn.* | transformer.decoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.attentions.1.attn.* | transformer.decoder.layers.1.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.ffns.0.layers.0.0.* | transformer.decoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.1.ffns.0.layers.1.*   | transformer.decoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.1.norms.0.*           | transformer.decoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.1.*           | transformer.decoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.2.*           | transformer.decoder.layers.1.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.attentions.0.attn.* | transformer.decoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.attentions.1.attn.* | transformer.decoder.layers.2.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.ffns.0.layers.0.0.* | transformer.decoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.2.ffns.0.layers.1.*   | transformer.decoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.2.norms.0.*           | transformer.decoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.1.*           | transformer.decoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.2.*           | transformer.decoder.layers.2.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.attentions.0.attn.* | transformer.decoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.attentions.1.attn.* | transformer.decoder.layers.3.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.ffns.0.layers.0.0.* | transformer.decoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.3.ffns.0.layers.1.*   | transformer.decoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.3.norms.0.*           | transformer.decoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.1.*           | transformer.decoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.2.*           | transformer.decoder.layers.3.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.attentions.0.attn.* | transformer.decoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.attentions.1.attn.* | transformer.decoder.layers.4.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.ffns.0.layers.0.0.* | transformer.decoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.4.ffns.0.layers.1.*   | transformer.decoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.4.norms.0.*           | transformer.decoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.1.*           | transformer.decoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.2.*           | transformer.decoder.layers.4.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.attentions.0.attn.* | transformer.decoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.attentions.1.attn.* | transformer.decoder.layers.5.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.ffns.0.layers.0.0.* | transformer.decoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.5.ffns.0.layers.1.*   | transformer.decoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.5.norms.0.*           | transformer.decoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.1.*           | transformer.decoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.2.*           | transformer.decoder.layers.5.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.post_norm_layer.*            | transformer.decoder.post_norm_layer.{bias,weight}                                                          | (256,) (256,)                                   |
| transformer.encoder.layers.0.attentions.0.attn.* | transformer.encoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.0.ffns.0.layers.0.0.* | transformer.encoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.0.ffns.0.layers.1.*   | transformer.encoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.0.norms.0.*           | transformer.encoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.0.norms.1.*           | transformer.encoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.attentions.0.attn.* | transformer.encoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.1.ffns.0.layers.0.0.* | transformer.encoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.1.ffns.0.layers.1.*   | transformer.encoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.1.norms.0.*           | transformer.encoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.norms.1.*           | transformer.encoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.attentions.0.attn.* | transformer.encoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.2.ffns.0.layers.0.0.* | transformer.encoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.2.ffns.0.layers.1.*   | transformer.encoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.2.norms.0.*           | transformer.encoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.norms.1.*           | transformer.encoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.attentions.0.attn.* | transformer.encoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.3.ffns.0.layers.0.0.* | transformer.encoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.3.ffns.0.layers.1.*   | transformer.encoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.3.norms.0.*           | transformer.encoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.norms.1.*           | transformer.encoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.attentions.0.attn.* | transformer.encoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.4.ffns.0.layers.0.0.* | transformer.encoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.4.ffns.0.layers.1.*   | transformer.encoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.4.norms.0.*           | transformer.encoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.norms.1.*           | transformer.encoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.attentions.0.attn.* | transformer.encoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.5.ffns.0.layers.0.0.* | transformer.encoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.5.ffns.0.layers.1.*   | transformer.encoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.5.norms.0.*           | transformer.encoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.norms.1.*           | transformer.encoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
[08/11 17:26:16] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[08/11 17:26:16] d2.engine.train_loop INFO: Starting training from iteration 0
[08/11 17:26:36] d2.utils.events INFO:  eta: 15:28:33  iter: 19  total_loss: 98.67  loss_ce: 0.4793  loss_bbox: 0.4334  loss_giou: 1.365  loss_ce_0: 0.5466  loss_bbox_0: 0.4584  loss_giou_0: 1.335  loss_ce_1: 0.5028  loss_bbox_1: 0.4561  loss_giou_1: 1.368  loss_ce_2: 0.5162  loss_bbox_2: 0.4463  loss_giou_2: 1.368  loss_ce_3: 0.4646  loss_bbox_3: 0.4308  loss_giou_3: 1.365  loss_ce_4: 0.4533  loss_bbox_4: 0.4336  loss_giou_4: 1.37  time: 0.6629  data_time: 0.1306  lr: 0.00039962  max_mem: 23481M
[08/11 17:26:50] d2.utils.events INFO:  eta: 15:18:04  iter: 39  total_loss: 172.9  loss_ce: 2.243  loss_bbox: 1.806  loss_giou: 1.755  loss_ce_0: 2.359  loss_bbox_0: 1.885  loss_giou_0: 1.761  loss_ce_1: 2.277  loss_bbox_1: 1.851  loss_giou_1: 1.732  loss_ce_2: 2.26  loss_bbox_2: 1.874  loss_giou_2: 1.747  loss_ce_3: 2.231  loss_bbox_3: 1.811  loss_giou_3: 1.729  loss_ce_4: 2.243  loss_bbox_4: 1.837  loss_giou_4: 1.734  time: 0.6925  data_time: 0.0993  lr: 0.00079922  max_mem: 34819M
[08/11 17:27:03] d2.utils.events INFO:  eta: 15:23:46  iter: 59  total_loss: 166.1  loss_ce: 2.341  loss_bbox: 1.427  loss_giou: 1.759  loss_ce_0: 2.338  loss_bbox_0: 1.499  loss_giou_0: 1.701  loss_ce_1: 2.343  loss_bbox_1: 1.459  loss_giou_1: 1.701  loss_ce_2: 2.316  loss_bbox_2: 1.371  loss_giou_2: 1.723  loss_ce_3: 2.307  loss_bbox_3: 1.424  loss_giou_3: 1.746  loss_ce_4: 2.31  loss_bbox_4: 1.418  loss_giou_4: 1.744  time: 0.6687  data_time: 0.1025  lr: 0.0011988  max_mem: 34819M
[08/11 17:27:16] d2.utils.events INFO:  eta: 15:23:34  iter: 79  total_loss: 164.2  loss_ce: 2.165  loss_bbox: 1.306  loss_giou: 1.59  loss_ce_0: 2.175  loss_bbox_0: 1.351  loss_giou_0: 1.594  loss_ce_1: 2.176  loss_bbox_1: 1.334  loss_giou_1: 1.607  loss_ce_2: 2.188  loss_bbox_2: 1.329  loss_giou_2: 1.586  loss_ce_3: 2.187  loss_bbox_3: 1.313  loss_giou_3: 1.579  loss_ce_4: 2.158  loss_bbox_4: 1.322  loss_giou_4: 1.593  time: 0.6611  data_time: 0.1120  lr: 0.0015984  max_mem: 34819M
[08/11 17:27:29] d2.utils.events INFO:  eta: 15:47:03  iter: 99  total_loss: 167.5  loss_ce: 2.24  loss_bbox: 1.275  loss_giou: 1.679  loss_ce_0: 2.232  loss_bbox_0: 1.307  loss_giou_0: 1.687  loss_ce_1: 2.248  loss_bbox_1: 1.297  loss_giou_1: 1.675  loss_ce_2: 2.231  loss_bbox_2: 1.281  loss_giou_2: 1.679  loss_ce_3: 2.253  loss_bbox_3: 1.26  loss_giou_3: 1.667  loss_ce_4: 2.233  loss_bbox_4: 1.272  loss_giou_4: 1.672  time: 0.6666  data_time: 0.1313  lr: 0.001998  max_mem: 34819M
[08/11 17:27:42] d2.utils.events INFO:  eta: 15:32:42  iter: 119  total_loss: 169  loss_ce: 2.185  loss_bbox: 1.396  loss_giou: 1.647  loss_ce_0: 2.228  loss_bbox_0: 1.322  loss_giou_0: 1.646  loss_ce_1: 2.221  loss_bbox_1: 1.344  loss_giou_1: 1.673  loss_ce_2: 2.19  loss_bbox_2: 1.376  loss_giou_2: 1.662  loss_ce_3: 2.214  loss_bbox_3: 1.398  loss_giou_3: 1.664  loss_ce_4: 2.188  loss_bbox_4: 1.384  loss_giou_4: 1.657  time: 0.6560  data_time: 0.1011  lr: 0.0023976  max_mem: 34819M
[08/11 17:27:54] d2.utils.events INFO:  eta: 15:17:11  iter: 139  total_loss: 161.9  loss_ce: 2.006  loss_bbox: 1.373  loss_giou: 1.716  loss_ce_0: 2.072  loss_bbox_0: 1.368  loss_giou_0: 1.683  loss_ce_1: 2.065  loss_bbox_1: 1.359  loss_giou_1: 1.704  loss_ce_2: 2.065  loss_bbox_2: 1.391  loss_giou_2: 1.67  loss_ce_3: 2.038  loss_bbox_3: 1.368  loss_giou_3: 1.708  loss_ce_4: 2.028  loss_bbox_4: 1.365  loss_giou_4: 1.718  time: 0.6466  data_time: 0.1025  lr: 0.0027972  max_mem: 34819M
[08/11 17:28:02] d2.engine.hooks INFO: Overall training speed: 153 iterations in 0:01:38 (0.6408 s / it)
[08/11 17:28:02] d2.engine.hooks INFO: Total training time: 0:01:38 (0:00:00 on hooks)
[08/11 17:28:02] d2.utils.events INFO:  eta: 15:15:47  iter: 155  total_loss: 166.4  loss_ce: 2.106  loss_bbox: 1.365  loss_giou: 1.686  loss_ce_0: 2.175  loss_bbox_0: 1.336  loss_giou_0: 1.704  loss_ce_1: 2.165  loss_bbox_1: 1.3  loss_giou_1: 1.712  loss_ce_2: 2.174  loss_bbox_2: 1.37  loss_giou_2: 1.698  loss_ce_3: 2.165  loss_bbox_3: 1.351  loss_giou_3: 1.703  loss_ce_4: 2.124  loss_bbox_4: 1.349  loss_giou_4: 1.673  time: 0.6404  data_time: 0.0964  lr: 0.0030969  max_mem: 34819M
[08/11 17:28:27] detectron2 INFO: Rank of current process: 0. World size: 1
[08/11 17:28:34] detectron2 INFO: Environment info:
----------------------  ------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/comp_robot/rentianhe/code/detectron2/detectron2
Compiler                GCC 9.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   8.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   A100-SXM4-40GB (arch=8.0)
Driver version          450.80.02
CUDA_HOME               /comp_robot/liushilong/software/cuda-11.1/
Pillow                  9.2.0
torchvision             0.13.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     Not found
----------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[08/11 17:28:34] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr_training.py', dist_url='tcp://127.0.0.1:54980', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[08/11 17:28:34] detectron2 INFO: Contents of args.config_file=configs/detr_training.py:
from ideadet.config import get_config

from .models.detr_r50 import model
from .common.coco_loader import dataloader

lr_multiplier = get_config("common/coco_schedule.py").lr_multiplier_1x
optimizer = get_config("common/optim.py").SGD
train = get_config("common/train.py").train

train.init_checkpoint = "/comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth"
train.output_dir = "./output"

[08/11 17:28:34] detectron2 INFO: Full config saved to ./output/config.yaml
[08/11 17:28:34] d2.utils.env INFO: Using a generated random seed 38302919
[08/11 17:28:37] detectron2 INFO: Model:
DETR(
  (transformer): DetrTransformer(
    (encoder): DetrTransformerEncoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (class_embed): Linear(in_features=256, out_features=81, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (query_embed): Embedding(100, 256)
  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
  (backbone): Joiner(
    (0): MaskedBackbone(
      (backbone): ResNet(
        (stem): BasicStem(
          (conv1): Conv2d(
            3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (res2): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv1): Conv2d(
              64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
        )
        (res3): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv1): Conv2d(
              256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
        )
        (res4): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
            (conv1): Conv2d(
              512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (4): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (5): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
        )
        (res5): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
            (conv1): Conv2d(
              1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[08/11 17:28:55] d2.data.datasets.coco INFO: Loading /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json takes 15.93 seconds.
[08/11 17:28:56] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json
[08/11 17:29:03] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[08/11 17:29:08] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[08/11 17:29:08] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[08/11 17:29:11] d2.data.common INFO: Serialized dataset takes 455.13 MiB
[08/11 17:29:14] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth ...
[08/11 17:29:14] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                   | Names in Checkpoint                                                                                        | Shapes                                          |
|:-------------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:------------------------------------------------|
| backbone.0.backbone.res2.0.conv1.*               | backbone.0.backbone.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| backbone.0.backbone.res2.0.conv2.*               | backbone.0.backbone.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.0.conv3.*               | backbone.0.backbone.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.0.shortcut.*            | backbone.0.backbone.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.1.conv1.*               | backbone.0.backbone.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.1.conv2.*               | backbone.0.backbone.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.1.conv3.*               | backbone.0.backbone.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.2.conv1.*               | backbone.0.backbone.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.2.conv2.*               | backbone.0.backbone.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.2.conv3.*               | backbone.0.backbone.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res3.0.conv1.*               | backbone.0.backbone.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| backbone.0.backbone.res3.0.conv2.*               | backbone.0.backbone.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.0.conv3.*               | backbone.0.backbone.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.0.shortcut.*            | backbone.0.backbone.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| backbone.0.backbone.res3.1.conv1.*               | backbone.0.backbone.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.1.conv2.*               | backbone.0.backbone.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.1.conv3.*               | backbone.0.backbone.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.2.conv1.*               | backbone.0.backbone.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.2.conv2.*               | backbone.0.backbone.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.2.conv3.*               | backbone.0.backbone.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.3.conv1.*               | backbone.0.backbone.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.3.conv2.*               | backbone.0.backbone.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.3.conv3.*               | backbone.0.backbone.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res4.0.conv1.*               | backbone.0.backbone.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| backbone.0.backbone.res4.0.conv2.*               | backbone.0.backbone.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.0.conv3.*               | backbone.0.backbone.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.0.shortcut.*            | backbone.0.backbone.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| backbone.0.backbone.res4.1.conv1.*               | backbone.0.backbone.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.1.conv2.*               | backbone.0.backbone.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.1.conv3.*               | backbone.0.backbone.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.2.conv1.*               | backbone.0.backbone.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.2.conv2.*               | backbone.0.backbone.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.2.conv3.*               | backbone.0.backbone.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.3.conv1.*               | backbone.0.backbone.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.3.conv2.*               | backbone.0.backbone.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.3.conv3.*               | backbone.0.backbone.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.4.conv1.*               | backbone.0.backbone.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.4.conv2.*               | backbone.0.backbone.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.4.conv3.*               | backbone.0.backbone.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.5.conv1.*               | backbone.0.backbone.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.5.conv2.*               | backbone.0.backbone.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.5.conv3.*               | backbone.0.backbone.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res5.0.conv1.*               | backbone.0.backbone.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| backbone.0.backbone.res5.0.conv2.*               | backbone.0.backbone.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.0.conv3.*               | backbone.0.backbone.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.0.shortcut.*            | backbone.0.backbone.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| backbone.0.backbone.res5.1.conv1.*               | backbone.0.backbone.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.1.conv2.*               | backbone.0.backbone.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.1.conv3.*               | backbone.0.backbone.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.2.conv1.*               | backbone.0.backbone.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.2.conv2.*               | backbone.0.backbone.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.2.conv3.*               | backbone.0.backbone.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.stem.conv1.*                 | backbone.0.backbone.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}           | (64,) (64,) (64,) (64,) (64,3,7,7)              |
| bbox_embed.layers.0.*                            | bbox_embed.layers.0.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.1.*                            | bbox_embed.layers.1.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.2.*                            | bbox_embed.layers.2.{bias,weight}                                                                          | (4,) (4,256)                                    |
| class_embed.*                                    | class_embed.{bias,weight}                                                                                  | (81,) (81,256)                                  |
| input_proj.*                                     | input_proj.{bias,weight}                                                                                   | (256,) (256,2048,1,1)                           |
| query_embed.weight                               | query_embed.weight                                                                                         | (100, 256)                                      |
| transformer.decoder.layers.0.attentions.0.attn.* | transformer.decoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.attentions.1.attn.* | transformer.decoder.layers.0.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.ffns.0.layers.0.0.* | transformer.decoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.0.ffns.0.layers.1.*   | transformer.decoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.0.norms.0.*           | transformer.decoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.1.*           | transformer.decoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.2.*           | transformer.decoder.layers.0.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.attentions.0.attn.* | transformer.decoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.attentions.1.attn.* | transformer.decoder.layers.1.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.ffns.0.layers.0.0.* | transformer.decoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.1.ffns.0.layers.1.*   | transformer.decoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.1.norms.0.*           | transformer.decoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.1.*           | transformer.decoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.2.*           | transformer.decoder.layers.1.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.attentions.0.attn.* | transformer.decoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.attentions.1.attn.* | transformer.decoder.layers.2.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.ffns.0.layers.0.0.* | transformer.decoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.2.ffns.0.layers.1.*   | transformer.decoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.2.norms.0.*           | transformer.decoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.1.*           | transformer.decoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.2.*           | transformer.decoder.layers.2.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.attentions.0.attn.* | transformer.decoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.attentions.1.attn.* | transformer.decoder.layers.3.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.ffns.0.layers.0.0.* | transformer.decoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.3.ffns.0.layers.1.*   | transformer.decoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.3.norms.0.*           | transformer.decoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.1.*           | transformer.decoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.2.*           | transformer.decoder.layers.3.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.attentions.0.attn.* | transformer.decoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.attentions.1.attn.* | transformer.decoder.layers.4.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.ffns.0.layers.0.0.* | transformer.decoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.4.ffns.0.layers.1.*   | transformer.decoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.4.norms.0.*           | transformer.decoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.1.*           | transformer.decoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.2.*           | transformer.decoder.layers.4.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.attentions.0.attn.* | transformer.decoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.attentions.1.attn.* | transformer.decoder.layers.5.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.ffns.0.layers.0.0.* | transformer.decoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.5.ffns.0.layers.1.*   | transformer.decoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.5.norms.0.*           | transformer.decoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.1.*           | transformer.decoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.2.*           | transformer.decoder.layers.5.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.post_norm_layer.*            | transformer.decoder.post_norm_layer.{bias,weight}                                                          | (256,) (256,)                                   |
| transformer.encoder.layers.0.attentions.0.attn.* | transformer.encoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.0.ffns.0.layers.0.0.* | transformer.encoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.0.ffns.0.layers.1.*   | transformer.encoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.0.norms.0.*           | transformer.encoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.0.norms.1.*           | transformer.encoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.attentions.0.attn.* | transformer.encoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.1.ffns.0.layers.0.0.* | transformer.encoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.1.ffns.0.layers.1.*   | transformer.encoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.1.norms.0.*           | transformer.encoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.norms.1.*           | transformer.encoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.attentions.0.attn.* | transformer.encoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.2.ffns.0.layers.0.0.* | transformer.encoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.2.ffns.0.layers.1.*   | transformer.encoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.2.norms.0.*           | transformer.encoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.norms.1.*           | transformer.encoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.attentions.0.attn.* | transformer.encoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.3.ffns.0.layers.0.0.* | transformer.encoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.3.ffns.0.layers.1.*   | transformer.encoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.3.norms.0.*           | transformer.encoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.norms.1.*           | transformer.encoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.attentions.0.attn.* | transformer.encoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.4.ffns.0.layers.0.0.* | transformer.encoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.4.ffns.0.layers.1.*   | transformer.encoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.4.norms.0.*           | transformer.encoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.norms.1.*           | transformer.encoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.attentions.0.attn.* | transformer.encoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.5.ffns.0.layers.0.0.* | transformer.encoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.5.ffns.0.layers.1.*   | transformer.encoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.5.norms.0.*           | transformer.encoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.norms.1.*           | transformer.encoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
[08/11 17:29:14] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[08/11 17:29:14] d2.engine.train_loop INFO: Starting training from iteration 0
[08/11 17:29:35] d2.utils.events INFO:  eta: 14:55:29  iter: 19  total_loss: 98.46  loss_ce: 0.9148  loss_bbox: 0.5858  loss_giou: 1.453  loss_ce_0: 1.05  loss_bbox_0: 0.6244  loss_giou_0: 1.453  loss_ce_1: 1.022  loss_bbox_1: 0.612  loss_giou_1: 1.453  loss_ce_2: 0.9147  loss_bbox_2: 0.6013  loss_giou_2: 1.453  loss_ce_3: 0.902  loss_bbox_3: 0.5953  loss_giou_3: 1.456  loss_ce_4: 0.9222  loss_bbox_4: 0.5892  loss_giou_4: 1.445  time: 0.6428  data_time: 0.1239  lr: 0.00039962  max_mem: 21756M
[08/11 17:35:13] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/comp_robot/rentianhe/code/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/comp_robot/rentianhe/code/detectron2/detectron2/engine/train_loop.py", line 274, in run_step
    loss_dict = self.model(data)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/projects/DETR/modeling/detr.py", line 110, in forward
    loss_dict = self.criterion(output, targets)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/modeling/criterion/criterion.py", line 186, in forward
    indices = self.matcher(outputs_without_aux, targets)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/modeling/matcher/matcher.py", line 91, in forward
    cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/layers/box_ops.py", line 55, in generalized_box_iou
    iou, union = box_iou(boxes1, boxes2)
  File "/comp_robot/rentianhe/code/IDEADet/ideadet/layers/box_ops.py", line 55, in generalized_box_iou
    iou, union = box_iou(boxes1, boxes2)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/rentianhe/anaconda3/envs/torch/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
[08/11 17:35:13] d2.engine.hooks INFO: Overall training speed: 30 iterations in 0:05:49 (11.6419 s / it)
[08/11 17:35:13] d2.engine.hooks INFO: Total training time: 0:05:49 (0:00:00 on hooks)
[08/11 17:35:13] d2.utils.events INFO:  eta: 14:55:21  iter: 32  total_loss: 179.8  loss_ce: 2.704  loss_bbox: 1.622  loss_giou: 1.856  loss_ce_0: 2.723  loss_bbox_0: 1.797  loss_giou_0: 1.872  loss_ce_1: 2.614  loss_bbox_1: 1.734  loss_giou_1: 1.786  loss_ce_2: 2.568  loss_bbox_2: 1.716  loss_giou_2: 1.842  loss_ce_3: 2.665  loss_bbox_3: 1.64  loss_giou_3: 1.833  loss_ce_4: 2.713  loss_bbox_4: 1.628  loss_giou_4: 1.88  time: 0.6300  data_time: 0.0898  lr: 0.00063938  max_mem: 23663M
[08/11 17:35:29] detectron2 INFO: Rank of current process: 0. World size: 1
[08/11 17:35:38] detectron2 INFO: Environment info:
----------------------  ------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/comp_robot/rentianhe/code/detectron2/detectron2
Compiler                GCC 9.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   8.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   A100-SXM4-40GB (arch=8.0)
Driver version          450.80.02
CUDA_HOME               /comp_robot/liushilong/software/cuda-11.1/
Pillow                  9.2.0
torchvision             0.13.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     Not found
----------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[08/11 17:35:38] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr_training.py', dist_url='tcp://127.0.0.1:54980', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[08/11 17:35:38] detectron2 INFO: Contents of args.config_file=configs/detr_training.py:
from ideadet.config import get_config

from .models.detr_r50 import model
from .common.coco_loader import dataloader

lr_multiplier = get_config("common/coco_schedule.py").lr_multiplier_1x
optimizer = get_config("common/optim.py").AdamW
train = get_config("common/train.py").train

train.init_checkpoint = "/comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth"
train.output_dir = "./output"

[08/11 17:35:38] detectron2 INFO: Full config saved to ./output/config.yaml
[08/11 17:35:38] d2.utils.env INFO: Using a generated random seed 42433211
[08/11 17:35:42] detectron2 INFO: Model:
DETR(
  (transformer): DetrTransformer(
    (encoder): DetrTransformerEncoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (class_embed): Linear(in_features=256, out_features=81, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (query_embed): Embedding(100, 256)
  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
  (backbone): Joiner(
    (0): MaskedBackbone(
      (backbone): ResNet(
        (stem): BasicStem(
          (conv1): Conv2d(
            3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (res2): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv1): Conv2d(
              64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
        )
        (res3): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv1): Conv2d(
              256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
        )
        (res4): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
            (conv1): Conv2d(
              512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (4): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (5): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
        )
        (res5): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
            (conv1): Conv2d(
              1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[08/11 17:35:58] d2.data.datasets.coco INFO: Loading /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json takes 16.33 seconds.
[08/11 17:35:59] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json
[08/11 17:36:08] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[08/11 17:36:13] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[08/11 17:36:13] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[08/11 17:36:17] d2.data.common INFO: Serialized dataset takes 455.13 MiB
[08/11 17:36:20] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth ...
[08/11 17:36:20] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                   | Names in Checkpoint                                                                                        | Shapes                                          |
|:-------------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:------------------------------------------------|
| backbone.0.backbone.res2.0.conv1.*               | backbone.0.backbone.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| backbone.0.backbone.res2.0.conv2.*               | backbone.0.backbone.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.0.conv3.*               | backbone.0.backbone.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.0.shortcut.*            | backbone.0.backbone.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.1.conv1.*               | backbone.0.backbone.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.1.conv2.*               | backbone.0.backbone.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.1.conv3.*               | backbone.0.backbone.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.2.conv1.*               | backbone.0.backbone.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.2.conv2.*               | backbone.0.backbone.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.2.conv3.*               | backbone.0.backbone.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res3.0.conv1.*               | backbone.0.backbone.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| backbone.0.backbone.res3.0.conv2.*               | backbone.0.backbone.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.0.conv3.*               | backbone.0.backbone.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.0.shortcut.*            | backbone.0.backbone.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| backbone.0.backbone.res3.1.conv1.*               | backbone.0.backbone.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.1.conv2.*               | backbone.0.backbone.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.1.conv3.*               | backbone.0.backbone.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.2.conv1.*               | backbone.0.backbone.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.2.conv2.*               | backbone.0.backbone.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.2.conv3.*               | backbone.0.backbone.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.3.conv1.*               | backbone.0.backbone.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.3.conv2.*               | backbone.0.backbone.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.3.conv3.*               | backbone.0.backbone.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res4.0.conv1.*               | backbone.0.backbone.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| backbone.0.backbone.res4.0.conv2.*               | backbone.0.backbone.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.0.conv3.*               | backbone.0.backbone.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.0.shortcut.*            | backbone.0.backbone.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| backbone.0.backbone.res4.1.conv1.*               | backbone.0.backbone.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.1.conv2.*               | backbone.0.backbone.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.1.conv3.*               | backbone.0.backbone.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.2.conv1.*               | backbone.0.backbone.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.2.conv2.*               | backbone.0.backbone.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.2.conv3.*               | backbone.0.backbone.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.3.conv1.*               | backbone.0.backbone.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.3.conv2.*               | backbone.0.backbone.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.3.conv3.*               | backbone.0.backbone.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.4.conv1.*               | backbone.0.backbone.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.4.conv2.*               | backbone.0.backbone.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.4.conv3.*               | backbone.0.backbone.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.5.conv1.*               | backbone.0.backbone.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.5.conv2.*               | backbone.0.backbone.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.5.conv3.*               | backbone.0.backbone.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res5.0.conv1.*               | backbone.0.backbone.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| backbone.0.backbone.res5.0.conv2.*               | backbone.0.backbone.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.0.conv3.*               | backbone.0.backbone.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.0.shortcut.*            | backbone.0.backbone.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| backbone.0.backbone.res5.1.conv1.*               | backbone.0.backbone.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.1.conv2.*               | backbone.0.backbone.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.1.conv3.*               | backbone.0.backbone.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.2.conv1.*               | backbone.0.backbone.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.2.conv2.*               | backbone.0.backbone.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.2.conv3.*               | backbone.0.backbone.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.stem.conv1.*                 | backbone.0.backbone.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}           | (64,) (64,) (64,) (64,) (64,3,7,7)              |
| bbox_embed.layers.0.*                            | bbox_embed.layers.0.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.1.*                            | bbox_embed.layers.1.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.2.*                            | bbox_embed.layers.2.{bias,weight}                                                                          | (4,) (4,256)                                    |
| class_embed.*                                    | class_embed.{bias,weight}                                                                                  | (81,) (81,256)                                  |
| input_proj.*                                     | input_proj.{bias,weight}                                                                                   | (256,) (256,2048,1,1)                           |
| query_embed.weight                               | query_embed.weight                                                                                         | (100, 256)                                      |
| transformer.decoder.layers.0.attentions.0.attn.* | transformer.decoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.attentions.1.attn.* | transformer.decoder.layers.0.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.ffns.0.layers.0.0.* | transformer.decoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.0.ffns.0.layers.1.*   | transformer.decoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.0.norms.0.*           | transformer.decoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.1.*           | transformer.decoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.2.*           | transformer.decoder.layers.0.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.attentions.0.attn.* | transformer.decoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.attentions.1.attn.* | transformer.decoder.layers.1.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.ffns.0.layers.0.0.* | transformer.decoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.1.ffns.0.layers.1.*   | transformer.decoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.1.norms.0.*           | transformer.decoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.1.*           | transformer.decoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.2.*           | transformer.decoder.layers.1.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.attentions.0.attn.* | transformer.decoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.attentions.1.attn.* | transformer.decoder.layers.2.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.ffns.0.layers.0.0.* | transformer.decoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.2.ffns.0.layers.1.*   | transformer.decoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.2.norms.0.*           | transformer.decoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.1.*           | transformer.decoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.2.*           | transformer.decoder.layers.2.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.attentions.0.attn.* | transformer.decoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.attentions.1.attn.* | transformer.decoder.layers.3.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.ffns.0.layers.0.0.* | transformer.decoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.3.ffns.0.layers.1.*   | transformer.decoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.3.norms.0.*           | transformer.decoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.1.*           | transformer.decoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.2.*           | transformer.decoder.layers.3.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.attentions.0.attn.* | transformer.decoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.attentions.1.attn.* | transformer.decoder.layers.4.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.ffns.0.layers.0.0.* | transformer.decoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.4.ffns.0.layers.1.*   | transformer.decoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.4.norms.0.*           | transformer.decoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.1.*           | transformer.decoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.2.*           | transformer.decoder.layers.4.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.attentions.0.attn.* | transformer.decoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.attentions.1.attn.* | transformer.decoder.layers.5.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.ffns.0.layers.0.0.* | transformer.decoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.5.ffns.0.layers.1.*   | transformer.decoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.5.norms.0.*           | transformer.decoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.1.*           | transformer.decoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.2.*           | transformer.decoder.layers.5.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.post_norm_layer.*            | transformer.decoder.post_norm_layer.{bias,weight}                                                          | (256,) (256,)                                   |
| transformer.encoder.layers.0.attentions.0.attn.* | transformer.encoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.0.ffns.0.layers.0.0.* | transformer.encoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.0.ffns.0.layers.1.*   | transformer.encoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.0.norms.0.*           | transformer.encoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.0.norms.1.*           | transformer.encoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.attentions.0.attn.* | transformer.encoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.1.ffns.0.layers.0.0.* | transformer.encoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.1.ffns.0.layers.1.*   | transformer.encoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.1.norms.0.*           | transformer.encoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.norms.1.*           | transformer.encoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.attentions.0.attn.* | transformer.encoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.2.ffns.0.layers.0.0.* | transformer.encoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.2.ffns.0.layers.1.*   | transformer.encoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.2.norms.0.*           | transformer.encoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.norms.1.*           | transformer.encoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.attentions.0.attn.* | transformer.encoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.3.ffns.0.layers.0.0.* | transformer.encoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.3.ffns.0.layers.1.*   | transformer.encoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.3.norms.0.*           | transformer.encoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.norms.1.*           | transformer.encoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.attentions.0.attn.* | transformer.encoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.4.ffns.0.layers.0.0.* | transformer.encoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.4.ffns.0.layers.1.*   | transformer.encoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.4.norms.0.*           | transformer.encoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.norms.1.*           | transformer.encoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.attentions.0.attn.* | transformer.encoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.5.ffns.0.layers.0.0.* | transformer.encoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.5.ffns.0.layers.1.*   | transformer.encoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.5.norms.0.*           | transformer.encoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.norms.1.*           | transformer.encoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
[08/11 17:36:20] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[08/11 17:36:20] d2.engine.train_loop INFO: Starting training from iteration 0
[08/11 17:36:42] d2.utils.events INFO:  eta: 14:56:36  iter: 19  total_loss: 56.43  loss_ce: 0.2506  loss_bbox: 0.1804  loss_giou: 0.484  loss_ce_0: 0.3414  loss_bbox_0: 0.212  loss_giou_0: 0.5388  loss_ce_1: 0.2891  loss_bbox_1: 0.2059  loss_giou_1: 0.5103  loss_ce_2: 0.2683  loss_bbox_2: 0.1949  loss_giou_2: 0.5073  loss_ce_3: 0.2503  loss_bbox_3: 0.1817  loss_giou_3: 0.4877  loss_ce_4: 0.2533  loss_bbox_4: 0.1796  loss_giou_4: 0.4848  time: 0.8011  data_time: 0.1223  lr: 1.9981e-06  max_mem: 23814M
[08/11 17:36:54] d2.utils.events INFO:  eta: 14:56:45  iter: 39  total_loss: 54.67  loss_ce: 0.2382  loss_bbox: 0.1851  loss_giou: 0.467  loss_ce_0: 0.3126  loss_bbox_0: 0.2086  loss_giou_0: 0.5269  loss_ce_1: 0.2844  loss_bbox_1: 0.1951  loss_giou_1: 0.496  loss_ce_2: 0.2725  loss_bbox_2: 0.1804  loss_giou_2: 0.4752  loss_ce_3: 0.2497  loss_bbox_3: 0.1783  loss_giou_3: 0.4694  loss_ce_4: 0.2424  loss_bbox_4: 0.1852  loss_giou_4: 0.4709  time: 0.7000  data_time: 0.0901  lr: 3.9961e-06  max_mem: 23814M
[08/11 17:37:09] d2.utils.events INFO:  eta: 14:56:33  iter: 59  total_loss: 51.96  loss_ce: 0.2528  loss_bbox: 0.1996  loss_giou: 0.5159  loss_ce_0: 0.3388  loss_bbox_0: 0.2225  loss_giou_0: 0.5694  loss_ce_1: 0.2904  loss_bbox_1: 0.2137  loss_giou_1: 0.5168  loss_ce_2: 0.2559  loss_bbox_2: 0.2067  loss_giou_2: 0.501  loss_ce_3: 0.2487  loss_bbox_3: 0.2023  loss_giou_3: 0.5077  loss_ce_4: 0.2605  loss_bbox_4: 0.1978  loss_giou_4: 0.5131  time: 0.7120  data_time: 0.0917  lr: 5.9941e-06  max_mem: 26860M
[08/11 17:37:23] d2.utils.events INFO:  eta: 15:14:23  iter: 79  total_loss: 57.61  loss_ce: 0.2552  loss_bbox: 0.2024  loss_giou: 0.5107  loss_ce_0: 0.33  loss_bbox_0: 0.2301  loss_giou_0: 0.564  loss_ce_1: 0.2827  loss_bbox_1: 0.2099  loss_giou_1: 0.5432  loss_ce_2: 0.2589  loss_bbox_2: 0.2121  loss_giou_2: 0.5377  loss_ce_3: 0.249  loss_bbox_3: 0.2051  loss_giou_3: 0.5284  loss_ce_4: 0.2474  loss_bbox_4: 0.2046  loss_giou_4: 0.516  time: 0.7124  data_time: 0.0886  lr: 7.9921e-06  max_mem: 27110M
[08/11 17:37:36] d2.utils.events INFO:  eta: 15:27:06  iter: 99  total_loss: 49.18  loss_ce: 0.2734  loss_bbox: 0.1897  loss_giou: 0.5357  loss_ce_0: 0.3771  loss_bbox_0: 0.2094  loss_giou_0: 0.5709  loss_ce_1: 0.3093  loss_bbox_1: 0.2048  loss_giou_1: 0.5573  loss_ce_2: 0.2883  loss_bbox_2: 0.196  loss_giou_2: 0.5551  loss_ce_3: 0.2718  loss_bbox_3: 0.1935  loss_giou_3: 0.5485  loss_ce_4: 0.2765  loss_bbox_4: 0.1906  loss_giou_4: 0.5441  time: 0.6997  data_time: 0.1029  lr: 9.9901e-06  max_mem: 32658M
[08/11 17:37:49] d2.utils.events INFO:  eta: 15:36:16  iter: 119  total_loss: 50.1  loss_ce: 0.253  loss_bbox: 0.1847  loss_giou: 0.4759  loss_ce_0: 0.3466  loss_bbox_0: 0.2228  loss_giou_0: 0.5376  loss_ce_1: 0.3021  loss_bbox_1: 0.1994  loss_giou_1: 0.5077  loss_ce_2: 0.2872  loss_bbox_2: 0.1911  loss_giou_2: 0.4871  loss_ce_3: 0.2607  loss_bbox_3: 0.1886  loss_giou_3: 0.49  loss_ce_4: 0.2573  loss_bbox_4: 0.1869  loss_giou_4: 0.4759  time: 0.6888  data_time: 0.0971  lr: 1.1988e-05  max_mem: 32658M
[08/11 17:38:02] d2.utils.events INFO:  eta: 15:40:10  iter: 139  total_loss: 48.82  loss_ce: 0.2475  loss_bbox: 0.1855  loss_giou: 0.4922  loss_ce_0: 0.3311  loss_bbox_0: 0.2112  loss_giou_0: 0.5519  loss_ce_1: 0.3041  loss_bbox_1: 0.1999  loss_giou_1: 0.5189  loss_ce_2: 0.2566  loss_bbox_2: 0.1881  loss_giou_2: 0.5039  loss_ce_3: 0.2654  loss_bbox_3: 0.1905  loss_giou_3: 0.5001  loss_ce_4: 0.2516  loss_bbox_4: 0.188  loss_giou_4: 0.4992  time: 0.6806  data_time: 0.0909  lr: 1.3986e-05  max_mem: 32658M
[08/11 17:38:17] d2.utils.events INFO:  eta: 15:44:52  iter: 159  total_loss: 53.79  loss_ce: 0.265  loss_bbox: 0.1843  loss_giou: 0.5254  loss_ce_0: 0.3608  loss_bbox_0: 0.2082  loss_giou_0: 0.5987  loss_ce_1: 0.3117  loss_bbox_1: 0.2016  loss_giou_1: 0.5393  loss_ce_2: 0.2822  loss_bbox_2: 0.1986  loss_giou_2: 0.5388  loss_ce_3: 0.2708  loss_bbox_3: 0.1912  loss_giou_3: 0.5415  loss_ce_4: 0.2648  loss_bbox_4: 0.1902  loss_giou_4: 0.5233  time: 0.6925  data_time: 0.0907  lr: 1.5984e-05  max_mem: 37761M
[08/11 17:38:30] d2.utils.events INFO:  eta: 15:41:14  iter: 179  total_loss: 65.25  loss_ce: 0.3393  loss_bbox: 0.2116  loss_giou: 0.5421  loss_ce_0: 0.4474  loss_bbox_0: 0.235  loss_giou_0: 0.602  loss_ce_1: 0.3927  loss_bbox_1: 0.2215  loss_giou_1: 0.5864  loss_ce_2: 0.3789  loss_bbox_2: 0.2208  loss_giou_2: 0.5801  loss_ce_3: 0.3562  loss_bbox_3: 0.2163  loss_giou_3: 0.5596  loss_ce_4: 0.3576  loss_bbox_4: 0.2108  loss_giou_4: 0.5509  time: 0.6844  data_time: 0.0882  lr: 1.7982e-05  max_mem: 37761M
[08/11 17:38:42] d2.utils.events INFO:  eta: 15:38:35  iter: 199  total_loss: 72.71  loss_ce: 0.3285  loss_bbox: 0.1991  loss_giou: 0.5202  loss_ce_0: 0.4017  loss_bbox_0: 0.2275  loss_giou_0: 0.5802  loss_ce_1: 0.3658  loss_bbox_1: 0.2157  loss_giou_1: 0.5451  loss_ce_2: 0.3233  loss_bbox_2: 0.209  loss_giou_2: 0.5354  loss_ce_3: 0.3237  loss_bbox_3: 0.2104  loss_giou_3: 0.5295  loss_ce_4: 0.3272  loss_bbox_4: 0.199  loss_giou_4: 0.516  time: 0.6778  data_time: 0.0869  lr: 1.998e-05  max_mem: 37761M
[08/11 17:38:55] d2.utils.events INFO:  eta: 15:37:37  iter: 219  total_loss: 53.89  loss_ce: 0.291  loss_bbox: 0.1907  loss_giou: 0.533  loss_ce_0: 0.3976  loss_bbox_0: 0.2369  loss_giou_0: 0.5938  loss_ce_1: 0.3344  loss_bbox_1: 0.2146  loss_giou_1: 0.5712  loss_ce_2: 0.3234  loss_bbox_2: 0.2046  loss_giou_2: 0.551  loss_ce_3: 0.2992  loss_bbox_3: 0.1988  loss_giou_3: 0.5478  loss_ce_4: 0.2813  loss_bbox_4: 0.1946  loss_giou_4: 0.5386  time: 0.6718  data_time: 0.0919  lr: 2.1978e-05  max_mem: 37761M
[08/11 17:39:09] d2.utils.events INFO:  eta: 15:36:35  iter: 239  total_loss: 59.41  loss_ce: 0.2658  loss_bbox: 0.1976  loss_giou: 0.4841  loss_ce_0: 0.3596  loss_bbox_0: 0.2191  loss_giou_0: 0.5428  loss_ce_1: 0.3155  loss_bbox_1: 0.206  loss_giou_1: 0.5142  loss_ce_2: 0.2997  loss_bbox_2: 0.2009  loss_giou_2: 0.4977  loss_ce_3: 0.2838  loss_bbox_3: 0.1999  loss_giou_3: 0.4962  loss_ce_4: 0.2789  loss_bbox_4: 0.1964  loss_giou_4: 0.4913  time: 0.6764  data_time: 0.0872  lr: 2.3976e-05  max_mem: 37761M
[08/11 17:39:22] d2.utils.events INFO:  eta: 15:35:02  iter: 259  total_loss: 59.63  loss_ce: 0.3136  loss_bbox: 0.1948  loss_giou: 0.5363  loss_ce_0: 0.3904  loss_bbox_0: 0.2235  loss_giou_0: 0.585  loss_ce_1: 0.3709  loss_bbox_1: 0.2132  loss_giou_1: 0.5523  loss_ce_2: 0.329  loss_bbox_2: 0.2022  loss_giou_2: 0.5438  loss_ce_3: 0.3257  loss_bbox_3: 0.1979  loss_giou_3: 0.5417  loss_ce_4: 0.3196  loss_bbox_4: 0.1921  loss_giou_4: 0.5405  time: 0.6723  data_time: 0.0884  lr: 2.5974e-05  max_mem: 37761M
[08/11 17:39:35] d2.utils.events INFO:  eta: 15:38:38  iter: 279  total_loss: 58.78  loss_ce: 0.305  loss_bbox: 0.2103  loss_giou: 0.4871  loss_ce_0: 0.3787  loss_bbox_0: 0.2408  loss_giou_0: 0.5458  loss_ce_1: 0.3465  loss_bbox_1: 0.2315  loss_giou_1: 0.5165  loss_ce_2: 0.329  loss_bbox_2: 0.2203  loss_giou_2: 0.5088  loss_ce_3: 0.3093  loss_bbox_3: 0.212  loss_giou_3: 0.5034  loss_ce_4: 0.3097  loss_bbox_4: 0.2122  loss_giou_4: 0.4875  time: 0.6705  data_time: 0.0910  lr: 2.7972e-05  max_mem: 37761M
[08/11 17:39:47] d2.utils.events INFO:  eta: 15:34:37  iter: 299  total_loss: 89.23  loss_ce: 0.7158  loss_bbox: 0.3246  loss_giou: 0.6476  loss_ce_0: 0.796  loss_bbox_0: 0.3691  loss_giou_0: 0.7217  loss_ce_1: 0.7952  loss_bbox_1: 0.3479  loss_giou_1: 0.6671  loss_ce_2: 0.7366  loss_bbox_2: 0.3354  loss_giou_2: 0.6594  loss_ce_3: 0.7446  loss_bbox_3: 0.3297  loss_giou_3: 0.6452  loss_ce_4: 0.7366  loss_bbox_4: 0.3255  loss_giou_4: 0.6472  time: 0.6661  data_time: 0.0884  lr: 2.997e-05  max_mem: 37761M
[08/11 17:39:59] d2.utils.events INFO:  eta: 15:34:25  iter: 319  total_loss: 111.5  loss_ce: 0.8348  loss_bbox: 0.3758  loss_giou: 0.7216  loss_ce_0: 0.9236  loss_bbox_0: 0.4379  loss_giou_0: 0.7952  loss_ce_1: 0.9146  loss_bbox_1: 0.4066  loss_giou_1: 0.7343  loss_ce_2: 0.8788  loss_bbox_2: 0.3888  loss_giou_2: 0.7351  loss_ce_3: 0.8741  loss_bbox_3: 0.3806  loss_giou_3: 0.7243  loss_ce_4: 0.8408  loss_bbox_4: 0.3723  loss_giou_4: 0.72  time: 0.6631  data_time: 0.0900  lr: 3.1968e-05  max_mem: 37761M
[08/11 17:40:11] d2.utils.events INFO:  eta: 15:25:33  iter: 339  total_loss: 91.51  loss_ce: 0.6889  loss_bbox: 0.3363  loss_giou: 0.611  loss_ce_0: 0.7472  loss_bbox_0: 0.3796  loss_giou_0: 0.6655  loss_ce_1: 0.732  loss_bbox_1: 0.3637  loss_giou_1: 0.6225  loss_ce_2: 0.6969  loss_bbox_2: 0.3498  loss_giou_2: 0.6133  loss_ce_3: 0.6792  loss_bbox_3: 0.3461  loss_giou_3: 0.6189  loss_ce_4: 0.6865  loss_bbox_4: 0.3401  loss_giou_4: 0.6239  time: 0.6595  data_time: 0.0953  lr: 3.3966e-05  max_mem: 37761M
[08/11 17:40:24] d2.utils.events INFO:  eta: 15:25:21  iter: 359  total_loss: 71.98  loss_ce: 0.5443  loss_bbox: 0.2555  loss_giou: 0.5591  loss_ce_0: 0.6143  loss_bbox_0: 0.3195  loss_giou_0: 0.6817  loss_ce_1: 0.5713  loss_bbox_1: 0.282  loss_giou_1: 0.6069  loss_ce_2: 0.5518  loss_bbox_2: 0.2572  loss_giou_2: 0.5781  loss_ce_3: 0.5448  loss_bbox_3: 0.259  loss_giou_3: 0.5647  loss_ce_4: 0.5469  loss_bbox_4: 0.2569  loss_giou_4: 0.5608  time: 0.6573  data_time: 0.0860  lr: 3.5964e-05  max_mem: 37761M
[08/11 17:40:36] d2.utils.events INFO:  eta: 15:23:59  iter: 379  total_loss: 72.39  loss_ce: 0.4411  loss_bbox: 0.2578  loss_giou: 0.5824  loss_ce_0: 0.5358  loss_bbox_0: 0.3054  loss_giou_0: 0.6345  loss_ce_1: 0.4971  loss_bbox_1: 0.2857  loss_giou_1: 0.6009  loss_ce_2: 0.4522  loss_bbox_2: 0.2804  loss_giou_2: 0.5891  loss_ce_3: 0.4551  loss_bbox_3: 0.2642  loss_giou_3: 0.5876  loss_ce_4: 0.444  loss_bbox_4: 0.2594  loss_giou_4: 0.5767  time: 0.6556  data_time: 0.0837  lr: 3.7962e-05  max_mem: 37761M
[08/11 17:40:49] d2.utils.events INFO:  eta: 15:26:30  iter: 399  total_loss: 69.44  loss_ce: 0.3904  loss_bbox: 0.2378  loss_giou: 0.5307  loss_ce_0: 0.478  loss_bbox_0: 0.282  loss_giou_0: 0.5752  loss_ce_1: 0.4307  loss_bbox_1: 0.2727  loss_giou_1: 0.5621  loss_ce_2: 0.4116  loss_bbox_2: 0.2504  loss_giou_2: 0.5395  loss_ce_3: 0.3949  loss_bbox_3: 0.2377  loss_giou_3: 0.542  loss_ce_4: 0.3949  loss_bbox_4: 0.2407  loss_giou_4: 0.5361  time: 0.6542  data_time: 0.0873  lr: 3.996e-05  max_mem: 37761M
[08/11 17:41:01] d2.utils.events INFO:  eta: 15:22:26  iter: 419  total_loss: 76.85  loss_ce: 0.4287  loss_bbox: 0.2583  loss_giou: 0.5523  loss_ce_0: 0.4926  loss_bbox_0: 0.2803  loss_giou_0: 0.6155  loss_ce_1: 0.4737  loss_bbox_1: 0.2661  loss_giou_1: 0.5848  loss_ce_2: 0.4475  loss_bbox_2: 0.2659  loss_giou_2: 0.5717  loss_ce_3: 0.4374  loss_bbox_3: 0.2646  loss_giou_3: 0.5537  loss_ce_4: 0.4249  loss_bbox_4: 0.2576  loss_giou_4: 0.5542  time: 0.6515  data_time: 0.0886  lr: 4.1958e-05  max_mem: 37761M
[08/11 17:41:13] d2.utils.events INFO:  eta: 15:18:45  iter: 439  total_loss: 70.93  loss_ce: 0.3798  loss_bbox: 0.2232  loss_giou: 0.5782  loss_ce_0: 0.4758  loss_bbox_0: 0.2619  loss_giou_0: 0.6529  loss_ce_1: 0.4547  loss_bbox_1: 0.2396  loss_giou_1: 0.6066  loss_ce_2: 0.4242  loss_bbox_2: 0.2371  loss_giou_2: 0.602  loss_ce_3: 0.3959  loss_bbox_3: 0.2295  loss_giou_3: 0.5873  loss_ce_4: 0.3915  loss_bbox_4: 0.2232  loss_giou_4: 0.5782  time: 0.6489  data_time: 0.0867  lr: 4.3956e-05  max_mem: 37761M
[08/11 17:41:25] d2.utils.events INFO:  eta: 15:16:38  iter: 459  total_loss: 65.14  loss_ce: 0.4234  loss_bbox: 0.2292  loss_giou: 0.5737  loss_ce_0: 0.5034  loss_bbox_0: 0.2697  loss_giou_0: 0.6323  loss_ce_1: 0.4645  loss_bbox_1: 0.2448  loss_giou_1: 0.597  loss_ce_2: 0.4288  loss_bbox_2: 0.2347  loss_giou_2: 0.5842  loss_ce_3: 0.42  loss_bbox_3: 0.2384  loss_giou_3: 0.5799  loss_ce_4: 0.4202  loss_bbox_4: 0.2286  loss_giou_4: 0.5904  time: 0.6472  data_time: 0.0954  lr: 4.5954e-05  max_mem: 37761M
[08/11 17:41:38] d2.utils.events INFO:  eta: 15:18:20  iter: 479  total_loss: 67.19  loss_ce: 0.4334  loss_bbox: 0.2502  loss_giou: 0.5926  loss_ce_0: 0.5027  loss_bbox_0: 0.2963  loss_giou_0: 0.6526  loss_ce_1: 0.4703  loss_bbox_1: 0.2803  loss_giou_1: 0.615  loss_ce_2: 0.4469  loss_bbox_2: 0.2612  loss_giou_2: 0.607  loss_ce_3: 0.435  loss_bbox_3: 0.2545  loss_giou_3: 0.6018  loss_ce_4: 0.435  loss_bbox_4: 0.2499  loss_giou_4: 0.5957  time: 0.6467  data_time: 0.0914  lr: 4.7952e-05  max_mem: 37761M
[08/11 17:41:50] d2.utils.events INFO:  eta: 15:16:14  iter: 499  total_loss: 70.16  loss_ce: 0.451  loss_bbox: 0.2532  loss_giou: 0.5756  loss_ce_0: 0.5214  loss_bbox_0: 0.3018  loss_giou_0: 0.6332  loss_ce_1: 0.4752  loss_bbox_1: 0.2737  loss_giou_1: 0.5856  loss_ce_2: 0.4599  loss_bbox_2: 0.2563  loss_giou_2: 0.5807  loss_ce_3: 0.4516  loss_bbox_3: 0.2506  loss_giou_3: 0.5746  loss_ce_4: 0.4557  loss_bbox_4: 0.2472  loss_giou_4: 0.5724  time: 0.6453  data_time: 0.0925  lr: 4.995e-05  max_mem: 37761M
[08/11 17:42:03] d2.utils.events INFO:  eta: 15:16:59  iter: 519  total_loss: 74.14  loss_ce: 0.4476  loss_bbox: 0.2581  loss_giou: 0.6146  loss_ce_0: 0.5647  loss_bbox_0: 0.2931  loss_giou_0: 0.7009  loss_ce_1: 0.5067  loss_bbox_1: 0.2647  loss_giou_1: 0.6542  loss_ce_2: 0.4753  loss_bbox_2: 0.2662  loss_giou_2: 0.6423  loss_ce_3: 0.4445  loss_bbox_3: 0.2598  loss_giou_3: 0.6254  loss_ce_4: 0.4505  loss_bbox_4: 0.2582  loss_giou_4: 0.6214  time: 0.6447  data_time: 0.0890  lr: 5.1948e-05  max_mem: 37761M
[08/11 17:42:15] d2.utils.events INFO:  eta: 15:14:54  iter: 539  total_loss: 64.15  loss_ce: 0.4645  loss_bbox: 0.2607  loss_giou: 0.5713  loss_ce_0: 0.5431  loss_bbox_0: 0.293  loss_giou_0: 0.6101  loss_ce_1: 0.5075  loss_bbox_1: 0.2703  loss_giou_1: 0.5697  loss_ce_2: 0.4973  loss_bbox_2: 0.2641  loss_giou_2: 0.5677  loss_ce_3: 0.4737  loss_bbox_3: 0.2664  loss_giou_3: 0.5763  loss_ce_4: 0.4744  loss_bbox_4: 0.2606  loss_giou_4: 0.5672  time: 0.6433  data_time: 0.0923  lr: 5.3946e-05  max_mem: 37761M
[08/11 17:42:28] d2.utils.events INFO:  eta: 15:16:20  iter: 559  total_loss: 73.03  loss_ce: 0.4474  loss_bbox: 0.2674  loss_giou: 0.5802  loss_ce_0: 0.5157  loss_bbox_0: 0.3022  loss_giou_0: 0.6386  loss_ce_1: 0.5016  loss_bbox_1: 0.2953  loss_giou_1: 0.6015  loss_ce_2: 0.465  loss_bbox_2: 0.2817  loss_giou_2: 0.5969  loss_ce_3: 0.4456  loss_bbox_3: 0.2739  loss_giou_3: 0.5855  loss_ce_4: 0.4443  loss_bbox_4: 0.2691  loss_giou_4: 0.5781  time: 0.6439  data_time: 0.0969  lr: 5.5944e-05  max_mem: 37761M
[08/11 17:42:41] d2.utils.events INFO:  eta: 15:16:07  iter: 579  total_loss: 81.17  loss_ce: 0.5168  loss_bbox: 0.2895  loss_giou: 0.6842  loss_ce_0: 0.611  loss_bbox_0: 0.3332  loss_giou_0: 0.7411  loss_ce_1: 0.5636  loss_bbox_1: 0.3199  loss_giou_1: 0.7109  loss_ce_2: 0.5274  loss_bbox_2: 0.3021  loss_giou_2: 0.695  loss_ce_3: 0.5116  loss_bbox_3: 0.2934  loss_giou_3: 0.6906  loss_ce_4: 0.5228  loss_bbox_4: 0.2918  loss_giou_4: 0.6881  time: 0.6429  data_time: 0.0928  lr: 5.7942e-05  max_mem: 37761M
[08/11 17:42:53] d2.utils.events INFO:  eta: 15:15:55  iter: 599  total_loss: 74.73  loss_ce: 0.4546  loss_bbox: 0.2621  loss_giou: 0.6174  loss_ce_0: 0.5471  loss_bbox_0: 0.3168  loss_giou_0: 0.6901  loss_ce_1: 0.4909  loss_bbox_1: 0.2895  loss_giou_1: 0.6521  loss_ce_2: 0.4685  loss_bbox_2: 0.2713  loss_giou_2: 0.6289  loss_ce_3: 0.4653  loss_bbox_3: 0.264  loss_giou_3: 0.6317  loss_ce_4: 0.4582  loss_bbox_4: 0.2621  loss_giou_4: 0.6263  time: 0.6423  data_time: 0.0870  lr: 5.994e-05  max_mem: 37761M
[08/11 17:43:02] d2.engine.hooks INFO: Overall training speed: 612 iterations in 0:06:33 (0.6423 s / it)
[08/11 17:43:02] d2.engine.hooks INFO: Total training time: 0:06:34 (0:00:01 on hooks)
[08/11 17:43:02] d2.utils.events INFO:  eta: 15:17:46  iter: 614  total_loss: 80.08  loss_ce: 0.5047  loss_bbox: 0.2545  loss_giou: 0.6018  loss_ce_0: 0.5754  loss_bbox_0: 0.3022  loss_giou_0: 0.6603  loss_ce_1: 0.5448  loss_bbox_1: 0.2717  loss_giou_1: 0.6259  loss_ce_2: 0.5056  loss_bbox_2: 0.2617  loss_giou_2: 0.6158  loss_ce_3: 0.5131  loss_bbox_3: 0.2621  loss_giou_3: 0.608  loss_ce_4: 0.5017  loss_bbox_4: 0.2597  loss_giou_4: 0.6032  time: 0.6422  data_time: 0.0909  lr: 6.1339e-05  max_mem: 37761M
[08/11 17:43:19] detectron2 INFO: Rank of current process: 0. World size: 1
[08/11 17:43:26] detectron2 INFO: Environment info:
----------------------  ------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/comp_robot/rentianhe/code/detectron2/detectron2
Compiler                GCC 9.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   8.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   A100-SXM4-40GB (arch=8.0)
Driver version          450.80.02
CUDA_HOME               /comp_robot/liushilong/software/cuda-11.1/
Pillow                  9.2.0
torchvision             0.13.0 @/home/rentianhe/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     Not found
----------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[08/11 17:43:26] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr_training.py', dist_url='tcp://127.0.0.1:54980', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[08/11 17:43:26] detectron2 INFO: Contents of args.config_file=configs/detr_training.py:
from ideadet.config import get_config

from .models.detr_r50 import model
from .common.coco_loader import dataloader

lr_multiplier = get_config("common/coco_schedule.py").lr_multiplier_1x
optimizer = get_config("common/optim.py").AdamW
train = get_config("common/train.py").train

train.init_checkpoint = "/comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth"
train.output_dir = "./output"

[08/11 17:43:26] detectron2 INFO: Full config saved to ./output/config.yaml
[08/11 17:43:26] d2.utils.env INFO: Using a generated random seed 26913732
[08/11 17:43:31] detectron2 INFO: Model:
DETR(
  (transformer): DetrTransformer(
    (encoder): DetrTransformerEncoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (class_embed): Linear(in_features=256, out_features=81, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (query_embed): Embedding(100, 256)
  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
  (backbone): Joiner(
    (0): MaskedBackbone(
      (backbone): ResNet(
        (stem): BasicStem(
          (conv1): Conv2d(
            3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (res2): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv1): Conv2d(
              64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv2): Conv2d(
              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
            (conv3): Conv2d(
              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
          )
        )
        (res3): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv1): Conv2d(
              256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv2): Conv2d(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
            )
            (conv3): Conv2d(
              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
          )
        )
        (res4): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
            (conv1): Conv2d(
              512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (4): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
          (5): BottleneckBlock(
            (conv1): Conv2d(
              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv2): Conv2d(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
            )
            (conv3): Conv2d(
              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
            )
          )
        )
        (res5): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(
              1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
            (conv1): Conv2d(
              1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(
              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv2): Conv2d(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
            )
            (conv3): Conv2d(
              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
            )
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[08/11 17:43:47] d2.data.datasets.coco INFO: Loading /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json takes 15.31 seconds.
[08/11 17:43:48] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from /comp_robot/rentianhe/code/IDEADet/datasets/coco/annotations/instances_train2017.json
[08/11 17:44:01] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[08/11 17:44:01] d2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[08/11 17:44:05] d2.data.common INFO: Serialized dataset takes 455.28 MiB
[08/11 17:44:08] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /comp_robot/rentianhe/code/IDEADet/weights/converted_new_detr_model.pth ...
[08/11 17:44:09] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                   | Names in Checkpoint                                                                                        | Shapes                                          |
|:-------------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:------------------------------------------------|
| backbone.0.backbone.res2.0.conv1.*               | backbone.0.backbone.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| backbone.0.backbone.res2.0.conv2.*               | backbone.0.backbone.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.0.conv3.*               | backbone.0.backbone.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.0.shortcut.*            | backbone.0.backbone.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.1.conv1.*               | backbone.0.backbone.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.1.conv2.*               | backbone.0.backbone.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.1.conv3.*               | backbone.0.backbone.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res2.2.conv1.*               | backbone.0.backbone.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| backbone.0.backbone.res2.2.conv2.*               | backbone.0.backbone.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| backbone.0.backbone.res2.2.conv3.*               | backbone.0.backbone.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| backbone.0.backbone.res3.0.conv1.*               | backbone.0.backbone.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| backbone.0.backbone.res3.0.conv2.*               | backbone.0.backbone.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.0.conv3.*               | backbone.0.backbone.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.0.shortcut.*            | backbone.0.backbone.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| backbone.0.backbone.res3.1.conv1.*               | backbone.0.backbone.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.1.conv2.*               | backbone.0.backbone.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.1.conv3.*               | backbone.0.backbone.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.2.conv1.*               | backbone.0.backbone.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.2.conv2.*               | backbone.0.backbone.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.2.conv3.*               | backbone.0.backbone.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res3.3.conv1.*               | backbone.0.backbone.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| backbone.0.backbone.res3.3.conv2.*               | backbone.0.backbone.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| backbone.0.backbone.res3.3.conv3.*               | backbone.0.backbone.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| backbone.0.backbone.res4.0.conv1.*               | backbone.0.backbone.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| backbone.0.backbone.res4.0.conv2.*               | backbone.0.backbone.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.0.conv3.*               | backbone.0.backbone.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.0.shortcut.*            | backbone.0.backbone.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| backbone.0.backbone.res4.1.conv1.*               | backbone.0.backbone.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.1.conv2.*               | backbone.0.backbone.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.1.conv3.*               | backbone.0.backbone.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.2.conv1.*               | backbone.0.backbone.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.2.conv2.*               | backbone.0.backbone.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.2.conv3.*               | backbone.0.backbone.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.3.conv1.*               | backbone.0.backbone.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.3.conv2.*               | backbone.0.backbone.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.3.conv3.*               | backbone.0.backbone.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.4.conv1.*               | backbone.0.backbone.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.4.conv2.*               | backbone.0.backbone.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.4.conv3.*               | backbone.0.backbone.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res4.5.conv1.*               | backbone.0.backbone.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| backbone.0.backbone.res4.5.conv2.*               | backbone.0.backbone.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| backbone.0.backbone.res4.5.conv3.*               | backbone.0.backbone.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| backbone.0.backbone.res5.0.conv1.*               | backbone.0.backbone.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| backbone.0.backbone.res5.0.conv2.*               | backbone.0.backbone.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.0.conv3.*               | backbone.0.backbone.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.0.shortcut.*            | backbone.0.backbone.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| backbone.0.backbone.res5.1.conv1.*               | backbone.0.backbone.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.1.conv2.*               | backbone.0.backbone.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.1.conv3.*               | backbone.0.backbone.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.res5.2.conv1.*               | backbone.0.backbone.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| backbone.0.backbone.res5.2.conv2.*               | backbone.0.backbone.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| backbone.0.backbone.res5.2.conv3.*               | backbone.0.backbone.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}         | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| backbone.0.backbone.stem.conv1.*                 | backbone.0.backbone.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}           | (64,) (64,) (64,) (64,) (64,3,7,7)              |
| bbox_embed.layers.0.*                            | bbox_embed.layers.0.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.1.*                            | bbox_embed.layers.1.{bias,weight}                                                                          | (256,) (256,256)                                |
| bbox_embed.layers.2.*                            | bbox_embed.layers.2.{bias,weight}                                                                          | (4,) (4,256)                                    |
| class_embed.*                                    | class_embed.{bias,weight}                                                                                  | (81,) (81,256)                                  |
| input_proj.*                                     | input_proj.{bias,weight}                                                                                   | (256,) (256,2048,1,1)                           |
| query_embed.weight                               | query_embed.weight                                                                                         | (100, 256)                                      |
| transformer.decoder.layers.0.attentions.0.attn.* | transformer.decoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.attentions.1.attn.* | transformer.decoder.layers.0.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.0.ffns.0.layers.0.0.* | transformer.decoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.0.ffns.0.layers.1.*   | transformer.decoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.0.norms.0.*           | transformer.decoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.1.*           | transformer.decoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.0.norms.2.*           | transformer.decoder.layers.0.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.attentions.0.attn.* | transformer.decoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.attentions.1.attn.* | transformer.decoder.layers.1.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.1.ffns.0.layers.0.0.* | transformer.decoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.1.ffns.0.layers.1.*   | transformer.decoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.1.norms.0.*           | transformer.decoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.1.*           | transformer.decoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.1.norms.2.*           | transformer.decoder.layers.1.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.attentions.0.attn.* | transformer.decoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.attentions.1.attn.* | transformer.decoder.layers.2.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.2.ffns.0.layers.0.0.* | transformer.decoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.2.ffns.0.layers.1.*   | transformer.decoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.2.norms.0.*           | transformer.decoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.1.*           | transformer.decoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.2.norms.2.*           | transformer.decoder.layers.2.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.attentions.0.attn.* | transformer.decoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.attentions.1.attn.* | transformer.decoder.layers.3.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.3.ffns.0.layers.0.0.* | transformer.decoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.3.ffns.0.layers.1.*   | transformer.decoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.3.norms.0.*           | transformer.decoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.1.*           | transformer.decoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.3.norms.2.*           | transformer.decoder.layers.3.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.attentions.0.attn.* | transformer.decoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.attentions.1.attn.* | transformer.decoder.layers.4.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.4.ffns.0.layers.0.0.* | transformer.decoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.4.ffns.0.layers.1.*   | transformer.decoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.4.norms.0.*           | transformer.decoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.1.*           | transformer.decoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.4.norms.2.*           | transformer.decoder.layers.4.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.attentions.0.attn.* | transformer.decoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.attentions.1.attn.* | transformer.decoder.layers.5.attentions.1.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.decoder.layers.5.ffns.0.layers.0.0.* | transformer.decoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.decoder.layers.5.ffns.0.layers.1.*   | transformer.decoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.decoder.layers.5.norms.0.*           | transformer.decoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.1.*           | transformer.decoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.layers.5.norms.2.*           | transformer.decoder.layers.5.norms.2.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.decoder.post_norm_layer.*            | transformer.decoder.post_norm_layer.{bias,weight}                                                          | (256,) (256,)                                   |
| transformer.encoder.layers.0.attentions.0.attn.* | transformer.encoder.layers.0.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.0.ffns.0.layers.0.0.* | transformer.encoder.layers.0.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.0.ffns.0.layers.1.*   | transformer.encoder.layers.0.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.0.norms.0.*           | transformer.encoder.layers.0.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.0.norms.1.*           | transformer.encoder.layers.0.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.attentions.0.attn.* | transformer.encoder.layers.1.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.1.ffns.0.layers.0.0.* | transformer.encoder.layers.1.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.1.ffns.0.layers.1.*   | transformer.encoder.layers.1.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.1.norms.0.*           | transformer.encoder.layers.1.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.1.norms.1.*           | transformer.encoder.layers.1.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.attentions.0.attn.* | transformer.encoder.layers.2.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.2.ffns.0.layers.0.0.* | transformer.encoder.layers.2.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.2.ffns.0.layers.1.*   | transformer.encoder.layers.2.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.2.norms.0.*           | transformer.encoder.layers.2.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.2.norms.1.*           | transformer.encoder.layers.2.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.attentions.0.attn.* | transformer.encoder.layers.3.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.3.ffns.0.layers.0.0.* | transformer.encoder.layers.3.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.3.ffns.0.layers.1.*   | transformer.encoder.layers.3.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.3.norms.0.*           | transformer.encoder.layers.3.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.3.norms.1.*           | transformer.encoder.layers.3.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.attentions.0.attn.* | transformer.encoder.layers.4.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.4.ffns.0.layers.0.0.* | transformer.encoder.layers.4.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.4.ffns.0.layers.1.*   | transformer.encoder.layers.4.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.4.norms.0.*           | transformer.encoder.layers.4.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.4.norms.1.*           | transformer.encoder.layers.4.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.attentions.0.attn.* | transformer.encoder.layers.5.attentions.0.attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)               |
| transformer.encoder.layers.5.ffns.0.layers.0.0.* | transformer.encoder.layers.5.ffns.0.layers.0.0.{bias,weight}                                               | (2048,) (2048,256)                              |
| transformer.encoder.layers.5.ffns.0.layers.1.*   | transformer.encoder.layers.5.ffns.0.layers.1.{bias,weight}                                                 | (256,) (256,2048)                               |
| transformer.encoder.layers.5.norms.0.*           | transformer.encoder.layers.5.norms.0.{bias,weight}                                                         | (256,) (256,)                                   |
| transformer.encoder.layers.5.norms.1.*           | transformer.encoder.layers.5.norms.1.{bias,weight}                                                         | (256,) (256,)                                   |
[08/11 17:44:09] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[08/11 17:44:09] d2.engine.train_loop INFO: Starting training from iteration 0
[08/11 17:44:29] d2.utils.events INFO:  eta: 15:32:10  iter: 19  total_loss: 50.36  loss_ce: 0.248  loss_bbox: 0.1938  loss_giou: 0.4855  loss_ce_0: 0.3476  loss_bbox_0: 0.2239  loss_giou_0: 0.5532  loss_ce_1: 0.2886  loss_bbox_1: 0.208  loss_giou_1: 0.5167  loss_ce_2: 0.2622  loss_bbox_2: 0.2039  loss_giou_2: 0.5029  loss_ce_3: 0.2525  loss_bbox_3: 0.1975  loss_giou_3: 0.4933  loss_ce_4: 0.2484  loss_bbox_4: 0.195  loss_giou_4: 0.4887  time: 0.6908  data_time: 0.1260  lr: 1.9981e-06  max_mem: 32466M
[08/11 17:44:41] d2.utils.events INFO:  eta: 15:24:03  iter: 39  total_loss: 54.49  loss_ce: 0.2508  loss_bbox: 0.1792  loss_giou: 0.5128  loss_ce_0: 0.3287  loss_bbox_0: 0.2152  loss_giou_0: 0.5637  loss_ce_1: 0.2928  loss_bbox_1: 0.196  loss_giou_1: 0.5337  loss_ce_2: 0.2686  loss_bbox_2: 0.1901  loss_giou_2: 0.5182  loss_ce_3: 0.2661  loss_bbox_3: 0.1869  loss_giou_3: 0.5137  loss_ce_4: 0.2627  loss_bbox_4: 0.1772  loss_giou_4: 0.5083  time: 0.6545  data_time: 0.0909  lr: 3.9961e-06  max_mem: 32466M
[08/11 17:44:54] d2.utils.events INFO:  eta: 15:46:26  iter: 59  total_loss: 57.38  loss_ce: 0.2628  loss_bbox: 0.1831  loss_giou: 0.4873  loss_ce_0: 0.3706  loss_bbox_0: 0.2063  loss_giou_0: 0.5346  loss_ce_1: 0.3165  loss_bbox_1: 0.1876  loss_giou_1: 0.4992  loss_ce_2: 0.2891  loss_bbox_2: 0.1863  loss_giou_2: 0.5078  loss_ce_3: 0.2714  loss_bbox_3: 0.1804  loss_giou_3: 0.508  loss_ce_4: 0.2577  loss_bbox_4: 0.1862  loss_giou_4: 0.4945  time: 0.6558  data_time: 0.1117  lr: 5.9941e-06  max_mem: 32466M
[08/11 17:45:08] d2.utils.events INFO:  eta: 15:56:49  iter: 79  total_loss: 50.47  loss_ce: 0.271  loss_bbox: 0.1782  loss_giou: 0.5052  loss_ce_0: 0.3337  loss_bbox_0: 0.2145  loss_giou_0: 0.5519  loss_ce_1: 0.2822  loss_bbox_1: 0.1959  loss_giou_1: 0.5434  loss_ce_2: 0.2481  loss_bbox_2: 0.1815  loss_giou_2: 0.5243  loss_ce_3: 0.2574  loss_bbox_3: 0.1803  loss_giou_3: 0.5156  loss_ce_4: 0.2696  loss_bbox_4: 0.1792  loss_giou_4: 0.5031  time: 0.6618  data_time: 0.1162  lr: 7.9921e-06  max_mem: 32466M
[08/11 17:45:21] d2.utils.events INFO:  eta: 15:53:15  iter: 99  total_loss: 58.38  loss_ce: 0.2605  loss_bbox: 0.1884  loss_giou: 0.5447  loss_ce_0: 0.3497  loss_bbox_0: 0.2209  loss_giou_0: 0.5876  loss_ce_1: 0.3217  loss_bbox_1: 0.2025  loss_giou_1: 0.5667  loss_ce_2: 0.2742  loss_bbox_2: 0.1997  loss_giou_2: 0.5595  loss_ce_3: 0.261  loss_bbox_3: 0.1936  loss_giou_3: 0.5478  loss_ce_4: 0.256  loss_bbox_4: 0.1886  loss_giou_4: 0.5456  time: 0.6580  data_time: 0.0962  lr: 9.9901e-06  max_mem: 32466M
[08/11 17:45:33] d2.utils.events INFO:  eta: 15:49:30  iter: 119  total_loss: 51.68  loss_ce: 0.2233  loss_bbox: 0.1819  loss_giou: 0.4815  loss_ce_0: 0.3275  loss_bbox_0: 0.2097  loss_giou_0: 0.5238  loss_ce_1: 0.2633  loss_bbox_1: 0.1982  loss_giou_1: 0.511  loss_ce_2: 0.2311  loss_bbox_2: 0.1859  loss_giou_2: 0.4949  loss_ce_3: 0.2335  loss_bbox_3: 0.1838  loss_giou_3: 0.4919  loss_ce_4: 0.2273  loss_bbox_4: 0.1855  loss_giou_4: 0.4791  time: 0.6522  data_time: 0.0917  lr: 1.1988e-05  max_mem: 32466M
[08/11 17:45:46] d2.utils.events INFO:  eta: 15:51:45  iter: 139  total_loss: 54.89  loss_ce: 0.2706  loss_bbox: 0.1913  loss_giou: 0.5043  loss_ce_0: 0.3427  loss_bbox_0: 0.2131  loss_giou_0: 0.5577  loss_ce_1: 0.3101  loss_bbox_1: 0.2046  loss_giou_1: 0.5202  loss_ce_2: 0.2844  loss_bbox_2: 0.1996  loss_giou_2: 0.5032  loss_ce_3: 0.2805  loss_bbox_3: 0.1922  loss_giou_3: 0.5084  loss_ce_4: 0.271  loss_bbox_4: 0.1918  loss_giou_4: 0.5068  time: 0.6493  data_time: 0.0886  lr: 1.3986e-05  max_mem: 32466M
[08/11 17:45:59] d2.utils.events INFO:  eta: 15:50:26  iter: 159  total_loss: 53.7  loss_ce: 0.3059  loss_bbox: 0.2053  loss_giou: 0.5433  loss_ce_0: 0.3928  loss_bbox_0: 0.2339  loss_giou_0: 0.6046  loss_ce_1: 0.3462  loss_bbox_1: 0.221  loss_giou_1: 0.569  loss_ce_2: 0.3126  loss_bbox_2: 0.2116  loss_giou_2: 0.5546  loss_ce_3: 0.3061  loss_bbox_3: 0.2092  loss_giou_3: 0.5401  loss_ce_4: 0.3003  loss_bbox_4: 0.2084  loss_giou_4: 0.5431  time: 0.6492  data_time: 0.0972  lr: 1.5984e-05  max_mem: 32466M
[08/11 17:46:12] d2.utils.events INFO:  eta: 15:47:12  iter: 179  total_loss: 56.58  loss_ce: 0.2687  loss_bbox: 0.2062  loss_giou: 0.4887  loss_ce_0: 0.3555  loss_bbox_0: 0.2272  loss_giou_0: 0.5435  loss_ce_1: 0.3113  loss_bbox_1: 0.2161  loss_giou_1: 0.5131  loss_ce_2: 0.3069  loss_bbox_2: 0.2111  loss_giou_2: 0.5094  loss_ce_3: 0.2779  loss_bbox_3: 0.2097  loss_giou_3: 0.4972  loss_ce_4: 0.2602  loss_bbox_4: 0.2107  loss_giou_4: 0.4971  time: 0.6475  data_time: 0.0969  lr: 1.7982e-05  max_mem: 32466M
[08/11 17:46:24] d2.utils.events INFO:  eta: 15:43:02  iter: 199  total_loss: 65.83  loss_ce: 0.334  loss_bbox: 0.2145  loss_giou: 0.5798  loss_ce_0: 0.4234  loss_bbox_0: 0.2466  loss_giou_0: 0.6225  loss_ce_1: 0.374  loss_bbox_1: 0.2221  loss_giou_1: 0.5952  loss_ce_2: 0.3505  loss_bbox_2: 0.2167  loss_giou_2: 0.5922  loss_ce_3: 0.3337  loss_bbox_3: 0.2185  loss_giou_3: 0.5922  loss_ce_4: 0.3328  loss_bbox_4: 0.2147  loss_giou_4: 0.5809  time: 0.6433  data_time: 0.0936  lr: 1.998e-05  max_mem: 32466M
[08/11 17:46:37] d2.utils.events INFO:  eta: 15:45:19  iter: 219  total_loss: 50.32  loss_ce: 0.2899  loss_bbox: 0.191  loss_giou: 0.4762  loss_ce_0: 0.4046  loss_bbox_0: 0.2304  loss_giou_0: 0.5153  loss_ce_1: 0.3503  loss_bbox_1: 0.205  loss_giou_1: 0.4896  loss_ce_2: 0.3146  loss_bbox_2: 0.204  loss_giou_2: 0.4776  loss_ce_3: 0.3095  loss_bbox_3: 0.1957  loss_giou_3: 0.4751  loss_ce_4: 0.2912  loss_bbox_4: 0.191  loss_giou_4: 0.4792  time: 0.6417  data_time: 0.0927  lr: 2.1978e-05  max_mem: 32466M
[08/11 17:46:49] d2.utils.events INFO:  eta: 15:45:06  iter: 239  total_loss: 58.82  loss_ce: 0.3165  loss_bbox: 0.2001  loss_giou: 0.5226  loss_ce_0: 0.4014  loss_bbox_0: 0.2251  loss_giou_0: 0.5672  loss_ce_1: 0.3475  loss_bbox_1: 0.21  loss_giou_1: 0.5595  loss_ce_2: 0.3268  loss_bbox_2: 0.2113  loss_giou_2: 0.5416  loss_ce_3: 0.3195  loss_bbox_3: 0.2096  loss_giou_3: 0.5266  loss_ce_4: 0.3217  loss_bbox_4: 0.2014  loss_giou_4: 0.526  time: 0.6408  data_time: 0.1034  lr: 2.3976e-05  max_mem: 32466M
[08/11 17:47:02] d2.utils.events INFO:  eta: 15:47:31  iter: 259  total_loss: 58.48  loss_ce: 0.3041  loss_bbox: 0.2111  loss_giou: 0.5197  loss_ce_0: 0.382  loss_bbox_0: 0.2566  loss_giou_0: 0.5555  loss_ce_1: 0.3451  loss_bbox_1: 0.2301  loss_giou_1: 0.5382  loss_ce_2: 0.3253  loss_bbox_2: 0.23  loss_giou_2: 0.5236  loss_ce_3: 0.3239  loss_bbox_3: 0.2234  loss_giou_3: 0.5237  loss_ce_4: 0.3035  loss_bbox_4: 0.212  loss_giou_4: 0.5203  time: 0.6414  data_time: 0.0888  lr: 2.5974e-05  max_mem: 32466M
[08/11 17:47:15] d2.utils.events INFO:  eta: 15:47:18  iter: 279  total_loss: 61.6  loss_ce: 0.3209  loss_bbox: 0.2085  loss_giou: 0.4964  loss_ce_0: 0.4189  loss_bbox_0: 0.2291  loss_giou_0: 0.5648  loss_ce_1: 0.3773  loss_bbox_1: 0.2129  loss_giou_1: 0.5218  loss_ce_2: 0.3374  loss_bbox_2: 0.2078  loss_giou_2: 0.5061  loss_ce_3: 0.3277  loss_bbox_3: 0.2119  loss_giou_3: 0.5076  loss_ce_4: 0.3234  loss_bbox_4: 0.2086  loss_giou_4: 0.4962  time: 0.6408  data_time: 0.1010  lr: 2.7972e-05  max_mem: 32466M
[08/11 17:47:27] d2.utils.events INFO:  eta: 15:43:17  iter: 299  total_loss: 104.7  loss_ce: 0.7896  loss_bbox: 0.3161  loss_giou: 0.7269  loss_ce_0: 0.8701  loss_bbox_0: 0.3699  loss_giou_0: 0.7924  loss_ce_1: 0.8221  loss_bbox_1: 0.3466  loss_giou_1: 0.7547  loss_ce_2: 0.7856  loss_bbox_2: 0.328  loss_giou_2: 0.7459  loss_ce_3: 0.7814  loss_bbox_3: 0.3216  loss_giou_3: 0.7327  loss_ce_4: 0.7856  loss_bbox_4: 0.3212  loss_giou_4: 0.7272  time: 0.6390  data_time: 0.0971  lr: 2.997e-05  max_mem: 32466M
[08/11 17:47:40] d2.utils.events INFO:  eta: 15:40:58  iter: 319  total_loss: 101.8  loss_ce: 0.6207  loss_bbox: 0.3372  loss_giou: 0.6808  loss_ce_0: 0.7874  loss_bbox_0: 0.3949  loss_giou_0: 0.7417  loss_ce_1: 0.7006  loss_bbox_1: 0.3674  loss_giou_1: 0.7082  loss_ce_2: 0.6669  loss_bbox_2: 0.3542  loss_giou_2: 0.6993  loss_ce_3: 0.6671  loss_bbox_3: 0.3364  loss_giou_3: 0.6913  loss_ce_4: 0.6261  loss_bbox_4: 0.3386  loss_giou_4: 0.6859  time: 0.6379  data_time: 0.0966  lr: 3.1968e-05  max_mem: 32466M
[08/11 17:47:52] d2.utils.events INFO:  eta: 15:40:46  iter: 339  total_loss: 74.76  loss_ce: 0.5085  loss_bbox: 0.2678  loss_giou: 0.5742  loss_ce_0: 0.5878  loss_bbox_0: 0.3221  loss_giou_0: 0.6279  loss_ce_1: 0.5525  loss_bbox_1: 0.2875  loss_giou_1: 0.5991  loss_ce_2: 0.5147  loss_bbox_2: 0.2792  loss_giou_2: 0.6007  loss_ce_3: 0.5051  loss_bbox_3: 0.2686  loss_giou_3: 0.5936  loss_ce_4: 0.5114  loss_bbox_4: 0.2629  loss_giou_4: 0.5759  time: 0.6366  data_time: 0.0916  lr: 3.3966e-05  max_mem: 32466M
[08/11 17:48:05] d2.utils.events INFO:  eta: 15:40:22  iter: 359  total_loss: 93.58  loss_ce: 0.4942  loss_bbox: 0.2603  loss_giou: 0.6149  loss_ce_0: 0.6201  loss_bbox_0: 0.2975  loss_giou_0: 0.6527  loss_ce_1: 0.5574  loss_bbox_1: 0.2811  loss_giou_1: 0.6322  loss_ce_2: 0.5126  loss_bbox_2: 0.2773  loss_giou_2: 0.6328  loss_ce_3: 0.496  loss_bbox_3: 0.2712  loss_giou_3: 0.6208  loss_ce_4: 0.4888  loss_bbox_4: 0.2604  loss_giou_4: 0.6161  time: 0.6362  data_time: 0.0934  lr: 3.5964e-05  max_mem: 32466M
[08/11 17:48:17] d2.utils.events INFO:  eta: 15:39:24  iter: 379  total_loss: 74.73  loss_ce: 0.4355  loss_bbox: 0.2723  loss_giou: 0.5815  loss_ce_0: 0.4764  loss_bbox_0: 0.316  loss_giou_0: 0.6396  loss_ce_1: 0.4569  loss_bbox_1: 0.2964  loss_giou_1: 0.6067  loss_ce_2: 0.437  loss_bbox_2: 0.2881  loss_giou_2: 0.5976  loss_ce_3: 0.4363  loss_bbox_3: 0.2811  loss_giou_3: 0.5972  loss_ce_4: 0.4393  loss_bbox_4: 0.2751  loss_giou_4: 0.5911  time: 0.6356  data_time: 0.0989  lr: 3.7962e-05  max_mem: 32466M
[08/11 17:48:30] d2.utils.events INFO:  eta: 15:37:25  iter: 399  total_loss: 72.4  loss_ce: 0.5051  loss_bbox: 0.2796  loss_giou: 0.6364  loss_ce_0: 0.5838  loss_bbox_0: 0.3233  loss_giou_0: 0.6886  loss_ce_1: 0.557  loss_bbox_1: 0.3056  loss_giou_1: 0.6738  loss_ce_2: 0.5097  loss_bbox_2: 0.286  loss_giou_2: 0.6547  loss_ce_3: 0.5052  loss_bbox_3: 0.2831  loss_giou_3: 0.6377  loss_ce_4: 0.4982  loss_bbox_4: 0.2795  loss_giou_4: 0.6384  time: 0.6342  data_time: 0.0962  lr: 3.996e-05  max_mem: 32466M
[08/11 17:48:42] d2.utils.events INFO:  eta: 15:37:13  iter: 419  total_loss: 70.06  loss_ce: 0.4342  loss_bbox: 0.2638  loss_giou: 0.5393  loss_ce_0: 0.484  loss_bbox_0: 0.3158  loss_giou_0: 0.595  loss_ce_1: 0.4513  loss_bbox_1: 0.2851  loss_giou_1: 0.5702  loss_ce_2: 0.423  loss_bbox_2: 0.2794  loss_giou_2: 0.5523  loss_ce_3: 0.4286  loss_bbox_3: 0.2704  loss_giou_3: 0.5416  loss_ce_4: 0.4327  loss_bbox_4: 0.2661  loss_giou_4: 0.5348  time: 0.6330  data_time: 0.0954  lr: 4.1958e-05  max_mem: 32466M
[08/11 17:48:56] d2.utils.events INFO:  eta: 15:39:31  iter: 439  total_loss: 62.83  loss_ce: 0.4398  loss_bbox: 0.2432  loss_giou: 0.6029  loss_ce_0: 0.506  loss_bbox_0: 0.2832  loss_giou_0: 0.6493  loss_ce_1: 0.4848  loss_bbox_1: 0.2524  loss_giou_1: 0.6286  loss_ce_2: 0.4456  loss_bbox_2: 0.2449  loss_giou_2: 0.6001  loss_ce_3: 0.4377  loss_bbox_3: 0.2485  loss_giou_3: 0.6007  loss_ce_4: 0.4289  loss_bbox_4: 0.2419  loss_giou_4: 0.5989  time: 0.6353  data_time: 0.0952  lr: 4.3956e-05  max_mem: 33807M
[08/11 17:49:08] d2.utils.events INFO:  eta: 15:38:34  iter: 459  total_loss: 67.65  loss_ce: 0.439  loss_bbox: 0.2519  loss_giou: 0.6133  loss_ce_0: 0.561  loss_bbox_0: 0.3013  loss_giou_0: 0.6619  loss_ce_1: 0.4957  loss_bbox_1: 0.2758  loss_giou_1: 0.6203  loss_ce_2: 0.474  loss_bbox_2: 0.2636  loss_giou_2: 0.6224  loss_ce_3: 0.445  loss_bbox_3: 0.2519  loss_giou_3: 0.6189  loss_ce_4: 0.4451  loss_bbox_4: 0.2507  loss_giou_4: 0.6158  time: 0.6344  data_time: 0.0932  lr: 4.5954e-05  max_mem: 33807M
[08/11 17:49:21] d2.utils.events INFO:  eta: 15:39:06  iter: 479  total_loss: 81.05  loss_ce: 0.5221  loss_bbox: 0.2993  loss_giou: 0.6784  loss_ce_0: 0.5943  loss_bbox_0: 0.3416  loss_giou_0: 0.7376  loss_ce_1: 0.5709  loss_bbox_1: 0.3135  loss_giou_1: 0.7084  loss_ce_2: 0.5253  loss_bbox_2: 0.3088  loss_giou_2: 0.6987  loss_ce_3: 0.5208  loss_bbox_3: 0.3013  loss_giou_3: 0.6996  loss_ce_4: 0.513  loss_bbox_4: 0.2926  loss_giou_4: 0.6832  time: 0.6348  data_time: 0.0929  lr: 4.7952e-05  max_mem: 33807M
[08/11 17:49:33] d2.utils.events INFO:  eta: 15:38:09  iter: 499  total_loss: 118.4  loss_ce: 1.164  loss_bbox: 0.4476  loss_giou: 0.8197  loss_ce_0: 1.284  loss_bbox_0: 0.4805  loss_giou_0: 0.8516  loss_ce_1: 1.243  loss_bbox_1: 0.4596  loss_giou_1: 0.8429  loss_ce_2: 1.2  loss_bbox_2: 0.4465  loss_giou_2: 0.8238  loss_ce_3: 1.203  loss_bbox_3: 0.4542  loss_giou_3: 0.8032  loss_ce_4: 1.187  loss_bbox_4: 0.452  loss_giou_4: 0.8167  time: 0.6340  data_time: 0.0865  lr: 4.995e-05  max_mem: 33807M
[08/11 17:49:46] d2.utils.events INFO:  eta: 15:37:56  iter: 519  total_loss: 106.1  loss_ce: 0.7905  loss_bbox: 0.3496  loss_giou: 0.7489  loss_ce_0: 0.8757  loss_bbox_0: 0.4263  loss_giou_0: 0.8099  loss_ce_1: 0.8311  loss_bbox_1: 0.3823  loss_giou_1: 0.7738  loss_ce_2: 0.7928  loss_bbox_2: 0.3681  loss_giou_2: 0.7566  loss_ce_3: 0.7882  loss_bbox_3: 0.359  loss_giou_3: 0.751  loss_ce_4: 0.7802  loss_bbox_4: 0.3515  loss_giou_4: 0.7496  time: 0.6342  data_time: 0.0941  lr: 5.1948e-05  max_mem: 33807M
[08/11 17:49:58] d2.utils.events INFO:  eta: 15:35:57  iter: 539  total_loss: 91.67  loss_ce: 0.7879  loss_bbox: 0.3259  loss_giou: 0.7154  loss_ce_0: 0.8534  loss_bbox_0: 0.3619  loss_giou_0: 0.7839  loss_ce_1: 0.8228  loss_bbox_1: 0.3422  loss_giou_1: 0.7485  loss_ce_2: 0.8014  loss_bbox_2: 0.3312  loss_giou_2: 0.7373  loss_ce_3: 0.7802  loss_bbox_3: 0.3275  loss_giou_3: 0.728  loss_ce_4: 0.787  loss_bbox_4: 0.3265  loss_giou_4: 0.7161  time: 0.6327  data_time: 0.0930  lr: 5.3946e-05  max_mem: 33807M
[08/11 17:50:10] d2.utils.events INFO:  eta: 15:34:47  iter: 559  total_loss: 89.38  loss_ce: 0.7369  loss_bbox: 0.316  loss_giou: 0.6481  loss_ce_0: 0.827  loss_bbox_0: 0.3535  loss_giou_0: 0.724  loss_ce_1: 0.7698  loss_bbox_1: 0.3491  loss_giou_1: 0.6821  loss_ce_2: 0.7729  loss_bbox_2: 0.3408  loss_giou_2: 0.6683  loss_ce_3: 0.7493  loss_bbox_3: 0.3179  loss_giou_3: 0.6595  loss_ce_4: 0.7565  loss_bbox_4: 0.3173  loss_giou_4: 0.6547  time: 0.6322  data_time: 0.0886  lr: 5.5944e-05  max_mem: 33807M
[08/11 17:50:23] d2.utils.events INFO:  eta: 15:34:12  iter: 579  total_loss: 75.04  loss_ce: 0.6316  loss_bbox: 0.3156  loss_giou: 0.675  loss_ce_0: 0.7224  loss_bbox_0: 0.367  loss_giou_0: 0.7378  loss_ce_1: 0.7032  loss_bbox_1: 0.3314  loss_giou_1: 0.697  loss_ce_2: 0.6829  loss_bbox_2: 0.3207  loss_giou_2: 0.6865  loss_ce_3: 0.669  loss_bbox_3: 0.322  loss_giou_3: 0.6795  loss_ce_4: 0.6409  loss_bbox_4: 0.3196  loss_giou_4: 0.6742  time: 0.6322  data_time: 0.0997  lr: 5.7942e-05  max_mem: 33807M
[08/11 17:50:36] d2.utils.events INFO:  eta: 15:33:34  iter: 599  total_loss: 89.58  loss_ce: 0.5961  loss_bbox: 0.3149  loss_giou: 0.6316  loss_ce_0: 0.6906  loss_bbox_0: 0.3722  loss_giou_0: 0.7227  loss_ce_1: 0.6353  loss_bbox_1: 0.3441  loss_giou_1: 0.6782  loss_ce_2: 0.6042  loss_bbox_2: 0.3324  loss_giou_2: 0.6513  loss_ce_3: 0.6085  loss_bbox_3: 0.3208  loss_giou_3: 0.6361  loss_ce_4: 0.597  loss_bbox_4: 0.3142  loss_giou_4: 0.6349  time: 0.6320  data_time: 0.0981  lr: 5.994e-05  max_mem: 33807M
[08/11 17:50:48] d2.utils.events INFO:  eta: 15:34:14  iter: 619  total_loss: 74.57  loss_ce: 0.5173  loss_bbox: 0.2962  loss_giou: 0.6267  loss_ce_0: 0.6077  loss_bbox_0: 0.3416  loss_giou_0: 0.6871  loss_ce_1: 0.5689  loss_bbox_1: 0.329  loss_giou_1: 0.6634  loss_ce_2: 0.5312  loss_bbox_2: 0.311  loss_giou_2: 0.645  loss_ce_3: 0.5139  loss_bbox_3: 0.2998  loss_giou_3: 0.6311  loss_ce_4: 0.526  loss_bbox_4: 0.2952  loss_giou_4: 0.6212  time: 0.6318  data_time: 0.0958  lr: 6.1938e-05  max_mem: 33807M
[08/11 17:51:01] d2.utils.events INFO:  eta: 15:33:44  iter: 639  total_loss: 83.5  loss_ce: 0.5025  loss_bbox: 0.2627  loss_giou: 0.6706  loss_ce_0: 0.5963  loss_bbox_0: 0.297  loss_giou_0: 0.7417  loss_ce_1: 0.5342  loss_bbox_1: 0.2789  loss_giou_1: 0.6967  loss_ce_2: 0.5357  loss_bbox_2: 0.2704  loss_giou_2: 0.685  loss_ce_3: 0.4986  loss_bbox_3: 0.2641  loss_giou_3: 0.6718  loss_ce_4: 0.5056  loss_bbox_4: 0.2607  loss_giou_4: 0.6721  time: 0.6314  data_time: 0.0968  lr: 6.3936e-05  max_mem: 33807M
[08/11 17:51:13] d2.utils.events INFO:  eta: 15:33:31  iter: 659  total_loss: 76.05  loss_ce: 0.5255  loss_bbox: 0.2844  loss_giou: 0.6851  loss_ce_0: 0.583  loss_bbox_0: 0.3239  loss_giou_0: 0.7481  loss_ce_1: 0.5579  loss_bbox_1: 0.3105  loss_giou_1: 0.7173  loss_ce_2: 0.5338  loss_bbox_2: 0.292  loss_giou_2: 0.7039  loss_ce_3: 0.5373  loss_bbox_3: 0.2832  loss_giou_3: 0.6856  loss_ce_4: 0.5368  loss_bbox_4: 0.2848  loss_giou_4: 0.6821  time: 0.6304  data_time: 0.0936  lr: 6.5934e-05  max_mem: 33807M
[08/11 17:51:25] d2.utils.events INFO:  eta: 15:33:47  iter: 679  total_loss: 78.59  loss_ce: 0.5369  loss_bbox: 0.2926  loss_giou: 0.6735  loss_ce_0: 0.6274  loss_bbox_0: 0.337  loss_giou_0: 0.734  loss_ce_1: 0.571  loss_bbox_1: 0.3178  loss_giou_1: 0.6824  loss_ce_2: 0.5531  loss_bbox_2: 0.3067  loss_giou_2: 0.6723  loss_ce_3: 0.543  loss_bbox_3: 0.2955  loss_giou_3: 0.6686  loss_ce_4: 0.5397  loss_bbox_4: 0.2955  loss_giou_4: 0.6722  time: 0.6303  data_time: 0.1013  lr: 6.7932e-05  max_mem: 33807M
[08/11 17:51:39] d2.utils.events INFO:  eta: 15:35:37  iter: 699  total_loss: 76.7  loss_ce: 0.563  loss_bbox: 0.2716  loss_giou: 0.6127  loss_ce_0: 0.6072  loss_bbox_0: 0.3172  loss_giou_0: 0.6933  loss_ce_1: 0.5775  loss_bbox_1: 0.2932  loss_giou_1: 0.6535  loss_ce_2: 0.5531  loss_bbox_2: 0.2764  loss_giou_2: 0.6252  loss_ce_3: 0.5552  loss_bbox_3: 0.2745  loss_giou_3: 0.6206  loss_ce_4: 0.5651  loss_bbox_4: 0.2754  loss_giou_4: 0.6137  time: 0.6318  data_time: 0.0972  lr: 6.993e-05  max_mem: 33807M
[08/11 17:51:51] d2.utils.events INFO:  eta: 15:34:04  iter: 719  total_loss: 83.4  loss_ce: 0.5519  loss_bbox: 0.3032  loss_giou: 0.6623  loss_ce_0: 0.6329  loss_bbox_0: 0.3598  loss_giou_0: 0.7345  loss_ce_1: 0.5939  loss_bbox_1: 0.3242  loss_giou_1: 0.6781  loss_ce_2: 0.5612  loss_bbox_2: 0.3155  loss_giou_2: 0.6826  loss_ce_3: 0.5612  loss_bbox_3: 0.312  loss_giou_3: 0.6729  loss_ce_4: 0.5514  loss_bbox_4: 0.3042  loss_giou_4: 0.6622  time: 0.6311  data_time: 0.0919  lr: 7.1928e-05  max_mem: 33807M
[08/11 17:52:05] d2.utils.events INFO:  eta: 15:35:38  iter: 739  total_loss: 119.9  loss_ce: 1.287  loss_bbox: 0.5195  loss_giou: 0.8015  loss_ce_0: 1.347  loss_bbox_0: 0.6054  loss_giou_0: 0.9222  loss_ce_1: 1.319  loss_bbox_1: 0.5733  loss_giou_1: 0.841  loss_ce_2: 1.283  loss_bbox_2: 0.5422  loss_giou_2: 0.8188  loss_ce_3: 1.303  loss_bbox_3: 0.5347  loss_giou_3: 0.81  loss_ce_4: 1.274  loss_bbox_4: 0.5256  loss_giou_4: 0.8037  time: 0.6320  data_time: 0.1073  lr: 7.3926e-05  max_mem: 33807M
[08/11 17:52:16] d2.engine.hooks INFO: Overall training speed: 756 iterations in 0:07:57 (0.6322 s / it)
[08/11 17:52:16] d2.engine.hooks INFO: Total training time: 0:08:00 (0:00:02 on hooks)
[08/11 17:52:16] d2.utils.events INFO:  eta: 15:36:16  iter: 758  total_loss: 137  loss_ce: 1.857  loss_bbox: 0.625  loss_giou: 1.037  loss_ce_0: 1.908  loss_bbox_0: 0.6941  loss_giou_0: 1.072  loss_ce_1: 1.903  loss_bbox_1: 0.6519  loss_giou_1: 1.048  loss_ce_2: 1.925  loss_bbox_2: 0.6398  loss_giou_2: 1.05  loss_ce_3: 1.871  loss_bbox_3: 0.6399  loss_giou_3: 1.045  loss_ce_4: 1.853  loss_bbox_4: 0.6251  loss_giou_4: 1.048  time: 0.6318  data_time: 0.0940  lr: 7.5724e-05  max_mem: 33807M
